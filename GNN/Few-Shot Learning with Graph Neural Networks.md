# Few-Shot Learning with Graph Neural Networks

## 핵심 주장과 주요 기여

이 논문은 **few-shot learning을 그래프 신경망의 supervised message passing 문제로 재정의**하는 혁신적인 접근법을 제시합니다[1]. 핵심 아이디어는 입력 이미지 컬렉션을 완전 연결 그래프로 표현하고, 노드 간 메시지 전달을 통해 레이블 정보를 전파하는 것입니다[1].

```
## 1. Supervised Message Passing 문제

**Supervised Message Passing**은 그래프에서 노드 간에 정보를 전달하여 레이블을 예측하는 문제입니다[1].

### 기본 개념
- 그래프의 일부 노드에는 레이블이 있고(labeled), 일부는 없음(unlabeled)
- 레이블이 있는 노드에서 없는 노드로 정보를 '메시지'로 전달
- 여러 번의 메시지 전달을 통해 최종 예측 수행

### 구체적 예시
5개 이미지가 있는 상황에서:
- 노드 1: 고양이 이미지 (레이블: 고양이)
- 노드 2: 개 이미지 (레이블: 개)
- 노드 3: ??? 이미지 (레이블: 미지) ← 예측 대상
- 노드 4: 고양이 이미지 (레이블: 고양이)
- 노드 5: 개 이미지 (레이블: 개)

**Message Passing 과정**:
1. 각 노드가 자신의 정보를 이웃들에게 전달
2. 받은 정보들을 결합하여 노드 표현을 업데이트
3. 1-2단계를 여러 번 반복
4. 최종적으로 노드 3의 레이블을 예측
```

**주요 기여**는 다음과 같습니다[1]:

- Few-shot learning을 graph neural network의 supervised message passing으로 재정의
- Omniglot과 Mini-ImageNet에서 SOTA 성능을 **훨씬 적은 파라미터**로 달성 (TCML 대비 1/10 수준)
- Semi-supervised와 Active learning으로 자연스럽게 확장 가능한 **통합 프레임워크** 제시
- 기존 모델들(Siamese Networks, Prototypical Networks, Matching Networks)을 **일반화하는 이론적 프레임워크** 구축

```
## 2. Active Learning의 자연스러운 확장

**Active Learning**은 모델이 능동적으로 '어떤 데이터의 레이블을 알고 싶은지' 선택하는 방법입니다[1].

### 자연스러운 확장이 가능한 이유

**1. 그래프 구조의 장점**
- 노드들 간의 관계를 이미 모델링하고 있음
- 어떤 노드의 레이블이 다른 노드에 얼마나 영향을 주는지 계산 가능

**2. 기존 아키텍처 재사용**
- Few-shot learning과 동일한 그래프 구조 사용
- 추가로 attention 메커니즘만 추가하면 됨

**3. 통합된 프레임워크**
- Few-shot: 레이블된 노드 → 미지 노드로 정보 전달
- Active: 미지 노드 중에서 가장 유용한 것 선택 → 레이블 획득 → 다시 정보 전달
- 같은 메시지 전달 메커니즘을 공유
```

## 해결하고자 하는 문제

### 문제 정의
기존 few-shot learning 방법들의 주요 한계는 **복잡한 의존성을 중간 단계에서 활용하지 못한다**는 점입니다[1]. 특히:

- Matching Networks: 독립적인 support set encoding으로 상호 의존성 부족
- Prototypical Networks: 클래스별 프로토타입에만 의존
- Siamese Networks: 단일 레이어 비교에 제한

### 수학적 문제 설정
논문은 부분적으로 레이블된 이미지 컬렉션에 대한 문제를 다음과 같이 정의합니다[1]:

$$T = \{(x_1, l_1), \ldots, (x_s, l_s)\}, \{\tilde{x}_1, \ldots, \tilde{x}_r\}, \{\bar{x}_1, \ldots, \bar{x}_t\}; l_i \in \{1, K\}$$

$$Y = (y_1, \ldots, y_t) \in \{1, K\}^t$$

여기서 s는 레이블된 샘플 수, r은 레이블되지 않은 샘플 수, t는 분류할 샘플 수, K는 클래스 수입니다[1].

## 제안하는 방법론

### 핵심 아키텍처

**1. 그래프 표현**
입력 컬렉션 T를 완전 연결 그래프 G_T = (V, E)로 변환합니다[1]:
- 노드: 각 이미지 (레이블된 것과 레이블되지 않은 것 모두)
- 엣지: 학습 가능한 유사도 커널

```
## 3. 완전 연결 그래프와 학습 가능한 유사도 커널

### 완전 연결 그래프 (Fully Connected Graph)
- 모든 노드가 다른 모든 노드와 연결된 그래프
- n개의 노드가 있으면 총 n×(n-1)/2 개의 엣지가 존재

### 학습 가능한 유사도 커널
기존 방식과 달리 신경망이 두 이미지를 보고 유사도 점수를 계산합니다[1]:

**구체적 계산 과정**:
1. 두 이미지의 특징 벡터를 추출: f₁, f₂
2. 절댓값 차이 계산: |f₁ - f₂|
3. 이 차이를 MLP에 입력하여 유사도 점수 계산: $$\tilde{\phi}_\theta(x_i^{(k)}, x_j^{(k)}) = \text{MLP}_{\tilde{\theta}}(\text{abs}(x_i^{(k)} - x_j^{(k)}))$$[1]
4. 이 점수가 엣지의 가중치가 됨
```

**2. GNN 레이어 업데이트**
각 레이어에서 노드 특징은 다음과 같이 업데이트됩니다[1]:

$$x_l^{(k+1)} = G_c(x^{(k)}) = \rho\left(\sum_{B \in \mathcal{A}} Bx^{(k)}\theta_{B,l}^{(k)}\right)$$

여기서:
- $$x^{(k)}$$: k번째 레이어의 노드 특징
- $$\mathcal{A}$$: graph intrinsic linear operators 집합

```
## 4. Graph Intrinsic Linear Operators 집합

**Linear Operator**는 그래프의 구조적 정보를 활용하는 행렬 연산입니다[1].

### 주요 Operators와 수치 예시

**1. 인접 행렬 (Adjacency Matrix) A**

A = [[0.0, 0.8, 0.3, 0.1],
     [0.8, 0.0, 0.6, 0.4],
     [0.3, 0.6, 0.0, 0.9],
     [0.1, 0.4, 0.9, 0.0]]
- 각 노드가 이웃 노드들의 가중평균 특징을 받음

**2. 항등 행렬 (Identity Matrix) I**

I = [[1, 0, 0, 0],
     [0, 1, 0, 0],
     [0, 0, 1, 0],
     [0, 0, 0, 1]]

- 자기 자신의 특징을 그대로 유지

**GNN 레이어 업데이트 수식**[1]:
$$x_l^{(k+1)} = G_c(x^{(k)}) = \rho\left(\sum_{B \in \mathcal{A}} Bx^{(k)}\theta_{B,l}^{(k)}\right)$$

여기서 집합 A = {A, I}를 사용하여 이웃 정보 수집과 자신 정보 보존을 균형있게 활용합니다.
```

- $$\theta$$: 학습 가능한 파라미터
- $$\rho$$: 비선형 활성화 함수 (leaky ReLU)

**3. 학습 가능한 인접 행렬**
두 노드 간의 유사도는 다음과 같이 계산됩니다[1]:

$$\tilde{A}\_{i,j}^{(k)} = \tilde{\phi}_\theta(x_i^{(k)}, x_j^{(k)})$$

$$\tilde{\phi}\_\theta(x_i^{(k)}, x_j^{(k)}) = \text{MLP}_{\tilde{\theta}}(\text{abs}(x_i^{(k)} - x_j^{(k)}))$$

이 방식은 대칭성과 동일성 속성을 자연스럽게 만족합니다[1].

```
## 5. 학습 가능한 인접 행렬의 유사도 계산

### 단계별 계산 과정

**1단계**: 두 노드의 특징 벡터 (예: [1.2, 0.8, 2.1], [1.5, 0.6, 1.9])

**2단계**: 절댓값 차이 계산: |xi - xj| = [0.3, 0.2, 0.2]

**3단계**: MLP를 통한 유사도 점수 계산
- 입력(3차원) → 은닉층(4차원) → 출력(1차원)
- 유사도 점수 예시: 0.384

**4단계**: Softmax 정규화로 확률 분포 변환
- 각 행의 합이 1.0이 되도록 정규화
- 높은 값 = 강한 연결, 낮은 값 = 약한 연결

```

**4. 초기 노드 특징**
각 노드의 초기 특징은 다음과 같이 구성됩니다[1]:

$$x_i^{(0)} = (\phi(x_i), h(l_i))$$

여기서:
- $$\phi(x_i)$$: CNN을 통한 이미지 임베딩
- $$h(l_i)$$: 레이블의 one-hot 인코딩 (미지 레이블의 경우 균등 분포 사용)

```
## 6. 미지 레이블에 균등 분포를 사용하는 이유

### 문제 상황
신경망은 모든 노드에 대해 동일한 형태의 입력이 필요하지만, 일부 노드는 레이블을 모름[1].

### 노드 특징 구성
$$x_i^{(0)} = (\phi(x_i), h(l_i))$$[1]

여기서:
- φ(xi): CNN을 통한 이미지 임베딩
- h(li): 레이블의 one-hot 인코딩

### 균등 분포를 선택하는 이유

**장점들**:
- **최대 불확실성 표현**: 모든 클래스에 대해 동일한 확률
- **편향되지 않음**: 어떤 클래스를 선호하지 않음
- **정보 이론적 합리성**: 최대 엔트로피 (1.585 bits vs 영벡터 0.000 bits)

3-way 분류에서 균등 분포 [1/3, 1/3, 1/3]는 가장 높은 엔트로피를 가져 '진짜 모른다'는 상태를 가장 잘 표현합니다.
```

## 모델 구조

### CNN 임베딩 네트워크
- **Omniglot**: 4개 블록 {3×3-conv(64), batch-norm, 2×2 max-pool, leaky-relu} → 64차원 임베딩[1]
- **Mini-ImageNet**: 4개 conv 레이어 → 128차원 임베딩[1]

### GNN 구조
3개 블록으로 구성되며, 각 블록은[1]:
1. 인접 행렬 계산 모듈
2. Graph convolutional layer
3. 밀집 연결(dense connection) 사용

```
## 7. 밀집 연결(Dense Connection) 사용 방식

### 구현 방식
논문에서는 ResNet 스타일의 밀집 연결을 사용합니다[1]:

$$h_{k+1} = [GNN\_layer(h_k), h_k]$$

현재 레이어 출력과 이전 레이어 입력을 연결(concatenate)합니다.

### 장점들
1. **기울기 소실 문제 완화**: 깊은 네트워크에서도 초기 정보가 보존됨
2. **정보 보존**: 초기 이미지 특징과 레이블 정보가 계속 유지됨
3. **표현력 향상**: 서로 다른 레벨의 특징을 동시에 활용

Active Learning에서의 구체적 활용[1]:
$$x_1^* = [G_c(x_0^*), x_0^*] = [G_c(x_0^*), (\phi(x^*), h(l^*))]$$
```

### Active Learning 메커니즘
첫 번째 GNN 레이어 후 attention을 적용하여 가장 정보가 많은 샘플을 선택합니다[1]:
- $$g(x_i^{(1)})$$: 2층 신경망으로 스칼라 값 매핑
- Softmax attention으로 multinomial 샘플링

```
## 8. Active Learning의 Softmax Attention과 Multinomial 샘플링

### 과정 설명

**1단계**: 각 unlabeled 노드의 유용성 점수 계산
- 2층 신경망 g()를 통해 스칼라 점수로 변환

**2단계**: Softmax로 확률 분포 변환

확률 예시: [0.213, 0.149, 0.303, 0.123, 0.212]

**3단계**: 훈련 vs 테스트 시 차이
- **훈련 시**: Multinomial 샘플링 (확률적 선택)
- **테스트 시**: 최댓값 선택 (결정적 선택)

### Multinomial 샘플링을 사용하는 이유

1. **탐색과 활용의 균형**: 가장 좋은 것만 선택하면 다른 가능성을 놓칠 수 있음
2. **학습의 안정성**: 다양한 샘플링으로 더 균형잡힌 학습 가능
3. **기울기 계산 가능**: 확률적 선택도 미분 가능한 연산
```

- 선택된 레이블을 가중치로 스케일링하여 노드에 추가

## 성능 향상

### 실험 결과

**Omniglot Dataset**[1]:
- 5-Way 1-shot: 99.2%
- 5-Way 5-shot: 99.7%
- 20-Way 1-shot: 97.4%
- 20-Way 5-shot: 99.0%
- 파라미터 수: ~300K (TCML 대비 1/17)

**Mini-ImageNet Dataset**[1]:
- 5-Way 1-shot: 50.33% ± 0.36%
- 5-Way 5-shot: 66.41% ± 0.63%
- 파라미터 수: ~400K (TCML 대비 1/27)

### 추가 학습 시나리오
**Semi-supervised Learning**[1]:
- Omniglot: 20% 레이블로 40% 감독학습과 동일한 성능
- Mini-ImageNet: ~2% 성능 향상

**Active Learning**[1]:
- Mini-ImageNet에서 ~3.4% 성능 향상
- 정보가 많은 샘플을 학습하여 선택

## 일반화 성능 향상 가능성

### 주요 일반화 요소

**1. 구조적 귀납 편향**
그래프 구조를 통한 관계적 귀납 편향(relational inductive bias)이 일반화 성능을 크게 향상시킵니다[1]. Message passing을 통해 전역적 정보 전파가 가능하며, 이는 local한 정보만 활용하는 기존 방법들보다 우수한 일반화를 제공합니다.

```
## 9. 관계적 귀납 편향과 일반화 성능 향상

### 관계적 귀납 편향의 핵심
- 데이터 포인트들 간의 '관계'가 중요하다는 가정[1]
- 개별 데이터보다는 데이터 간의 상호작용에 집중
- 그래프 구조로 이런 관계를 명시적으로 모델링

### 일반화 성능 향상 메커니즘

**1. 구조적 일반화**
- 학습한 관계 패턴을 새로운 상황에 적용
- 예: '털이 있는 동물들끼리는 비슷하다'는 패턴

**2. 컨텍스트 활용**
- 단일 이미지가 아닌 전체 컨텍스트를 고려
- 더 풍부한 정보로 더 정확한 판단

**3. 희소 데이터에서의 강건성**
- Few-shot에서는 데이터가 매우 적음
- 관계 정보를 통해 부족한 정보를 보완

**4. 전이 학습 효과**
- 학습한 관계 패턴이 새로운 도메인에도 적용 가능

### 실험적 증거
- Omniglot: 99.2% (기존 최고 98.96%)
- Mini-ImageNet: 50.33% (기존 대비 경쟁적)
- 파라미터 수: 기존 방법의 1/10 ~ 1/27

더 적은 파라미터로 더 좋은 성능을 달성하여 더 효율적인 일반화를 보여줍니다[1].
```

**2. Task-specific 적응성**
학습 가능한 유사도 메트릭을 통해 각 task에 특화된 적응이 가능합니다[1]. 이는 다양한 도메인에서의 일반화 성능을 향상시키는 핵심 요소입니다.

**3. Permutation Invariance**
입력 순서에 무관한 안정성을 제공하여 다양한 데이터 배치에 대해 일관된 성능을 보장합니다[1].

**4. 효율적 파라미터 사용**
적은 파라미터로 효율적 학습이 가능하여 overfitting을 방지하고 일반화 성능을 향상시킵니다[1].

### 실험적 증거
- Semi-supervised 실험에서 unlabeled 데이터로부터 효과적인 정보 추출 성공[1]
- Active learning에서 정보가 많은 샘플 선택을 통한 성능 향상[1]
- 기존 방법들을 통합하는 일반화된 프레임워크 제시[1]

## 한계점

**1. 계산 복잡도**
완전 연결 그래프로 인한 O(n²) 계산 복잡도가 주요 한계입니다[1]. 대규모 support set에서 확장성 문제가 발생할 수 있습니다.

**2. 메모리 사用량**
그래프 구조 저장과 인접 행렬 계산으로 인한 메모리 사용량 증가가 문제가 될 수 있습니다[1].

**3. 복잡한 도메인에서의 제한**
Mini-ImageNet과 같은 복잡한 도메인에서는 Omniglot 대비 상대적으로 제한적인 성능 향상을 보입니다[1].

**4. 그래프 구조 가정**
완전 연결 그래프가 항상 최적이라는 가정에는 한계가 있으며, 도메인별로 다른 그래프 구조가 더 적합할 수 있습니다[1].

## 미래 연구에 미치는 영향

### 주요 영향

**1. 패러다임 전환**
Graph-based Meta-learning 패러다임을 확립하여 관계적 추론(Relational Reasoning)을 few-shot learning에 본격 도입했습니다[1].

**2. 통합 프레임워크**
Multiple learning paradigms (Few-shot, Semi-supervised, Active learning)의 통합 프레임워크를 제시하여 후속 연구의 방향성을 제시했습니다[1].

**3. 구조적 귀납 편향의 중요성**
메타학습에서 구조적 귀납 편향의 중요성을 실험적으로 입증했습니다[1].

### 후속 연구 고려사항

**1. 확장성 개선**
- Hierarchical GNN, Graph coarsening 방법 개발
- Sparse graph construction, Approximate message passing 연구
- 분산 처리 가능한 그래프 아키텍처 설계

**2. 일반화 성능 향상**
- Domain shift에 robust한 그래프 구조 학습
- Cross-domain few-shot learning 지원
- Zero-shot learning으로의 확장 가능성 탐구

**3. 실용적 응용**
- 모바일/엣지 디바이스에서의 경량화
- 다중 모달 데이터 처리 방안
- 설명 가능한 그래프 기반 의사결정 시스템

**4. 이론적 발전**
- GNN의 expressivity와 generalization bound 분석
- Graph topology와 learning performance 간의 이론적 관계 규명
- Causal inference 관점에서의 그래프 설계

이 논문은 few-shot learning 분야에 그래프 기반 접근법의 강력한 가능성을 제시했으며, 향후 메타학습과 관계적 추론 연구의 중요한 기반이 될 것으로 전망됩니다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/a7879ed4-a085-4c77-901c-b6412175f353/1711.04043v3.pdf
