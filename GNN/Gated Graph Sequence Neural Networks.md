# Gated Graph Sequence Neural Networks

**핵심 주장**  
GGS-NNs는 그래프 구조 데이터를 다루는 기존 Graph Neural Networks (GNNs)를 확장하여, 단일 분류뿐 아니라 그래프 입력으로부터 **시퀀스 출력**을 생성할 수 있는 범용 신경망 모델을 제안한다. 이 모델은 gated recurrent units (GRUs)를 활용한 전파(propagation) 과정을 통해 장기 의존성을 효과적으로 학습하며, 노드별 상태(annotations)를 동적으로 갱신해 복잡한 그래프 기반 문제––경로 탐색, 논리식 생성 등––에 유리한 귀납적 편향(inductive bias)을 제공한다.

**주요 기여**  
-  GNNs를 GRU 기반 전파 및 backpropagation through time로 대체하여, 수렴 제약을 제거하고 학습 효율 증대.  
-  **노드 어노테이션**을 도입해 그래프 내 특수 노드(예: 프로그램 변수, 시작·종료 지점) 표시 및 상태 업데이트를 가능케 함.  
-  **시퀀스 출력**을 위한 GGS-NN 아키텍처 제안: 각 출력 단계마다 두 개의 GG-NN(예측 및 상태 갱신)을 순차적으로 적용해 내부 상태를 전파.  
-  인공 bAbI 과제, 그래프 알고리즘 학습(최단 경로·오일러 회로) 및 프로그램 검증(분리 논리식 예측)에서 **최소 데이터**로도 SOTA 성능 달성.

***

## 1. 문제 정의 및 해결 방법

### 1.1 해결하고자 하는 문제  
그래프 구조 입력에서  
-  단일 클래스 분류를 넘어,  
-  경로(path), 논리식(formula) 등 **순차적 출력(sequence output)** 생성  

이전 GNNs는 단일 그래프 레벨 또는 노드 레벨 출력을 다뤘으나, 시퀀스 출력 문제에는 적합하지 않음.[1]

### 1.2 제안 방법  
1) **Gated Graph Neural Networks (GG-NNs)**  
   - 전파 단계: GRU 업데이트 방식을 도입해 T 스텝 동안 state $$h_t^v$$를 전개하고, 역전파로 학습.  
   - 노드 어노테이션 $$x^v$$: 입력 특수 노드 표시 후 초기 hidden state에 삽입.[1]
   - 출력 모델:  
     -  노드 선택: $$o_v = g(h_T^v, x^v)$$, softmax  
     -  그래프 수준: $$h_G = \tanh\!\bigl(\sum_v i(h_T^v,x^v)\bigr)$$.[1]

2) **Gated Graph Sequence Neural Networks (GGS-NNs)**  
   - 각 시퀀스 단계 $$k$$:  
     a. 입력 어노테이션 행렬 $$X^k$$로 GG-NN 전파  
     b. 출력을 위한 GG-NN $$F^k_o$$와 다음 단계 어노테이션 예측 $$F^k_X$$ 적용  
   - 중간 상태 $$X^{k+1}$$를 순차적 입력으로 활용.[1]

3) **학습 설정**  
   - 관측된 중간 어노테이션(예: 프로그램 검증) 활용 시 별도 GG-NN 학습  
   - 은닉 어노테이션: 전체 시퀀스를 역전파해 end-to-end 학습  

***

## 2. 모델 구조와 수식

### 2.1 GG-NN 전파(recursion)  

$$
\begin{aligned}
h_1^v &= [\,x^v; \,0\,],\\
a_t^v &= A_\text{out} \sum_{u\in\mathrm{IN}(v)} h_{t-1}^u + A_\text{in} \sum_{u\in\mathrm{OUT}(v)} h_{t-1}^u,\\
z_t^v &= \sigma(W_z\,a_t^v + U_z\,h_{t-1}^v),\\
r_t^v &= \sigma(W_r\,a_t^v + U_r\,h_{t-1}^v),\\
\tilde h_t^v &= \tanh(W\,a_t^v + U\,(r_t^v \odot h_{t-1}^v)),\\
h_t^v &= (1 - z_t^v)\odot h_{t-1}^v + z_t^v \odot \tilde h_t^v.
\end{aligned}
$$

– $$A_{\text{out}}, A_{\text{in}}$$는 엣지 타입·방향별 매개변수 공유 구조.[1]

### 2.2 GGS-NN 시퀀스 출력  
각 단계 $$k$$:  
- **상태 갱신**: $$H_{k,1} \leftarrow X^k$$  
- **출력 예측**: $$\,o_k = F^k_o(H_{k,T})$$  
- **어노테이션 갱신**: $$X^{k+1} = F^k_X(H_{k,T})$$.[1]

***

## 3. 성능 및 한계

### 3.1 성능 향상  
- **bAbI Task 19 (경로 탐색)**: RNN/LSTM ~25% vs. GGS-NN 99% (250 예) / 71% (50 예).  
- **최단 경로·오일러 회로 학습**: RNN/LSTM 10% 미만 vs. GGS-NN 100% (50 예).  
- **프로그램 검증 (분리 논리식)**: 수동 피처 없이 89.96% vs. 기존 89.11%.

### 3.2 모델 한계  
- **비구조적 입력**: 자연어→그래프 변환 없이 바로 적용 어려움.  
- **질문 지향적 전파**: 모든 사실을 일단 전파한 뒤 질문 처리 방식은 비효율적.  
- **시계열 그래프**: 시간 축 변경에 따른 동적 전파 방식 연구 필요.[1]

***

## 4. 일반화 성능 향상 가능성

- **어노테이션**을 통해 다양한 문제 도메인에서 내재 지식(encoded state) 반영  
- **GRU 전파**로 장기 의존성 보존, 비선형 GNN 대비 정보 손실 완화$$$$.[1]
- **모듈화된 GG-NN 구성**: 문제별 서브그래프-시퀀스 모델링에 적합  
- **배치 예측(batch prediction)**: 다중 그래프 상태 동시 처리로 견고성 강화.[1]

***

## 5. 향후 연구 영향 및 고려 사항

1. **자연어→그래프** 파이프라인 통합: 언어 이해와 그래프 신경망 연결  
2. **질문 주도 전파**: 입력 전부가 아닌 필요한 부분만 선택적 확산  
3. **고차 관계(ternary 이상)** 모델링: 인수 간 상호작용 확장  
4. **동적·대규모 그래프**: 시간적 변화 및 대규모 네트워크 적용  

이 논문은 그래프 구조와 딥러닝의 결합 가능성을 제시하며, 특히 복합적 시퀀스 출력 문제에 대한 새로운 접근을 제공한다. 앞으로 **지식 베이스, 프로그램 분석, 분자 구조 예측** 등 다양한 분야에서 GGS-NN의 **귀납적 편향과 모듈성**을 활용한 연구가 활발히 진행될 것으로 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ecf6d583-b394-4611-aa8a-a982bfed258a/1511.05493v4.pdf)
