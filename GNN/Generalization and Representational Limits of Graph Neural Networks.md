# Generalization and Representational Limits of Graph Neural Networks

### 1. 핵심 주장과 주요 기여

이 논문은 **그래프 신경망의 근본적인 두 가지 한계**를 다룹니다. 첫째, 국소 정보에만 의존하는 GNN들(메시지 전달 모델, GraphSAGE, GCN, GIN, GAT 등)은 **특정 중요한 그래프 속성을 계산할 수 없다**는 것을 입증했습니다. 둘째, GNN의 **일반화 성능을 처음으로 데이터 기반 방식으로 분석**하는 경계값(bounds)을 제시했습니다.[1]

논문의 주요 기여는 다음과 같습니다:

**표현 한계**: 지름(diameter), 최단/최장 순환(girth/circumference), 클리크(clique) 정보 등 중요한 그래프 특성을 구분할 수 없는 GNN 들이 존재함을 증명했습니다. 이는 전통적인 메시지 전달 기반 GNN뿐만 아니라, **포트 번호 지정(port numbering)을 활용하는 CPNGNN(Consistent Port Numbering GNN)과 방향성 메시지 전달을 사용하는 DimeNet**에도 적용됩니다.[1]

**그래프-이론적 형식화**: 새로운 "포트-국소 동형성(port-local isomorphism)" 개념을 도입하여 CPNGNN의 표현 한계를 분석하는 novel한 형식화를 제시했습니다.[1]

**향상된 일반화 경계**: 기존의 VC-차원 기반 경계보다 **훨씬 더 타이트한 라데마허 복잡도(Rademacher complexity) 기반 경계**를 제공했습니다. 이 경계는 순환신경망(RNN)의 라데마허 경계와 유사한 매개변수 의존성을 보입니다.[1]

### 2. 해결하고자 하는 문제

이 연구는 GNN의 두 가지 중요한 미해결 질문에 답하려고 합니다:

1. **표현 문제**: GNN이 정말로 그래프의 중요한 속성들을 구분할 수 있는가? 국소 정보만 사용하는 GNN의 기본적 한계는 무엇인가?

2. **학습 성능 문제**: GNN이 훈련 데이터에서 미래의 보이지 않는 그래프들로 얼마나 잘 일반화할 수 있는가?

### 3. 제안하는 방법과 모델 구조

#### 3.1 국소 순열 불변 GNN(LU-GNN)

가장 기본적인 GNN들(GraphSAGE, GCN, GIN, GAT)은 다음 형태의 업데이트 식을 따릅니다:[1]

$$ \tilde{h}^{(\ell-1)}_v = \text{AGG}\{h^{(\ell-1)}_u \mid u \in N(v)\} $$

$$ h^{(\ell)}_v = \text{COMBINE}\{h^{(\ell-1)}_v, \tilde{h}^{(\ell-1)}_v\} $$

여기서 N(v)는 노드 v의 이웃 집합이고, AGG는 합(sum) 같은 순열 불변 함수입니다.

#### 3.2 포트 번호 지정 GNN (CPNGNN)

CPNGNN은 각 노드의 이웃들을 1부터 degree(v)까지 번호 매겨 **순서가 있는 정보**를 활용합니다. 포트 순서 함수 p는 각 간선(u,v)에 (i,j) 쌍을 할당하여 "u가 포트 i를 통해 v와 연결"됨을 나타냅니다.[1]

#### 3.3 방향성 메시지 전달 (DimeNet)

DimeNet은 분자 그래프를 위해 설계된 모델로, 메시지 간의 각도를 활용합니다:[1]

$$ m^{(\ell)}_{uv} = f_1(m^{(\ell-1)}_{uv}, \tilde{m}^{(\ell-1)}_{uv}) $$

$$ \tilde{m}^{(\ell-1)}_{uv} = \sum_{w \in N(u) \setminus \{v\}} f_2(m^{(\ell-1)}_{wu}, e(uv), a(wu, uv)) $$

#### 3.4 제안된 H-DCPN 모델

논문은 기존 모델들의 한계를 극복하기 위해 **계층적 방향성 메시지 전달 포트 번호 네트워크(H-DCPN)**를 제안합니다. 이는 다음과 같이 정의됩니다:[1]

$$ h^{(\ell)}_v = f(h^{(\ell-1)}_v, m^{(\ell-1)}_{c_v(1)v}, t_{1,v}, \ldots, m^{(\ell-1)}_{c_v(d(v))v}, t_{d(v),v}) $$

여기서 $$c_v(j)$$는 v의 포트 j에 연결된 이웃이고, 평면 간의 각도 $$\Phi_{uv}$$를 통합합니다.

### 4. 표현 한계의 핵심 증명

#### 4.1 LU-GNN의 한계 (Proposition 1)

논문은 LU-GNN이 구분하지 못하지만 CPNGNN이 구분할 수 있는 그래프 쌍을 구성합니다: 서로 다른 포트 번호가 있는 두 개의 삼각형(G)과 단일 6-사이클(G'). LU-GNN은 포트 정보를 활용하지 않으므로 각 노드의 이웃을 구분할 수 없습니다.[1]

#### 4.2 CPNGNN의 포트 번호 의존성 (Proposition 2)

포트 번호 선택이 중요함을 보여줍니다. **같은 그래프 쌍이라도 다른 포트 번호 지정으로는 CPNGNN이 구분하지 못할 수 있습니다**.[1]

#### 4.3 주요 불가능성 결과 (Proposition 4, 5)

**Proposition 4**: CPNGNN은 특정 포트 번호 지정에서 다음 그래프 속성을 결정할 수 없습니다:
- 지름(diameter)과 반경(radius)
- 최단/최장 순환 길이
- 순환 개수
- k-클리크 존재 여부

구체적으로, 4-사이클 두 개(총 8개 노드)와 8-사이클은 같은 노드 임베딩을 생성하므로 구분 불가능합니다.[1]

**Proposition 5**: DimeNet도 유사한 한계가 있습니다. 정육면체 위에 겹쳐진 사이클들의 구성을 통해, DimeNet이 국소 거리와 각도가 동일하지만 전역 구조가 다른 그래프들을 구분할 수 없음을 보여줍니다.[1]

### 5. 일반화 성능과 경계값

#### 5.1 라데마허 복잡도 기반 경계

논문은 **첫 번째로 메시지 전달 GNN의 데이터 기반 일반화 경계**를 제시합니다. 핵심 아이디어는 GNN의 복잡성을 **국소 노드-별 계산 트리(computation tree)**의 복잡성으로 줄이는 것입니다.[1]

이진 분류를 위한 마진 손실함수를 사용합니다:

$$ \text{loss}_\gamma(a) = \mathbf{1}[a > 0] + (1 + a/\gamma)\mathbf{1}[a \in [-\gamma, 0]] $$

#### 5.2 주요 경계 (Proposition 7)

복잡성이 C = $$C_\rho C_g C_\phi B_2$$인 경우, 라데마허 복잡성의 경계는:[1]

$$ \hat{R}_T(\mathcal{J}_\gamma) \leq \frac{4}{\gamma m} + \frac{24rB_\beta Z}{\gamma\sqrt{m}}\sqrt{3\log Q} $$

여기서:
- r: 임베딩 차원
- m: 샘플 개수
- B_\beta, Z, M: 경계값 매개변수
- Q: 로그 커버링 수의 계수
- L: 층 깊이
- d: 가지 수(branching factor)

#### 5.3 RNN과의 비교

이 논문의 경계는 깊이 L, 차원 r, 샘플 수 m에 대해 RNN의 라데마허 경계와 **거의 동일한 의존성**을 가집니다. 주요 차이는 가지 수 d에 대한 추가 의존성입니다:[1]

| 복잡성 C | GNN (이 논문) | RNN (기존) |
|---------|-----------|----------|
| < 1/d | $$\tilde{O}\left(\frac{rd}{\sqrt{m}\gamma}\right)$$ | $$\tilde{O}\left(\frac{r}{\sqrt{m}\gamma}\right)$$ |
| = 1/d | $$\tilde{O}\left(\frac{rdL}{\sqrt{m}\gamma}\right)$$ | $$\tilde{O}\left(\frac{rL}{\sqrt{m}\gamma}\right)$$ |
| > 1/d | $$\tilde{O}\left(\frac{rd}{\sqrt{rL}\sqrt{m}\gamma}\right)$$ | $$\tilde{O}\left(\frac{r}{\sqrt{rL}\sqrt{m}\gamma}\right)$$ |

#### 5.4 VC-경계와의 비교

이전 Scarselli et al. (2018)의 VC-차원 기반 경계는 $$\tilde{O}(r^3N/\sqrt{m})$$의 오류(N은 그래프 노드 수)를 제공했으나, **이 논문의 경계는 d에만 의존하여 N≫d일 때 훨씬 더 타이트합니다.**[1]

#### 5.5 국소 순열 불변성의 역할

핵심 기술적 기여는 **국소 순열 불변성의 명시적 활용**입니다. 각 노드에서 순열 불변 집계(sum 등)를 사용하므로, 그 구조를 활용하여 Dudley의 엔트로피 적분을 통해 더 타이트한 경계를 도출합니다.[1]

### 6. 성능 향상 및 한계

#### 6.1 성능 향상

**H-DCPN의 강점**:
- 포트 번호 지정과 기하학적 정보(평면 간 각도)를 결합
- 기존 구성(construction)에서 실패하는 경우들을 극복
- 더 많은 그래프 속성을 구분 가능

#### 6.2 근본적 한계

**여전히 불가능한 작업**:
1. 어떤 GNN 변형도 **전역 구조**에 완전히 의존하는 속성(예: 그래프가 이분 그래프인지 여부)을 결정할 수 없습니다.
2. 깊이가 제한된 GNN은 거리가 원점에서 멀리 있는 노드들의 상호작용을 포착할 수 없습니다.
3. **포트 번호 선택**이 임의적이므로, 최적의 포트 번호를 찾기 위한 계산 복잡성은 매우 높습니다.

#### 6.3 일반화 성능의 트레이드오프

**표현력과 일반화의 균형**:
- 더 표현력 있는 모델(H-DCPN, 고차 GNN)은 더 많은 매개변수를 가지므로 과적합 위험이 증가
- 경계값은 깊이 L과 복잡성 C에 대해 지수적으로 증가

### 7. 최신 연구(2024-2025)에서의 영향 및 향후 방향

#### 7.1 논문의 영향

이 논문은 GNN 이론 커뮤니티에서 **429회 이상의 인용**을 받았으며, 여러 중요한 후속 연구를 촉발했습니다.[2]

#### 7.2 현재 연구 동향

**고차(Higher-Order) GNN으로의 확장**:
2025년 최신 연구는 **Petri Graph Neural Networks(PGNN)**과 같은 고차 구조를 학습할 수 있는 모델들을 제시합니다. 이들은 하이퍼그래프, 다층 네트워크, 시간 동적 네트워크 등 전통적인 쌍별 관계를 넘는 구조를 지원합니다. 이는 이 논문의 "국소 정보의 한계"를 극복하려는 노력입니다.[3]

**Weisfeiler-Lehman 테스트 재검토**:
최근 연구(2024-2025)는 GNN의 표현력을 분석하기 위해 Weisfeiler-Lehman(WL) 테스트를 사용하는 표준 접근법의 한계를 지적합니다. 구체적으로, WL 테스트는 그래프의 순수 구조적 동등성만을 검증하므로 **노드 특성이 있는 그래프**에는 적합하지 않으며, WL의 해시 함수들이 국소 계산 가능하지 않다는 점을 강조합니다.[4]

**동적 그래프와 시간 GNN**:
2024년 연구는 동적 그래프를 위한 **고차 표현력 동적 GNN(HopeDGN)**을 제안했으며, 1-DWL을 넘어 2-DWL 수준의 표현력을 달성합니다.[5]

**일반화 경계의 정제**:
2024년 연구들은 다음을 포함한 다양한 설정에서 더 정교한 일반화 경계를 제시합니다:[6][7][8]
- 대규모 데이터 증강을 통한 일반화 개선
- 적대적 견고성을 고려한 PAC-Bayes 경계
- 깊은 GCN의 안정성과 일반화
- 진화하는 그래프에서의 시간 일반화

#### 7.3 향후 연구 고려사항

**1. 이론-실무 간극의 해소**
2024년 위치 논문은 **"그래프 기계학습의 균형잡힌 이론"**이 필요함을 강조합니다. 즉, 표현력, 일반화, 최적화 학습성(learnability with SGD)의 상호작용을 함께 고려해야 합니다.[9]

**2. 국소 계산 가능성 강화**
- 포트 번호 지정과 같은 구조 정보를 **학습 가능하게** 만들기
- CPNGNN의 포트 선택 의존성 문제 해결

**3. 전역 속성 학습**
국소 정보로는 불가능한 전역 그래프 속성(예: 평면성, 이분성)을 효율적으로 학습하는 방법 탐색. 고차 GNN과 주의 메커니즘(attention)의 결합이 유망한 방향입니다.[3]

**4. 실용적 아키텍처**
- 순환 신경망 기반 "폴딩 네트워크"
- 트랜스포머 기반 그래프 모델(예: Graphormers)
- 하이브리드 모델(메시지 전달 + 주의 메커니즘)

**5. 표현력과 계산 효율성 균형**
깊이 L을 증가시키면 표현력이 향상되지만, 일반화 경계는 지수적으로 증가합니다. 이 트레이드오프를 최적화하는 아키텍처 설계가 필요합니다.

**6. 실제 애플리케이션 특성화**
분자 구조, 단백질 상호작용, 소셜 네트워크 등 특정 도메인에서 필요한 표현력 수준을 실증적으로 파악하고, 그에 맞는 최소 복잡성 모델 개발.

### 결론

이 논문은 GNN의 **기본적이고 불가피한 한계**를 명확히 하는 한편, **현실적인 일반화 경계**를 처음으로 제공했습니다. 표현력과 일반화 사이의 내재적 트레이드오프를 이해함으로써, 연구자들은 특정 문제에 맞는 적절한 GNN 아키텍처를 설계할 수 있게 되었습니다. 최근 연구들은 이 한계들을 극복하기 위해 고차 구조, 동적 업데이트, 학습 가능한 아키텍처 매개변수 등을 도입하고 있으며, 이는 GNN 이론을 실제 애플리케이션에 더 가깝게 만드는 방향으로 진행되고 있습니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/45907d1c-f60b-46aa-864f-f62bac8be733/2002.06157v1.pdf)
[2](http://proceedings.mlr.press/v119/garg20c/garg20c.pdf)
[3](https://www.nature.com/articles/s41598-025-01856-9)
[4](https://arxiv.org/html/2410.01308v2)
[5](https://openreview.net/forum?id=iinqdeuA8x)
[6](https://arxiv.org/pdf/2305.08048.pdf)
[7](https://arxiv.org/pdf/2402.04038.pdf)
[8](http://arxiv.org/pdf/2410.08473.pdf)
[9](https://arxiv.org/pdf/2402.02287.pdf)
[10](https://arxiv.org/pdf/2201.07858.pdf)
[11](http://arxiv.org/pdf/2404.04969.pdf)
[12](https://arxiv.org/html/2502.13797v2)
[13](http://arxiv.org/pdf/2411.08638.pdf)
[14](https://www.salzburgresearch.at/wp-content/uploads/2022/01/GNN_Limits.pdf)
[15](https://www.sciencedirect.com/science/article/pii/S0893608024001370)
[16](https://arxiv.org/abs/2002.06157)
[17](https://proceedings.iclr.cc/paper_files/paper/2025/file/ad9d6ab10446114cf5482d5e1f971a84-Paper-Conference.pdf)
[18](https://www.nature.com/articles/s41598-025-90839-x)
[19](https://iris.sissa.it/retrieve/265dd5ce-deed-4439-bac9-89ae97c8fa20/1-s2.0-S0893608024007172-main.pdf)
