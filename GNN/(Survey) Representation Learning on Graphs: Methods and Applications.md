# Representation Learning on Graphs: Methods and Applications

### 1. 핵심 주장과 주요 기여[1]

본 논문은 **그래프 구조를 저차원 벡터 공간으로 인코딩하는 표현 학습(Representation Learning on Graphs)** 분야에 대한 포괄적인 개념적 리뷰를 제시한다. 주요 기여는 다음과 같다.

**핵심 주장**: 전통적인 머신러닝에서는 손으로 설계한 특징(hand-engineered features)을 사용하여 그래프 구조 정보를 추출했으나, 이러한 방식은 학습 과정에서 적응할 수 없고 설계 비용이 높다. 반면, 최신 **표현 학습 방법은 데이터 기반 접근법(data-driven approach)**을 사용하여 그래프 구조를 자동으로 저차원 임베딩으로 학습한다.

**주요 기여**:
- 노드 임베딩과 서브그래프 임베딩 방법을 통일된 인코더-디코더(encoder-decoder) 프레임워크로 체계화
- 얕은 임베딩(shallow embedding), 자기부호화(autoencoder), 이웃 집계(neighborhood aggregation) 방법의 진화 과정을 명확히 제시
- 행렬 인수분해, 무작위 보행(random walk), 그래프 신경망(GNN) 등 주요 기법 통합

***

### 2. 문제 정의, 제안 방법(수식 포함), 모델 구조, 성능 및 한계[1]

#### **2.1 해결하고자 하는 문제**

그래프 기반 머신러닝의 근본적인 문제는 **비유클리드(non-Euclidean) 그래프 구조 정보를 높은 차원의 벡터 공간으로 인코딩하는 방법**이다. 예를 들어:
- 사회 네트워크에서 링크 예측: 노드 간 관계 강도를 인코딩
- 노드 분류: 그래프에서 노드의 전역 위치와 국소 이웃 구조 정보 필요

#### **2.2 통일된 인코더-디코더 프레임워크**

논문의 핵심은 모든 노드 임베딩 방법을 다음의 4가지 구성 요소로 설명하는 것이다:

**기본 구조:**

$$
\text{ENC} : V \rightarrow \mathbb{R}^d \quad \text{(1)}
$$

여기서 $$\text{ENC}(v_i) = z_i$$는 노드 $$v_i$$를 저차원 벡터 $$z_i \in \mathbb{R}^d$$로 매핑한다.

**페어와이즈 디코더:**

$$
\text{DEC} : \mathbb{R}^d \times \mathbb{R}^d \rightarrow \mathbb{R}^+ \quad \text{(2)}
$$

**재구성 목표:**

$$
\text{DEC}(\text{ENC}(v_i), \text{ENC}(v_j)) = \text{DEC}(z_i, z_j) \approx s_G(v_i, v_j) \quad \text{(3)}
$$

여기서 $$s_G(v_i, v_j)$$는 사용자 정의의 그래프 기반 유사도 측도이다.

**손실 함수:**

$$
L = \sum_{(v_i, v_j) \in D} \ell(\text{DEC}(z_i, z_j), s_G(v_i, v_j)) \quad \text{(4)}
$$

여기서 $$\ell : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$$는 사용자가 정의한 손실 함수이다.

#### **2.3 주요 방법론**

**A) 얕은 임베딩 방법 (Shallow Embedding Methods)**

엔코더가 단순한 임베딩 룩업(embedding lookup)으로 정의된다:

$$
\text{ENC}(v_i) = Z_{v_i}
$$

여기서 $$Z \in \mathbb{R}^{d \times |V|}$$는 모든 노드의 임베딩 벡터를 포함하는 행렬이다.

**i) 행렬 인수분해 기반 방법 (Laplacian Eigenmaps, Graph Factorization)**

라플라시안 고유맵:

$$
\text{DEC}(z_i, z_j) = \|z_i - z_j\|_2^2 \quad \text{(6)}
$$

내적 기반 방법 (Graph Factorization, GraRep, HOPE):

$$
\text{DEC}(z_i, z_j) = z_i^T z_j \quad \text{(7)}
$$

$$
L = \sum_{(v_i, v_j) \in D} \|z_i^T z_j - s_G(v_i, v_j)\|_2^2 \quad \text{(8)}
$$

**ii) 무작위 보행 기반 방법 (DeepWalk, node2vec)**

$$
\text{DEC}(z_i, z_j) = \frac{e^{z_i^T z_j}}{\sum_{v_k \in V} e^{z_i^T z_k}} \approx p_{G,T}(v_j|v_i) \quad \text{(10)}
$$

여기서 $$p_{G,T}(v_j|v_i)$$는 $$v_i$$에서 시작하는 길이-T 무작위 보행에서 $$v_j$$를 방문할 확률이다.

손실 함수:

$$
L = \sum_{(v_i, v_j) \in D} -\log(\text{DEC}(z_i, z_j)) \quad \text{(11)}
$$

**node2vec의 핵심 혁신**: 무작위 보행에 편향을 도입하는 두 개의 하이퍼파라미터 $$p$$와 $$q$$를 사용하여 폭-우선 탐색(BFS)과 깊이-우선 탐색(DFS)을 보간한다.

**B) 심층 인코더 기반 방법**

**i) 이웃 자기부호화 (Neighborhood Autoencoder)**

DNGR과 SDNE은 자기부호화를 사용하여 노드 이웃 정보를 압축한다:

$$
\text{DEC}(\text{ENC}(s_i)) = \text{DEC}(z_i) \approx s_i \quad \text{(13)}
$$

$$
L = \sum_{v_i \in V} \|\text{DEC}(z_i) - s_i\|_2^2 \quad \text{(14)}
$$

여기서 $$s_i \in \mathbb{R}^{|V|}$$는 노드 $$v_i$$의 모든 다른 노드에 대한 유사도를 포함하는 고차원 벡터이다.

**ii) 이웃 집계 기반 신경망 (GraphSAGE, GCN)**

반복적 이웃 집계 알고리즘 (Algorithm 1의 개념):

$$
h_v^0 = x_v, \quad \forall v \in V
$$

$$
h_{N(v)}^k = \text{AGGREGATE}_k(\{h_u^{k-1}, \forall u \in N(v)\})
$$

$$
h_v^k = \sigma(W^k \cdot \text{COMBINE}(h_v^{k-1}, h_{N(v)}^k))
$$

여기서 $$\sigma$$는 비선형 활성화 함수, $$W^k$$는 학습 가능한 가중치 행렬이다.

최종 임베딩: $$z_v = h_v^K$$

**주요 특징:**
- 매개변수 공유로 계산 효율성 증가 ($$O(|V|)$$에서 독립적)
- 노드 속성(features) 활용
- 학습 중 관찰되지 않은 노드의 임베딩 생성 가능 (귀납적 학습)

#### **2.4 성능 향상 메커니즘**

**1) 구조적 역할 임베딩 (Structural Role Embedding)**

node2vec는 $$p$$와 $$q$$ 파라미터로 커뮤니티 구조 vs 국소 역할을 거듭제곱 법칙으로 조절한다. struc2vec과 GraphWave는 구조적 동등성을 직접 목표로 한다.

GraphWave 공식:

$$
\psi_{v_i} = UGU^T v_i \quad \text{(20)}
$$

여기서 $$G = \text{diag}([g(\lambda_1), ..., g(\lambda_{|V|})])$$, $$g(\lambda) = e^{-s\lambda}$$는 열 커널이다.

**2) 다중-모달 그래프 처리**

이종 그래프(heterogeneous graph)에서 엣지 타입별 디코더:

$$
\text{DEC}_\tau(z_i, z_j) = z^T A_\tau z \quad \text{(17)}
$$

**3) 다층 그래프 정규화**

$$
L(v_i)' = L(v_i) + \lambda \|z_i^{G_1} - z_i^{G_2}\| \quad \text{(18)}
$$

#### **2.5 성능 비교 및 평가**

논문은 다양한 벤치마크에서 성능을 비교한다:

| 방법 | 노드 분류 | 링크 예측 | 확장성 | 귀납성 |
|------|---------|---------|-------|-------|
| 얕은 임베딩 | 좋음 | 좋음 | 우수 | ✗ |
| 자기부호화 | 우수 | 우수 | 중간 | ✗ |
| 이웃 집계 | **최고** | **최고** | 우수 | ✓ |

#### **2.6 한계점**

**A) 얕은 임베딩 방법의 한계:**
- 매개변수 공유 부재로 인한 통계적 비효율성
- 노드 속성 활용 불가
- 순환적(transductive) 특성: 학습되지 않은 새로운 노드에 대한 임베딩 생성 불가

**B) 자기부호화 방법의 한계:**
- 입력 차원이 $$|V|$$로 고정되어 수백만 노드 규모 그래프에서 실용 불가능
- 순환적 특성 유지
- 동적 그래프에 적응 불가

**C) 일반적 한계:**
- **페어와이즈 디코더의 제한**: 2개 노드 간의 관계만 모델링하며, 3개 이상의 노드 간 고차 구조(motifs) 무시
- **스케일링**: 수십억 개 노드와 엣지를 가진 실제 프로덕션 환경에서의 도전
- **해석 가능성**: 임베딩이 어떤 그래프 정보를 인코딩하는지 불명확
- **동적 그래프 미지원**: 시간 정보를 포함한 동적 그래프에 적합한 방법 부재

---

### 3. 일반화 성능 향상 관련 내용[1]

#### **3.1 일반화 능력 향상 전략**

**A) 매개변수 공유를 통한 정규화 효과**

이웃 집계 방법(GraphSAGE, GCN 등)은 모든 노드에 동일한 집계 함수와 가중치 행렬을 사용하므로:
- 매개변수 차원이 $$O(|V|)$$에 독립적
- 강력한 정규화 효과로 과적합 감소
- **귀납적 학습**: 학습 중 관찰되지 않은 새로운 노드에 대해 임베딩 생성 가능

**B) 노드 속성 활용**

Algorithm 1에서:
- 초기 임베딩: $$h_v^0 = x_v$$ (노드 속성 사용)
- 얕은 임베딩과 달리 노드 고유의 정보(사용자 프로필, 분자 특성 등)를 직접 활용
- 구조적 정보와 노드 속성의 결합으로 더 풍부한 표현 학습

**C) 지도 학습 신호 통합**

노드 분류 작업에서 지도 손실(supervised loss)을 결합:

$$
L = \sum_{v_i \in V} y_i \log(\sigma(\text{ENC}(v_i)^T \theta)) + (1-y_i) \log(1-\sigma(\text{ENC}(v_i)^T \theta)) \quad \text{(16)}
$$

이는 비지도 재구성 손실과 함께 사용되어 임베딩이 다운스트림 작업에 더 잘 맞추어진다.

**D) 다층 그래프 정규화**

OhmNet 방법(식 18)은 다층 그래프에서 동일 노드의 임베딩을 계층 간 일관성으로 제약하여:
- 부분적 관찰 데이터에서의 일반화 개선
- 계층 간 정보 공유로 샘플 효율성 증가

**E) HARP 메타 전략**

그래프 조대화(coarsening)를 통한 계층적 임베딩:
- 조대화된 그래프에서 먼저 임베딩 학습
- 학습된 임베딩을 세밀한 그래프의 초기값으로 사용
- 반복적 프로세스로 지역 최적점 개선

#### **3.2 일반화 성능에 영향을 미치는 주요 설계 선택**

**1) 유사도 측도의 선택:**
- 1차 유사도 ($$s_G(v_i, v_j) = A_{i,j}$$): 직접 연결만 모델링
- 고차 유사도 ($$s_G(v_i, v_j) = A_{i,j}^2$$): 더 원거리 구조 정보 포함
- 무작위 보행 기반: 확률적 유사도로 더 유연한 학습

**2) 이웃 집계 함수:**
- 평균(mean): 계산 효율적
- 최대 풀링(max-pooling): 구별적 특징 추출
- LSTM: 이웃 순서 정보 활용

**3) 깊이(depth) 선택:**
- $$K$$가 클수록 먼 이웃 정보 포함
- 과도한 깊이는 과적합 유발 가능
- Column networks의 보간 기법(식 15)으로 국소 정보 보존:

$$
h_v^{k'} = \alpha h_v^k + (1-\alpha) h_v^{k-1}
$$

#### **3.3 벤치마크 성능 개선**

논문에 따르면, 이웃 집계 방법들이 얕은 임베딩 대비:
- **노드 분류**: 일관되게 우수한 성능
- **링크 예측**: 우수한 성능
- 특히 정점이 속성을 가질 때 성능 향상 극대화

---

### 4. 향후 연구 영향 및 고려사항[1]

#### **4.1 이 논문이 향후 연구에 미치는 영향**

**A) 이론적 기초 구축의 필요성**

논문은 표현 학습이 **통일된 이론적 틀 부재**라는 점을 강조한다. 향후 연구는:
- 그래프 표현이 인코딩해야 하는 구조의 명확한 정의
- 모델이 정보를 인코딩하는 메커니즘에 대한 이론
- 학습된 잠재 공간에 부과할 제약 조건의 이론화

**B) 응용 영역 확대**

현재 벤치마크:
- 노드 분류, 링크 예측, 시각화, 클러스터링

향후 확장 영역:
- 약물 발견, 단백질 함수 예측
- 소셜 네트워크 분석
- 지식 그래프 완성
- 추천 시스템

**C) 아키텍처 혁신의 방향성**

논문의 인코더-디코더 프레임워크는 향후 연구를 위한 **명확한 설계 공간** 제공:
- 새로운 집계 함수 설계
- 대안적 디코더 구조 개발
- 제약 조건 추가

#### **4.2 향후 연구 시 고려할 점**

**1) 스케일링 문제**

현재 대부분의 방법:
- $$O(|E|)$$ 학습 시간 이론적 복잡도
- 실제로는 수백만 노드 그래프에서 메모리/계산 한계

**해결 방안:**
- 배치 처리 전략 개선
- 분산 학습 아키텍처
- 메모리 효율적 샘플링

**2) 고차 구조 모델링**

현재 한계: 페어와이즈 디코더로 2-node 관계만 모델링

향후 개선:
- 삼원소(triplets) 이상의 관계 모델링
- 그래프 모티프(motifs) 직접 학습

**3) 동적 및 시간 그래프**

**미해결 문제**:
- 엣지 추가/삭제의 시간적 역학
- 노드/엣지 속성의 시간 변화

**필요 혁신:**
- 임베딩의 시간 업데이트 메커니즘
- 시간 제약을 명시적으로 인코딩하는 디코더

**4) 대규모 서브그래프 후보 공간 처리**

**현재 한계:**
- 서브그래프 임베딩 방법은 대상 서브그래프를 사전에 지정 필요
- 조합론적으로 대규모 후보 공간 탐색 불가

**향후 방향:**
- 특성을 가진 서브그래프 자동 발견
- 효율적인 후보 공간 탐색 알고리즘

**5) 해석 가능성 강화**

**현재 도전:**
- 임베딩이 어떤 구조 정보를 인코딩하는지 불명확
- 모델 편향 미지수

**필요 연구:**
- 특성맵 시각화 기법 개선 (t-SNE 이상)
- 어탠션 메커니즘 도입
- 구조적 특징과 임베딩의 명시적 연결

**6) 비균형 그래프 및 이질성 처리**

**고려 사항:**
- 실제 네트워크는 거의 항상 이질적(heterogeneous)
- 노드/엣지 타입의 다양성
- 멀티 스케일 구조

**개선 방향:**
- 타입별 특화된 인코더/디코더
- 계층적 구조의 명시적 모델링
- 드물게 관찰되는 노드 타입의 일반화

**7) 벤치마크 표준화**

**논문의 지적:**
- 산재된 벤치마크로 인한 메서드 비교 어려움
- 필드 내 분야별 단절 문제

**필요사항:**
- 통일된 평가 프로토콜
- 다양한 그래프 특성별 벤치마크 스위트
- 클래스 불균형, 동적 특성 등 고려

---

### 결론

이 논문은 **그래프 표현 학습의 포괄적 개념적 리뷰**로서, 다양한 방법들을 통일된 인코더-디코더 프레임워크로 체계화했다. 주요 혁신은 얕은 임베딩에서 심층 신경망 기반 방법으로의 진화를 명확히 보여주었으며, 특히 **매개변수 공유, 노드 속성 활용, 귀납적 학습 능력**이 일반화 성능을 크게 향상시킴을 강조한다. 

그러나 **이론적 기초의 부재, 고차 구조 미모델링, 동적 그래프 미지원, 스케일링 한계**는 향후 연구의 중요한 과제이다. 특히 실제 응용 환경에서의 해석 가능성 강화와 프로덕션 규모의 스케일링이 중요한 병목이 될 것으로 예상된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/eb640386-b9d7-4364-aaae-06bb76c069ad/1709.05584v3.pdf)
