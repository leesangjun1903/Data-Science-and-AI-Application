# AdaGCN: Adaboosting Graph Convolutional Networks into Deep Models

**핵심 주장 및 주요 기여**  
AdaGCN은 그래프 신경망(GNN)의 *깊이*를 효율적으로 확장하기 위해 **AdaBoost** 부스팅 기법을 도입한 RNN-유사 구조의 그래프 컨볼루션 네트워크이다. 각 층이 서로 다른 홉 거리의 이웃 정보(고차원 이웃 특징)를 추출하는 *약한 분류기(base classifier)* 역할을 하며, 부스팅 방식으로 결합해 딥 모델에서도 **고차원 이웃 정보의 효과적 통합**과 **오버스무딩(oversmoothing) 완화**를 동시에 달성한다.[1]

***

## 1. 해결하고자 하는 문제
기존 GCN 기반 모델은 2~3층으로 얕게 설계되어 고차원 이웃의 풍부한 구조 정보를 충분히 활용하지 못한다. 깊이를 늘릴 경우  
1. **오버스무딩**: 반복적 라플라시안 스무딩으로 노드 표현이 수렴하며 판별력이 소실됨.[1]
2. **계산 및 최적화 비용**: 여러 층의 ReLU 및 희소 텐서 연산으로 계산량 급증.

***

## 2. 제안하는 방법

### 2.1. 모델 구조 개요  
- **입력**: 그래프 $$G=(V,E)$$의 노드 특성 행렬 $$X$$와 정규화 인접 행렬 $$A$$.  
- **Base Classifier $$f$$**: 파라미터를 공유하는 2-layer fully-connected 네트워크.  
- **Layer 진행**:  
  1. $$l$$-홉 이웃 정보 행렬 $$A^lX$$ 계산  
  2. $$f_l(A^lX)$$으로 예측 logits $$Z_l$$ 생성  
  3. Multi-class SAMME.R 부스팅 절차로 노드 가중치 $$w_i$$ 업데이트  
- **최종 결합**: $$\displaystyle \hat{C}(A,X)=\arg\max_k\sum_{l=0}^L \alpha_l\,Z_l^{(k)}$$  

이때 부스팅 계수 $$\alpha_l$$은  

$$
\alpha_l = \ln\frac{1-\mathrm{err}_l}{\mathrm{err}_l} + \ln(K-1)
$$

$$\mathrm{err}_l$$은 $$l$$-th 분류기의 가중치 오분류율.[1]

### 2.2. 수식 요약  
- **Simplified Graph Convolution**: $$Z_l = f(A^lX)$$  
- **노드 가중치 업데이트**:  

$$
  w_i \leftarrow w_i \exp\Bigl(\alpha_l \cdot \mathbf{1}[y_i \neq h_l(x_i)]\Bigr)
  $$

- **결합 예측**:  

$$
  \hat{y}_i = \arg\max_k \sum_{l=0}^L \alpha_l\,h_l^{(k)}(x_i)
  $$

### 2.3. 오버스무딩 방지  
ReLU 제거와 부스팅 기반 가중치 조정으로 층이 깊어져도 노드 표현이 무작위로 스무딩되는 속도를 늦춘다. 층별 예측을 부스팅 결합함으로써 얕은 GCN에 비해 **층 증가 시 예측 성능이 꾸준히 향상**됨을 보여준다.

***

## 3. 모델 구조 비교 및 이론적 연결
- **GCN/SGC/JK 네트워크**: 순차적 레이어 스택 및 스킵 연결 사용.  
- **AdaGCN**: RNN-유사 구조로 매 층 파라미터를 순차 최적화, 부스팅 결합.[1]
- **APPNP 연결**: Personalized PageRank 기반 APPNP는 EMA(지수이동평균) 형태로 해석 가능하며, AdaGCN은 *Adaptive EMA*로 확장.[1]
- **MixHop 대비**: MixHop은 선형적 피처 믹싱+concat, AdaGCN은 비선형 변환+부스팅 합산으로 더 강한 표현력 보장.[1]

***

## 4. 성능 향상 및 한계

### 4.1. 성능 평가  
- **노드 분류 정확도**: Cora-ML(85.97%), Citeseer(76.68%), PubMed(79.95%), MS-Academic(93.17%)로 APPNP 대비 유의미한 향상($$p<10^{-5}$$~$$10^{-9}$$).  
- **레이블율 변화**: 레이블이 적을수록 AdaGCN의 성능 이점이 더 커지며, 소수 라벨 환경에서도 강건함을 입증.  
- **대규모 그래프**: Reddit 데이터셋에서 F1=95.39%로 GCN/APPNP 상회, 에포크당 학습 시간 32ms로 월등히 빠름.

### 4.2. 계산 효율  
- **희소 텐서 연산 최소화**: 각 층에서 $$A^lX$$만 선계산해 네트워크 학습 시 희소 연산 배제.  
- **층별 시간 복잡도**: 층 수 증가에도 학습 시간 거의 일정, GCN 대비 선형적 증가 회피.

### 4.3. 한계  
- **부스팅 깊이 선택**: 과도한 층 수 시 VC-차원 증가로 과적합 위험. Depth $$L$$는 교차검증으로 결정 필요[Appendix A.7].  
- **비독립 데이터 문제**: 부스팅은 i.i.d. 가정하나, 그래프의 종속성에도 -믹싱 이론을 통해 수렴성 보장 가능[Appendix A.5].

***

## 5. 일반화 성능 향상 관점
AdaGCN의 부스팅 방식은 각 층의 **오류에 기반한 가중치 재조정**으로, 노드 분포 편향을 완화해 **레이블 희소 환경**에서도 일반화 성능을 크게 개선한다. 더불어, 비선형 $$f$$를 사용함으로써 고차원 이웃 정보의 표현력을 높여 **다양한 그래프 구조**에 적응 가능하다.

***

## 6. 연구의 영향 및 향후 고려 사항
AdaGCN은 부스팅 이론을 GNN에 접목한 선구적 시도로, 이후 연구에서 다음을 고려할 수 있다:

- **다양한 부스팅 알고리즘 적용**: SAMME.R 외에도 XGBoost, LightGBM 등의 아이디어 도입.  
- **하이브리드 구조**: attention 메커니즘과 부스팅 결합으로 정교한 이웃 중요도 학습.  
- **동적/시계열 그래프**: 부스팅 기반 순차 최적화가 시간 종속 그래프 학습에 효과적일지 검토.  
- **스케일업**: 레벨별 샘플링 및 분산 학습으로 수억 노드 규모 적용 가능성 탐색.

이로써 AdaGCN은 **딥 GNN 설계**에 새로운 방향을 제시하며, 차후 그래프 구조 학습 및 일반화 연구에서 부스팅 기반 접근을 적극 고려해야 함을 시사한다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ac4d8f98-5504-4be3-9f2c-9a4a271cdcbe/1908.05081v3.pdf)
