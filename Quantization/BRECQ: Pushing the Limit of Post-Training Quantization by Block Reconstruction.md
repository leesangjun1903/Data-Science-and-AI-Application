# BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction

## 1. 핵심 주장 및 주요 기여
BRECQ는 **사후(포스트) 양자화(Post-Training Quantization; PTQ)**에서 낮은 비트(bitwidth)까지 안정적으로 모델을 압축하기 위해, 네트워크를 블록 단위로 재구성(block reconstruction)하여 **2‐비트(weight) 양자화**를 달성한 최초의 방법이다.  
주요 기여:
- **블록 단위 재구성**: 레이어 간 상호 의존성을 고려하면서 과적합을 억제해, 2–4비트 양자화 시에도 높은 정확도를 유지.  
- **이차 오차(second-order error) 이론적 분석**: Gauss-Newton 근사 및 Fisher 정보 행렬(FIM) 기반으로 블록 출력 재구성 목표 식 수립.  
- **혼합 정밀도(mixed precision)** 지원: 블록 수준에서 2/4/8비트 조합을 유전 알고리즘으로 탐색, 하드웨어 성능 제약 하에서 최적 비트배치 결정.  
- **광범위 과제에 적용**: ResNet, MobileNetV2, RegNet, NAS 모델, 그리고 COCO 객체 탐지에 이르는 다양한 구조에서 검증.

## 2. 문제 정의·제안 기법·구조·성능·한계

### 2.1 해결하려는 문제
사후 양자화는 전체 데이터 없이 빠르게 모델을 압축하지만, 낮은 비트(≤4비트)에서 **정확도 급락**이 발생한다. 기존 PTQ는 레이어별 독립 재구성(layer-wise)만 수행해, 레이어 간 상관성(off-diagonal Hessian)을 무시함으로써 **2비트 양자화 불가** 문제를 안고 있다.

### 2.2 제안 방법
1) **이차 오차 변환**  
   - 모델 파라미터 θ perturbation ∆θ에 따른 손실 변화:  

$$
       E[L(θ+∆θ)]-E[L(θ)] \approx \tfrac{1}{2} ∆θ^T\bar H(θ)∆θ
     $$  
   
   - Gauss-Newton 근사를 이용해  

$$
       ∆θ^T\bar H(θ)∆θ \approx E\bigl[∆z^{(n)T}H(z^{(n)})∆z^{(n)}\bigr]
     $$  
   
   - 여기서 $$∆z^{(n)}$$은 최종 출력 변화.

2) **블록 재구성(Block-wise Reconstruction)**  
   - 네트워크를 스테이지·레이어 중간 수준인 “블록” 단위로 재구성:  

$$
       \min_{\hat w_{\mathrm{block}}} E\bigl[∆z^{(\ell)T}H(z^{(\ell)})∆z^{(\ell)}\bigr]
     $$  
   
   - FIM 대각 원소 $$\bigl(\partial L/\partial z_i^{(\ell)}\bigr)^2$$로 가중치 부여[Eq.10].

3) **양자화 기법**  
   - **가중치**: AdaRound(적응적 둥림)으로 반올림 정책 학습[Eq.16–17].  
   - **활성화**: 학습 가능한 스텝 사이즈(s) 최적화[Eq.18].

4) **혼합 정밀도 탐색**  
   - 비트벡터 $$c∈\{2,4,8\}^n$$ 최적화  
   - 제약: 하드웨어 성능 $$H(c)≤δ$$  
   - 유전 알고리즘으로 2비트 조합만 고려, 탐색 공간 대폭 축소(3ⁿ→2ⁿ).

### 2.3 모델 구조
- **블록 단위**: ResNet 계열의 잔차 블록, MobileNetV2의 bottleneck 블록 등 “기본 building block” 하나씩 재구성.  
- **첫/마지막 레이어**: 블록 단위에 포함되지 않는 레이어는 레이어 단위 재구성.

### 2.4 성능 향상
- **2비트 PTQ**: ResNet-18 top-1 71.08→66.30% (−4.78%), 기존 PTQ는 0–55% 급락.  
- **4비트 PTQ**: ResNet-18 71.08→70.70% (−0.38%).  
- **완전 양자화(activations 4비트)**도 기존 대비 +2–10% 상대 우위.  
- **객체 탐지(COCO)**: Faster R-CNN/ResNet-18 mAP 34.55→34.34% (−0.21%).  
- **혼합 정밀도**: 동일 모델 사이즈·지연(latency) 기준, 2비트 대비 10% 이상 정확도 ↑.

### 2.5 한계
- **데이터 필요성**: 1024개 캘리브레이션 이미지 요구(Zero-shot PTQ보다 데이터 의존).  
- **블록 정의 의존**: CNN 블록 구조에 특화, Transformer 등 비블록 구조에 적용 어려움.  
- **Hessian 근사 한계**: FIM 대각 근사는 여전히 근사이며, 복잡한 상관성 완전 반영 불가.

## 3. 일반화 성능 향상 관점
- 블록 단위로 재구성하여 “과적합”을 줄이는 **바이어스-분산(bias-variance) 트레이드오프**를 최적화.  
- 레이어별(과도한 분산)·네트워크 전체(과도한 바이어스) 양쪽 단점을 보완.  
- FIM 기반 가중 재구성으로 “중요한 출력 채널”에 집중, **소량 데이터**로도 우수한 **검증 성능** 달성.

## 4. 향후 연구 영향 및 고려 사항
- **블록 재구성 아이디어 일반화**: Transformer, 그래프 신경망 등 다양한 구조에 맞춘 “유닛” 정의 및 재구성 확대.  
- **더 정교한 이차 정보 활용**: 대각+비대각 혼합 근사나 저순위 Hessian 모델 도입.  
- **데이터 효율 개선**: 소량 데이터 없이 Zero-shot 방식과 블록 재구성 결합.  
- **하드웨어 매칭**: 2비트 이하·비정형연산 지원 가속기 설계와 동시 탐색.  
- **안정성·보안**: 양자화 오류가 모델의 예측 불확실성에 미치는 영향 분석.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/dd6da809-a1f3-4fa4-acbd-c1ec59348198/2102.05426v2.pdf)
