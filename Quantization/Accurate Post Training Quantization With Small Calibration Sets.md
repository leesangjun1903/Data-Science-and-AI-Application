# Accurate Post Training Quantization With Small Calibration Sets

## 1. 핵심 주장과 주요 기여
이 논문은 소규모의 미라벨(무라벨) 보정(calibration) 데이터만으로도 4비트 이하의 **포스트 트레이닝 양자화**를 안정적으로 수행할 수 있음을 보인다. 주요 기여는 다음 세 가지이다.  
- **AdaQuant**: 레이어별(또는 블록별)로 가중치와 활성화의 양자화 파라미터를 보정 집합에 맞춰 최적화하여 MSE를 최소화.  
- **정수계획(Integer Programming) 기반 비트 할당**: 모델의 전체 정확도 저하 범위 내에서 각 레이어의 비트폭을 최적으로 배분해 성능(속도·전력·압축률)을 극대화.  
- **Para-Normalization & Bias Tuning**: 양자화 후 배치정규화 통계와 바이어스를 재수집·보정해 편향을 제거, 추가 정확도 회복.

## 2. 문제 정의 및 제안 기법

### 2.1 해결하고자 하는 문제
- **데이터·시간 제약**: QAT(Quantization-Aware Training)은 대량의 라벨링된 데이터와 긴 재학습 시간이 필요해 실무 적용이 어려움.  
- **8비트 이하 한계**: 기존 포스트 트레이닝 양자화는 보정 데이터가 작을수록 8비트 이하에서 심각한 정확도 저하 발생.

### 2.2 제안 방법

1) AdaQuant (식 (2), (3))  
   - 레이어별로 원본 출력 $$WX$$와 양자화 출력 $$Q_{\Delta_w}(W+V)\,Q_{\Delta_x}(X)$$ 사이의 MSE를 최소화:  

$$
       \{\hat\Delta_w,\hat\Delta_x,\hat V\}
       = \arg\min_{\Delta_w,\Delta_x,V}\big\|WX - Q_{\Delta_w}(W+V)\,Q_{\Delta_x}(X)\big\|^2.
     $$  
   
   - **Parallel**: 모든 레이어 독립 최적화  
   - **Sequential**: 앞 레이어 양자화 오차를 고려해 순차 최적화  

2) 정수계획 기반 비트 할당 (식 (4a)–(4d))  
   - 이진 변수 $$I_{l}^{k,n}$$를 사용해 레이어 $$l$$를 $$k$$-비트 가중치, $$n$$-비트 활성화로 설정  
   - 전체 성능 개선 $$\sum_l\Delta P_l$$을 최대화하되, 정확도 저하 $$\sum_l\Delta L_l \le \Delta L$$ 제약 만족  

3) Para-Normalization  
   - 양자화 전 fusing된 BatchNorm 레이어를 재구성 후 보정 집합으로 running-mean/variance 업데이트  
   - 재융합 시 스케일·오프셋 및 바이어스 조정으로 통계 편향 제거  

4) Bias Tuning  
   - KD(Knowledge Distillation) 손실을 최소화하도록 보정 집합만으로 바이어스만 재학습  

## 3. 모델 구조 및 성능 향상
- **Vision**: ResNet-18/50/101, MobileNet-V2  
- **NLP**: BERT-Base (SQuAD1.1)  
- **결과**:  
  - ResNet50 4비트 양자화 시 top-1 정확도 75.9%(FP32 대비 1.3%↓) 달성  
  - MobileNet-V2 4비트 양자화 시 top-1 71.6%(0.3%↓) 달성  
  - BERT-Base 8비트 양자화 시 F1 88.45%(0.36%↓) 달성  
- **파이프라인**  
  - **Light**: IP + Para-Normalization (수 분 소요, backward pass 불필요)  
  - **Advanced**: AdaQuant + IP + Para-Normalization + Bias Tuning (수 분~10분 내외)

## 4. 일반화 성능 향상 관련 논의
- **소규모 보정 세트에서도 과적합 없이 안정**: AdaQuant은 과도한 파라미터 튜닝 없이 레이어 단위 최적화만으로도 1~10개 샘플 수준에서 분산이 작아짐.  
- **Sequential AdaQuant의 누적 오차 보정**: 앞 레이어 오차를 반영해 순차적으로 최적화함으로써 양자화 과정의 누적 오류를 완화.  
- **Para-Normalization의 보정 효과**: 불완전하게 양자화된 모델에서도 BatchNorm 통계만 재수집하여 1~2% 추가 회복 가능.  
이로써 **소규모 보정 데이터** 만으로 다양한 네트워크에서 높은 **일반화 성능**을 확보할 수 있음을 입증했다.

## 5. 한계 및 향후 연구 고려사항
- **2비트 이하 낮은 비트폭**: 순수 포스트 트레이닝 설정에서 2비트는 여전히 심각한 정확도 저하.  
- **보정 데이터 분포**: 보정 세트가 학습·평가 데이터와 분포 차이가 클 경우 일반화 불확실성 존재.  
- **비용 모델 정확도**: IP에서 사용한 성능·전력 모델이 실제 하드웨어 특성에 완전 일치하지 않을 수 있음.  

## 6. 향후 연구에 미치는 영향 및 고려점
이 논문은 “포스트 트레이닝으로도 4비트 양자화가 실용적”임을 처음으로 체계적으로 제시, **경량 디바이스** 및 **데이터 프라이버시** 환경에서의 모델 배포 가능성을 확장했다.  
향후 연구에서는  
- 더 낮은 비트폭(2비트 이하) 안정화  
- 보정 데이터의 **무라벨 도메인 적응** 기법  
- 실제 **하드웨어 벤치마크 기반 성능 모델링**  
- 양자화 기법과 **다른 경량화(프루닝·지식 증류)** 통합 전략  
등을 고려해 더욱 보편적이고 견고한 초저정밀도 양자화 파이프라인 구축이 기대된다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/ae0c8864-1812-4d3a-8c03-a36bdb02d046/hubara21a.pdf)
