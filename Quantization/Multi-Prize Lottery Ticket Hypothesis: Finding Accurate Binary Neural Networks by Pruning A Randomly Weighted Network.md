# Multi-Prize Lottery Ticket Hypothesis: Finding Accurate Binary Neural Networks by Pruning A Randomly Weighted Network

## 핵심 주장과 주요 기여

이 논문은 기존의 Lottery Ticket Hypothesis를 확장하여 **Multi-Prize Lottery Ticket Hypothesis**를 제안합니다. 핵심 주장은 충분히 과매개화된 랜덤 가중치 신경망이 다음 세 가지 상(prize)을 동시에 달성할 수 있는 부분망을 포함한다는 것입니다:[1]

- **Prize 1**: 학습된 조밀한 네트워크와 비교 가능한 정확도
- **Prize 2**: 추가 학습 없이 Prize 1 달성  
- **Prize 3**: 극단적 양자화(이진 가중치/활성화)에 대한 강건성

주요 기여는 다음과 같습니다:
1. 랜덤 가중치 신경망 내 고정밀 이진 부분망 존재의 **이론적 증명**
2. 이를 찾기 위한 **biprop 알고리즘** 제안
3. CIFAR-10과 ImageNet에서 **최고 성능** 달성 (94.8%와 74.03%)

## 해결하고자 하는 문제

### 문제 정의
기존 신경망 압축 기법들의 한계를 해결하고자 합니다:
- **이진 신경망(BNN) 성능 격차**: 전정밀도 네트워크 대비 현저한 성능 저하
- **훈련 의존성**: 기존 압축 방법들은 대부분 가중치 훈련에 의존
- **계산 비효율성**: 큰 모델을 훈련한 후 압축하는 비효율적 과정

## 제안하는 방법

### 이론적 기반
**정리 1 (비공식적 진술)**: 모든 완전연결 ReLU 목표 네트워크(깊이 ℓ, 너비 n)에 대해, 깊이 2ℓ이고 너비 $$O\left(\frac{ℓn^{3/2}}{\varepsilon} + ℓn\log\frac{ℓn}{\delta}\right)$$인 랜덤 이진 완전연결 네트워크는 확률 (1-δ)로 목표 네트워크를 오차 ε 이내로 근사하는 이진 부분망을 포함합니다.[1]

### biprop 알고리즘
다음 세 가지 핵심 구성요소로 구성됩니다:

1. **이진화**: $$B^* = \text{sign}(W)$$
2. **이득 항**: $$\alpha^* = \frac{\|M \odot W\|_1}{\|M\|_1}$$
3. **가지치기 마스크 업데이트**: 

$$\frac{\partial L}{\partial S_{p,q}^{(j)}} = \frac{\partial L}{\partial U_q^{(j)}} \alpha^{(j)} B_{p,q}^{(j)} \sigma(U_p^{(j-1)})$$

여기서 straight-through estimator를 사용하여 기울기를 근사합니다.[1]

### 모델 구조
두 가지 변형을 제안합니다:
- **MPT-1/32**: 1비트 가중치, 32비트 활성화
- **MPT-1/1**: 1비트 가중치, 1비트 활성화

MPT-1/1의 경우 sign 함수의 기울기 추정을 위해 이차 스플라인 근사를 사용:

$$s'\_t(x) = \frac{2}{t}\left(1 - \frac{|x|}{t}\right)\mathbf{1}_{\{x \in [-t,t]\}}(x)$$

## 성능 향상 및 실험 결과

### 주요 성능 지표
**CIFAR-10**:
- MPT-1/32: 94.8% (기존 최고 대비 1.78% 향상)
- MPT-1/1: 91.9% (이진 네트워크 최고 성능)

**ImageNet**:
- MPT-1/32: 74.03% (기존 최고 대비 0.76% 향상)
- 전정밀도 네트워크보다 우수한 성능

### 매개변수 효율성
- 원본 네트워크 대비 **60-80% 적은 매개변수**
- 더 깊고 넓은 네트워크일수록 더 공격적인 가지치기 가능

## 일반화 성능 향상 가능성

### 과매개화와 일반화의 관계
이 연구는 **과매개화가 일반화에 미치는 긍정적 영향**을 시사합니다:[1]
- 네트워크가 깊고 넓을수록 MPT 성능이 조밀한 학습된 네트워크에 근접
- **압축성과 일반화의 상관관계** 제시: 효과적으로 압축 가능한 네트워크가 더 나은 일반화 성능을 보임

### 표현력과 보편 근사성
MPT는 **보편 근사자(universal approximator)**임을 증명:
- 실값 신경망이 보편 근사자이므로, 이를 근사할 수 있는 희소 이진 네트워크도 보편 근사자
- 극도로 압축된 네트워크도 복잡한 함수를 표현할 수 있음을 시사

## 한계점

1. **이론과 실험 간 격차**: 이론적 결과는 완전연결 ReLU 네트워크에 국한되나, 실험은 합성곱 네트워크에서 수행
2. **초매개변수 민감성**: 최적 성능을 위해서는 신중한 하이퍼파라미터 튜닝 필요
3. **구조적 제약**: MPT-1/1의 경우 특별한 네트워크 구조 수정 필요 (배치 정규화 위치 조정 등)
4. **스케일링 한계**: 매우 큰 모델에서의 확장성은 아직 충분히 검증되지 않음

## 미래 연구에의 영향 및 고려사항

### 연구 패러다임의 전환
1. **훈련 없는 압축**: 가중치 최적화 없이도 고성능 압축 네트워크 획득 가능성
2. **아키텍처 설계 관점**: 과매개화의 긍정적 역할에 대한 새로운 이해

### 향후 연구 방향
1. **이론적 확장**: 합성곱 네트워크와 다양한 활성화 함수로의 이론 확장
2. **효율적 탐색**: 더 빠른 MPT 발견 알고리즘 개발
3. **구조적 최적화**: BNN 전용 아키텍처와의 결합 연구

### 실용적 고려사항
- **에너지 효율성**: 환경 친화적 AI 개발에 기여 가능
- **엣지 컴퓨팅**: 자원 제약 환경에서의 딥러닝 적용 확대
- **민주화**: 대규모 모델 없이도 고성능 달성 가능

이 연구는 신경망 압축과 이진화 분야에서 중요한 이론적 토대를 제공하며, 앞으로 훈련 효율성과 모델 압축 연구의 새로운 방향을 제시할 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/882fc15c-7e86-428e-8a86-7c3937171827/2103.09377v1.pdf)
