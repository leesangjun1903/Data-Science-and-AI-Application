# Learnable Companding Quantization for Accurate Low‐bit Neural Networks

## 1. 핵심 주장 및 주요 기여 요약  
이 논문은 **매우 저비트(2–4비트) 양자화 신경망**에서도 풀프리시전 모델에 근접하는 정확도를 달성하기 위해 다음을 제안한다.  
- **Learnable Companding Quantization (LCQ)**: 비균일(non‐uniform) 양자화 레벨을 모델 학습 과정에서 직접 최적화하는 **학습 가능한 컴패딩(companding) 함수** 도입  
- **Limited Weight Normalization (LWN)**: 양자화 안정성을 높이기 위한 **가중치 표준편차 복원 기법** 제안  
- **LUT 기반 효율적 추론**: 출력 후재양자화(outer bit‐width) 적용으로 **룩업 테이블 메모리**를 최소화하면서 정수 연산 지원  

이로써 2‐비트 ResNet-50이 ImageNet에서 **75.1%** Top-1 정확도를 달성하며(full-precision 대비 격차 1.7%P) 비균일 양자화의 잠재력을 입증한다.

***

## 2. 문제 정의·제안 방법·구조·성능·한계

### 2.1 해결하고자 하는 문제  
- 저비트 양자화 시 **양자화 오차** 증가로 인한 정확도 저하  
- 기존 비균일 양자화(고정 PoT, 클러스터링)나 균일 양자화(LSQ·PACT 등)는 비트 폭이 낮아질수록 성능이 급감함  

### 2.2 제안 방법  
LCQ는 입력값 $$x$$에 대해 세 단계로 처리한다:  
1) **컴패딩(Compressing)**: 학습 가능한 단조증가 분할선형 함수  

```math
   f_{\Theta}(v)=\sum_{k=1}^K \bigl(\gamma_k(v-d_{k-1})+\beta_{k-1}\bigr)\,\mathbf{1}_{[d_{k-1},d_k)}(v)
```

2) **균일 양자화**: $$q_b(v)=\lfloor s\,v\rceil/s$$, $$s=2^{b-1}-1$$  
3) **익스팬딩(Expanding)**: 역함수 적용  

$$
   g(v)=(f_{\Theta}^{-1}\circ q_b\circ f_{\Theta})(v)
   $$

결과 양자화 함수는  

$$
Q_L(x;\alpha,\Theta)=\mathrm{sgn}(x)\,\alpha\,g\bigl(|x|/\alpha\bigr)
$$  

로, $$\alpha$$와 $$\Theta$$를 **손실 함수 기울기**로 공동 최적화한다.

#### Limited Weight Normalization  
가중치 분포를 $$\tilde w=\sigma_w \cdot Q^*\bigl((w-\mu_w)/\sigma_w\bigr)$$로 표준화 후 복원하여, **양자화 전후 스케일 불일치**를 완화한다.

#### LUT 기반 추론  
컴패딩 후 재양자화(bit‐width $$b'>b$$)로 룩업 테이블 크기를 줄이고, 정수 연산만으로 추론 가능하도록 설계한다.

### 2.3 모델 구조  
- CNN(ResNet-20/56, ResNet-18/34/50), MobileNet-V2, RetinaNet 등  
- 첫·마지막 레이어는 8비트 양자화, 나머지는 2–4비트 LCQ 적용  
- 배치정규화로 unsigned 활성화 하한 학습  

### 2.4 성능 향상  
- CIFAR-10 ResNet-20: W2/A2에서 **91.8%** (기존 APoT 91.0% 대비 +0.8%P)  
- ImageNet ResNet-50: W2/A2에서 **75.1%** (기존 APoT 73.4% 대비 +1.7%P)  
- COCO RetinaNet (W4/A4): ResNet-34 백본에서 **36.4 AP** (Auxi 34.7 대비 +1.7)  

### 2.5 한계  
- **LUT 메모리** 오버헤드(수백 바이트~수십 킬로바이트/레이어)  
- **하드웨어 가속기** 미반영: 실제 추론 속도 이점 불명확  
- 매개변수 $$K$$ 증가는 성능 향상이지만 연산 복잡도 증가  

***

## 3. 일반화 성능 향상 가능성  
- **비균일 분포 적응**: 컴패딩 함수가 데이터 분포에 맞춰 양자화 레벨을 학습하므로, 다양한 도메인·입력 분포에 강인함  
- **LWN**으로 초기화 불안정성 해소 → 과적합 억제 가능  
- **재양자화 기법**이 과도한 정밀도 손실 없이 추론 단계 일반화를 지원  

이로 인해 LCQ는 **다양한 네트워크·태스크**에 적용 시에도 일관된 성능 유지 및 일반화 능력 향상을 기대할 수 있다.

***

## 4. 향후 연구 영향 및 고려 사항  
- **하드웨어 공동설계**: FPGA·ASIC 기반 LUT 엑셀러레이터 개발  
- **자동화된 $$K,b'$$ 탐색**: 효율성과 정확도 균형 위해 하이퍼파라미터 최적화  
- **다중 비트폭·혼합정밀도**: 레이어별 동적 비트폭 학습  
- **적응형 컴패딩 구조**: 비선형 차원 축소·신경망 기반 컴패딩 함수 확장  

이 논문은 저비트 양자화 연구에 **비균일 학습 가능 레벨** 개념을 도입함으로써, **양자화-학습 통합 설계**의 새로운 방향을 제시하며, 향후 소형 디바이스 및 엣지 컴퓨팅에 핵심적인 기술적 토대를 마련할 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/4ac04694-77e3-4a24-8c76-fb90fb1bb362/2103.07156v1.pdf)
