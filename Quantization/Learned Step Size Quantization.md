# Learned Step Size Quantization

**핵심 주장**  
Learned Step Size Quantization(LSQ)은 양자화(quantization) 과정에서 각 층의 양자화 구간(step size)을 **학습 가능한 파라미터**로 두고, 손실 함수(task loss)에 민감하게 반응하는 구간별 기울기를 도입함으로써 2–4비트 저정밀(low-precision) 신경망이 풀 정밀도(full-precision) 모델 수준의 정확도를 달성하거나 근접할 수 있음을 보인다.

**주요 기여**  
1. **구간별(step) 기울기(gradient) 근사**: 양자화 함수의 불연속성을 고려하여, 입력값이 양자화 경계에 가까울수록 손실 변화에 민감하게 반응하는 새로운 기울기 근사를 제안.  
2. **기울기 스케일링(gradient scaling) 휴리스틱**: 각 층의 가중치 개수 및 비트 정밀도에 기반해 양자화 스텝 사이즈의 기울기 크기를 조정, 학습 안정성과 수렴을 개선.  
3. **풀 정밀도 수준 성능 달성**: ImageNet 데이터셋에서 3비트 저정밀 모델이 풀 정밀도 정확도에 도달하거나 근접하는 최초의 사례를 제시.  

***

## 1. 해결하고자 하는 문제

딥 뉴럴 네트워크를 저전력·고속 구현하기 위해 가중치(weights)와 활성화(activations)를 2–4비트 등 낮은 정밀도로 양자화하되, **정확도 손실을 최소화**하는 최적의 양자화 매핑(mapping)을 찾는 것이 주요 과제이다. 기존 방식은  
- **고정 스텝 크기** 또는  
- **데이터 통계 기반 스텝 크기 최적화** 혹은  
- **양자화 오차 최소화**에 초점을 맞추지만,  
이들은 실제 **과제 손실(task loss)을 직접 최소화**하지 않아 최적이 아닐 수 있다.

***

## 2. 제안하는 방법

### 2.1 양자화 정의
입력 값 $$v$$를 스텝 크기 $$s$$로 나눈 뒤, 정수 범위 $$[-Q_N, Q_P]$$로 클리핑(clipping)하고 반올림(rounding)하여 양자화된 값 $$\bar v$$를 얻는다. 양자화 후 복원값은 $$\hat v = \bar v \times s$$.

$$
\bar v = \big\lfloor\,\mathrm{clip}(v/s,\,-Q_N,\,Q_P)\big\rceil,\quad
\hat v = \bar v \times s
$$

여기서 $$Q_P, Q_N$$은 비트 정밀도 $$b$$에 따라 $$(2^b-1)$$ 또는 $$(2^{b-1}-1,\,2^{b-1})$$로 결정된다.

### 2.2 구간별 스텝 크기 기울기

스텝 크기 $$s$$에 대한 손실 기울기 $$\partial \hat v / \partial s$$를 다음과 같이 정의하여, 입력 $$v$$가 클리핑 경계나 반올림 경계에 가까울수록 더 큰 기울기가 전달되도록 한다.

$$
\frac{\partial \hat v}{\partial s} = 
\begin{cases}
-\,\dfrac{v}{s} + \big\lfloor\tfrac{v}{s}\big\rceil, 
& -Q_N < \dfrac{v}{s} < Q_P,\\
-\,Q_N, & \dfrac{v}{s}\le -Q_N,\\
Q_P, & \dfrac{v}{s}\ge Q_P.
\end{cases}
$$

### 2.3 기울기 스케일링

가중치 층과 활성화 층마다 스텝 크기 파라미터의 기울기 크기를 다음 휴리스틱으로 조정하여, 가중치 업데이트 크기와 균형을 맞춘다.

$$
g_\text{weight} = \frac{1}{\sqrt{N_W\,Q_P}}, 
\quad
g_\text{activation} = \frac{1}{\sqrt{N_F\,Q_P}}
$$

여기서 $$N_W$$는 가중치 개수, $$N_F$$는 활성화 특성(feature) 개수이다.

### 2.4 학습 절차

- 각 층의 양자화 스텝 크기 $$s$$를 학습 가능한 파라미터로 도입.  
- 순전파(forward) 시 양자화된 값 사용, 역전파(backward) 시 제안된 기울기 근사 전략 적용.  
- 나머지 모델 파라미터는 기존 방식으로 업데이트.  
- 2–4비트 네트워크는 90에폭(epoc), 8비트는 1에폭만 미세조정(fine-tune).  

***

## 3. 모델 구조 및 성능 향상

- **지원하는 아키텍처**: ResNet-18/34/50/101/152, VGG-16bn, SqueezeNext-23-2x  
- **첫/마지막 층**: 항상 8비트 유지  
- **주요 성능**:  
  - 3비트 ResNet-50이 풀 정밀도(32비트) 수준의 Top-1 정확도 달성  
  - 2비트 ResNet-18: 풀 정밀도 대비 정확도 감소 불과 2.9%  
  - 대부분 4비트 이상에서 풀 정밀도와 동등하거나 근접한 성능  

### 한계

- **2비트 네트워크**: 여전히 풀 정밀도 대비 수퍼바이즈드 정확도가 몇 % 포인트 낮음  
- **학습 시간**: 2–4비트 학습 시 90에폭 필요, 8비트만 단 1에폭  
- **하드웨어 종속성**: 제안된 스텝 크기 재스케일링이 모든 저정밀 하드웨어에 최적일지는 미확인  

***

## 4. 일반화 성능 향상 관점

- **과적합 저감**: 비트 수 감소에 따른 양자화 효과가 규제(regularization) 역할을 하여 과적합 위험을 낮춤  
- **지식 증류(Knowledge Distillation) 결합**:  
  - 풀 정밀도 교사 네트워크로부터 학생 네트워크(저정밀 모델)에 지식을 전이하여 최대 1.1% 정확도 추가 향상  
  - 3비트 모델이 풀 정밀도 수준 정확도를 재현 가능  
- **결과**: 저정밀 양자화만으로도 풀 정밀도 수준의 **일반화 성능**을 달성하거나 근접  

***

## 5. 향후 연구에 미치는 영향 및 고려점

- **향후 영향**  
  - 신경망 압축·가속 분야에서 스텝 크기를 학습 파라미터로 도입하는 표준 기법 가능성  
  - 1–2비트 초저정밀 연구 및 생물학적 신경망 비교 연구 가속  
- **연구 시 고려점**  
  - **스텝 크기 학습 안정성**: 기울기 비율 균형 휴리스틱을 더욱 엄밀히 이론화  
  - **비트 수와 모델 구조의 상관관계**: 아키텍처별 최적 비트 정밀도 자동 탐색  
  - **하드웨어 통합**: 실제 저전력·고속 실행 환경에서 제안 방법의 효율성 및 호환성 평가  
  - **더 낮은 비트 한계**: 1비트 이진 양자화에서 풀 정밀도 성능 근접성 여부 탐구

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/7758eebb-6ead-4a2d-82bf-f1427630dcdd/1902.08153v3.pdf)
