# Post-training Quantization on Diffusion Models

## 1. 핵심 주장 및 주요 기여
**핵심 주장**  
본 논문은 대규모 예측 기반 생성 모델인 **Diffusion Model**의 생성 속도를 획기적으로 높이기 위하여, 모델 재학습 없이 사후에 양자화(Post-training Quantization, PTQ)만으로 **32비트 → 8비트** 변환을 수행해도 성능 저하를 최소화할 수 있음을 보인다.

**주요 기여**  
1. **PTQ4DM 제안**: Diffusion Model 구조의 특성을 고려한 새로운 PTQ 파이프라인 ‘PTQ4DM’을 제안.  
2. **시간-스텝 인식 보정(NDTC)**: 노이즈 추정 네트워크 출력 분포가 시간-스텝에 따라 변하는 점을 고려하여, 스텝을 정규분포에서 샘플링해 보정 데이터를 생성하는 NDTC 기법을 개발.  
3. **Training-free 압축 성능**: 추가 학습 없이 8비트 양자화만으로도 FID, IS 지표에서 32비트 모델과 동등하거나 더 나은 성능을 달성하고 실질적 추론 속도 2× 개선을 실증.

***

## 2. 문제 정의
Diffusion Model은 노이즈를 제거하며 반복적으로 샘플을 생성하는 과정에서
- (i) **수천~수만 회의 반복**  
- (ii) **무거운 네트워크 연산**  
두 가지 측면에서 매우 느리며, 특히 엣지 디바이스에서는 배포가 어렵다.  
기존 가속화 연구들은 주로 반복 횟수 단축에만 집중해왔으나, 네트워크 연산 비용을 줄이는 연구는 미흡했다.

***

## 3. 제안 방법

### 3.1. 전반적 PTQ4DM 프레임워크
1. **연산선택**: 주요 합성곱·FC 연산 및 출력 분포(µ, Σ), 샘플링 과정(xt–1)도 모두 8비트 양자화  
2. **보정 데이터 생성**: NDTC 알고리즘으로 시간-스텝 t를 정규분포 $$ \mathcal{N}(\mu,\,(\tfrac{T}{2})^2) $$에서 샘플링한 뒤, 전체 denoising 과정을 거쳐 xt 샘플 수집  
3. **파라미터 보정**: MSE 기준으로 스케일 팩터와 제로 포인트를 최적화  

### 3.2. 시간-스텝 인식 보정 (NDTC)
- **관찰**: 네트워크 출력 분포가 스텝에 따라 크게 변동 → 단일 스텝 보정 불가  
- **NDTC**:  
  - 시간-스텝 $$t_i \sim \mathcal{N}(\mu,(\tfrac{T}{2})^2) $$ (여기서 $$\mu\le\frac{T}{2} $$)  
  - $$x_T \sim \mathcal{N}(0,I) $$에서 시작하여 $$p_\theta(x_{t-1}|x_t) $$로 $$x_{t_i} $$ 생성  
  - 이 과정을 $$N $$회 반복해 보정 집합 $$C=\{x_{t_i}\}_{i=1}^N $$ 구성  

***

## 4. 모델 구조 및 수식
- **Noise Estimation UNet**: 입력 $$(x_t, t) $$ → 출력 $$(\mu_\theta,\Sigma_\theta) $$ → 가우시안 샘플링  
- **양자화 수식**  

$$
    x_{\text{int}} = \mathrm{clamp}\Bigl(\bigl\lfloor\tfrac{x}{s}\rceil - z,\;p_{\min},p_{\max}\Bigr),\quad
    x_{\mathrm{sim}} = s\,(x_{\text{int}}+z)
  $$  

$$
    (s^\ast,z^\ast) = \arg\min_{s,z}\;\mathrm{MSE}\bigl(X_{\mathrm{sim}},X_{\mathrm{fp}}\bigr)
  $$

***

## 5. 성능 향상 및 한계
### 5.1. 성능
- **ImageNet64, DDIM 250**:  
  - 32비트 → IS 14.88 → 8비트 IS 15.88, FID 23.96 (≈32비트 21.63)  
- **CIFAR32, DDPM**:  
  - 32비트 → IS 9.28 → 8비트 IS 9.55, FID 7.10→7.14  
- **추론 속도**: RTX A6000 기준 약 2배 가속

### 5.2. 한계
- **보정 샘플 수급 비용**: 1,024개 샘플 생성에 상당한 denoising 연산 필요  
- **고해상도 이미지**: 64×64 이상 고해상도 모델에서 보정 효율성 추가 검증 필요  
- **정규분포 하이퍼파라미터**: µ, σ 설정 민감도 존재

***

## 6. 일반화 성능 향상 관점
NDTC를 통해 다양한 시간-스텝 분포를 균형 있게 반영함으로써, **모델이 학습되지 않은 양자화 영역에서도 안정적 출력 분포를 유지**한다. 이로 인해:
- **도메인 변화**: 훈련 데이터와 다른 노이즈 스케줄, 다른 스케일의 입력에도 강건  
- **타 모델 적용**: DDIM, DDPM 등 다양한 구조·스케줄에 plug-and-play 방식으로 적용 가능

***

## 7. 향후 연구 영향 및 고려 사항
- **플러그인 가속 모듈**: PTQ4DM은 기존 fast-sampling 기법(DDIM, DPM-Solver 등)에 결합해 복합적인 가속 효과 기대  
- **고해상도·다양 모달리티**: 텍스트-이미지, 오디오, 비디오 확장 시 보정 전략 재설계 필요  
- **경량화 하드웨어 최적화**: NPU·모바일 칩셋에 맞춘 정밀도-성능 균형 제어 연구  
- **보정 분포 최적화**: 정규분포 외 스케일·스킵 노이즈 스케줄에 따른 이론적 분석 및 자동 튜닝 기법 개발    

---  

**결론적으로**, PTQ4DM은 Diffusion Model의 추론 효율을 크게 높이면서도 성능 저하를 최소화하는 **첫 번째 훈련 불필요 양자화 방법**으로, 생성 모델 배포 및 후속 연구의 새로운 표준이 될 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/01bae23c-efda-480b-95a7-1d296ba9ae37/2211.15736v3.pdf)
