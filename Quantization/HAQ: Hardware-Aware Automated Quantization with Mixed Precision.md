# HAQ: Hardware-Aware Automated Quantization with Mixed Precision

## 1. 핵심 주장 및 주요 기여  
**핵심 주장**  
하드웨어의 실제 지연시간(latency)과 에너지 소비를 직접 피드백으로 활용하는 강화학습 기반의 자동화된 혼합 정밀도(1–8비트) 양자화 프레임워크(HAQ)를 제안한다. 이를 통해 각 네트워크 계층별 최적 비트폭을 찾아, 획일적 8비트 양자화 대비 지연시간 1.4–1.95×, 에너지 소비 1.9× 절감하면서 정확도 저하는 거의 없앴다.

**주요 기여**  
- 자동화(Automation): 전문가 개입 없이 강화학습으로 비트 폭 설계를 자동 탐색  
- 하드웨어 인식(Hardware-Aware): FLOPs, 모델 크기 대신 하드웨어 시뮬레이터에서 직접 측정한 지연시간 및 에너지 사용량을 보상으로 활용  
- 특화 설계(Specialization): 엣지(FPGA)와 클라우드 가속기 구조별로 상이한 최적 양자화 정책 제시  
- 설계 인사이트(Design Insights): 계층별 계산 대 메모리 병목 특성에 따른, 예컨대 depthwise vs. pointwise 계층의 비트폭 배분 차이를 해석  

***

## 2. 문제 정의, 제안 기법, 모델 구조, 성능 및 한계

### 문제 정의  
- 딥러닝 모델 추론시 실시간 제약(지연시간, 에너지, 모델 크기)  
- 기존 양자화는 모든 계층에 동일 비트 폭 적용하거나 전문가 규칙에 의존 → 매우 큰 설계 공간(ResNet-50 기준 ~10^100)  
- FLOPs 등 간접 지표는 실제 하드웨어 성능 불일치

### 제안 기법  
1. **강화학습(DDPG) 기반 정책 탐색**  
   - 상태(observation): 각 계층의 입력/출력 채널, 커널 크기, 파라미터 수, 이전 계층 비트폭 등 10차원 벡터  
   - 행동(action): 연속값 $$\in $$ → 비트폭 $$b_k = \mathrm{round}(b_{\min}-0.5 + a_k\times(b_{\max}-b_{\min}+1))$$[1]
   - 보상: 모델 재학습 후 정확도 차이만 반영 $$\;R = \lambda\,(acc_{quant}-acc_{orig})$$  
   - 제약: 지연시간·에너지·모델 크기 예산 초과 시 순차적으로 비트폭 감소  

2. **하드웨어 피드백**  
   - 에지(FPGA Zynq-7020, 소량 배치) 및 클라우드(FPGA VU9P, 대량 배치) 가속기에 대한 시뮬레이터 직접 호출  
   - 프로파일링된 지연시간 및 에너지 값을 강화학습 환경 보상으로 활용  

3. **양자화(Quantization)**  
   - 가중치: $$\mathrm{quantize}(w,b,c)=\mathrm{round}(\mathrm{clamp}(w,c)/s)\times s,\;s=\tfrac{c}{2^{b-1}-1}$$, $$c$$는 KL 발산 최소화로 결정  
   - 활성화: $$[0,c]$$ 범위 선형 양자화  

### 모델 구조  
- 대상 모델: MobileNet-V1/V2 (depthwise separable conv 기반), ResNet-50  
- 에이전트 네트워크: actor-critic 모두 400-300-1 완전연결망, actor 출력은 sigmoid로  매핑[1]

### 성능 향상  
- **지연시간 절감**: MobileNet-V1 8비트 대비 엣지 1.9×, 클라우드 1.4× 가속  
- **에너지 절감**: MobileNet-V1 2× 절감  
- **모델 크기**: Deep Compression 대비 동일 크기에서 Top-1 정확도 최대 +3.5% 향상  

### 한계  
- 강화학습 탐색 비용(모델 수백 회 시뮬레이션 및 재학습)  
- 하드웨어 시뮬레이터 의존성: 실제 실물 가속기와 오차 가능성  
- 오직 이미지 분류 모델에 국한, 자연어·음성 등 다른 도메인 적용성 검증 필요  

***

## 3. 일반화 성능 향상 가능성  
- **혼합 정밀도 활용**: 계층별 비트폭 차등 적용으로 과도한 정보 손실 방지 → 얕은 계층 민감도 반영  
- **하드웨어 제약 내 최적화**: 다양한 메모리 대역폭·병렬도 환경에서 일관된 성능 보장  
- **강화학습 기반 자동화 프레임워크**: 새로운 모델·가속기에 대해 추가 규칙 없이 재탐색 가능  
→ 전반적인 모델 일반화(다양한 아키텍처·하드웨어) 성능 향상에 기여  

***

## 4. 향후 연구 영향 및 고려 사항  
**영향**  
- 소프트웨어·하드웨어 공동 설계(co-design) 패러다임 확장  
- AutoML 분야에서 하드웨어 인식 최적화의 표준화 기여  
- 혼합 정밀도 및 하드웨어 피드백 직접 활용 방안 제시  

**고려 사항**  
- 탐색 효율 개선을 위한 샘플 효율적 강화학습 기법 도입  
- 실제 반영을 위한 하드웨어-시뮬레이터 정합도 향상  
- 비전 이외 도메인(언어·추천시스템)의 특성별 최적화 전략 연구  
- 동적 입력·배치 변동 환경에 대한 적응적 재탐색(neighbor search) 메커니즘 구축

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/56d492a6-049f-4bfe-bf80-0456f833ff77/1811.08886v3.pdf)
