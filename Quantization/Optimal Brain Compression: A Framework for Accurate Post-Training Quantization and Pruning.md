# Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning
# 핵심 요약

**Optimal Brain Compression (OBC)** 논문은 사후 훈련(post-training)만으로도 높은 정확도를 유지하며 딥러닝 모델을 압축하는 통합 프레임워크를 제안한다. 주요 기여는 다음과 같다.  
1. **통합 프레임워크 제안**: 기존에는 별도로 수행되던 가중치 가지치기(pruning)와 양자화(quantization)를 하나의 수학적 틀에서 통합.  
2. **효율적 OBS 적용**: 고전적 Optimal Brain Surgeon(OBS) 방식을 대규모 DNN에 맞게 최적화하여, 계층별(layer-wise)로 정확하고 효율적인 그리디(한 번에 하나의 가중치) 제거·양자화 알고리즘을 개발.  
3. **시간·공간 복잡도 감소**: 직접적 OBS는 Θ(d⁴) 계산 복잡도를 가지나, 제안 기법은 Θ(d_row·d_col³) 시간 및 Θ(d_col²) 메모리로 감소시켜 실제 GPU에서 대형 모델도 1회 처리 가능.  
4. **복합 압축 지원**: 가지치기와 양자화를 동시에 적용해 12× FLOP 감소에도 2% 이하의 정확도 손실, CPU상에서 4× 속도 향상에 1% 손실을 달성.

# 문제 정의 및 제안 방법

## 1. 해결하고자 하는 문제  
- **사후 훈련 압축**: 이미 훈련된 모델과 소량의 보정 데이터(calibration data)만을 이용해 추가적 재훈련 없이 일회성으로 압축  
- **통합 압축**: 가지치기와 양자화를 별도 단계가 아닌 하나의 절차로 병합  

## 2. 수학적 층별 압축 문제  
각 층 ℓ의 가중치 $$W_ℓ\in\mathbb{R}^{d_{row}×d_{col}}$$와 입력 샘플 $$X_ℓ\in\mathbb{R}^{d_{col}×N}$$이 주어질 때,  

$$
\widehat W_ℓ = \arg\min_{W} \|W_ℓ X_ℓ - W X_ℓ\|_2^2
\quad\text{s.t.}\quad C(W) \le C_0,
$$  

여기서 $$C(·)$$는 희소도 또는 비트 수 제약.

## 3. Optimal Brain Surgeon(OBS) 확장  
- **고전 OBS**: Hessian $$H$$의 역행렬 $$H^{-1}$$을 이용해 한 번에 하나의 가중치를 제거할 최적 후보 $$p$$와 나머지 가중치 보정량 $$\delta$$를 계산  

```math
  p = \arg\min_p \frac{w_p^2}{[H^{-1}]_{pp}}, 
  \quad \delta = -\frac{w_p}{[H^{-1}]_{pp}}\,H^{-1}_{:,p}.
```

- **효율적 계층별 OBS (ExactOBS)**  
  1) 각 행(row)마다 $$H=2\,X X^T$$를 한 번만 계산.  
  2) 가중치 제거 시 역행렬 갱신을 Θ(d_col²)로 수행하는 Lemma 1 적용.  
  3) 전 행 결과를 종합해 전역 마스크 결정.

## 4. Optimal Brain Quantizer (OBQ)  
- 양자화 그리드 $$q(w_p)$$로 일반화:  

```math
  p = \arg\min_p \frac{(q(w_p)-w_p)^2}{[H^{-1}]_{pp}}, 
  \quad \delta = -\frac{q(w_p)-w_p}{[H^{-1}]_{pp}}\,H^{-1}_{:,p}.
```

- 한 번에 하나의 가중치를 양자화하고, 나머지를 보정함으로써 계층별 독립 최적화에도 순차 방식과 동등한 성능 달성.

# 모델 구조 및 성능

| 모델       | 압축 기법              | 압축 배율      | 성능 손실          |
|------------|-----------------------|---------------|--------------------|
| ResNet50   | Unstructured pruning  | 4× FLOPs 감소  | 약 1.0%↓           |
| ResNet50   | 2:4 N:M pruning       | 2× FLOPs 감소  | 0.93%↓             |
| ResNet50   | 4-bit 양자화          | –             | 0.41%↓             |
| ResNet50   | 복합(4-bit+2:4 pruning) | 12–14× BOP↓ | ≈2.5%↓            |
| BERT       | 복합(4-bit+2:4 pruning) | 7–8× BOP↓   | ≈2.0%↓            |

*BOP: 가중치 비트 수×연산량

# 한계 및 일반화 성능

- **보정 데이터 의존성**: 소량의 보정 데이터만으로도 충분하나, 입력 분포가 크게 변할 경우 일반화 성능 저하 우려  
- **계층 독립성**: 계층별 독립 최적화로 유연성↑, 순차 최적화 대비 미세한 상호보정 기회 상실 가능  
- **대규모 모델**: 거대 언어 모델 규모에서는 계산 비용·메모리 부담 여전

# 향후 연구 영향 및 고려 사항

- **혼합 정밀도(Mixed Precision)**: 계층별 제약 최적화와 결합해 다양한 하드웨어 제약 하 빠른 모델 탐색 가속  
- **구조적 가지치기 확대**: 블록·채널 수준 구조적 압축과의 결합으로 CPU·가속기 친화적 모델 확보  
- **보정 데이터 최소화**: 더 적은 데이터로도 안정적 일반화를 위한 이론적·경험적 연구 필요  
- **대형 언어 모델 적용**: 수십억 파라미터 규모에 대한 효율적 OBS 근사·병렬화 기술 발전이 핵심

OBC 프레임워크는 복합 압축 실험을 통해 사후 훈련만으로도 재훈련 기법과 버금가는 성능을 보여, 추후 경량화 연구 및 실용화에 중요한 기반이 될 것이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d84983ab-a043-449f-85e6-15487cd45fda/2208.11580v2.pdf)
