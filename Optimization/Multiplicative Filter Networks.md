# Multiplicative Filter Networks

### 1. 핵심 주장과 주요 기여

**Multiplicative Filter Networks**는 저차원이면서 복잡한 함수 근사 문제에서 기존의 깊은 신경망 구조를 재설계한 혁신적 접근법을 제시합니다. 이 논문의 핵심 주장은 다음과 같습니다.[1]

먼저 **구조적 단순화**입니다. 기존의 깊은 신경망이 비선형 함수들의 **합성(compositional depth)**을 통해 표현력을 얻는 반면, MFN은 비선형 필터들의 **곱셈(multiplicative operation)**을 사용합니다. 이는 기본적으로 다층 구조에서 각 층이 비선형 함수의 출력에 다시 비선형 함수를 적용하지 않고, 대신 원래 입력에 직접 적용한 여러 필터들의 선형 결합을 곱하는 방식입니다.[1]

두 번째는 **수학적 해석가능성**입니다. MFN의 가장 중요한 기여는 전체 함수를 명시적으로 계산 가능한 형태로 표현할 수 있다는 점입니다. 특히 Theorem 1과 Theorem 2를 통해, FourierNet은 정확히 정현파 기저 함수들의 선형 결합으로, GaborNet은 Gabor 웨이블릿 기저 함수들의 선형 결합으로 표현되며, 이 과정에서 지수 개의 기저 함수가 생성되지만 다항식 개의 계수로만 제한된다는 점이 증명됩니다.[1]

### 2. 문제 정의, 제안 방법, 모델 구조

#### 문제 정의

기존 연구(SIREN, Fourier Features)는 저차원 입력 공간에서 복잡한 함수를 근사하는 문제, 특히 이미지 좌표를 입력으로 픽셀값을 출력하는 것(f: ℝ² → ℝ³), 미분방정식 풀이, 부호있는 거리장(SDF) 또는 신경 방사 필드(NeRF) 표현 등에서 단순한 ReLU 네트워크가 충분하지 않음을 보였습니다. 하지만 SIREN이나 Fourier Features의 효과가 왜 나타나는지에 대한 명확한 이론적 설명이 부족했습니다.[1]

#### 제안 방법: 수식

**전통적인 깊은 신경망**은 다음과 같이 정의됩니다:[1]

$$
\begin{align}
z^{(1)} &= x \\
z^{(i+1)} &= \sigma(W^{(i)}z^{(i)} + b^{(i)}), \quad i = 1, \ldots, k-1 \\
f(x) &= W^{(k)}z^{(k)} + b^{(k)}
\end{align}
$$

**MFN의 재귀 구조**는 다음과 같습니다:[1]

$$
\begin{align}
z^{(1)} &= g(x; \theta^{(1)}) \\
z^{(i+1)} &= (W^{(i)}z^{(i)} + b^{(i)}) \circ g(x; \theta^{(i+1)}), \quad i = 1, \ldots, k-1 \\
f(x) &= W^{(k)}z^{(k)} + b^{(k)}
\end{align}
$$

여기서 ◦는 요소별 곱셈(Hadamard product)입니다.[1]

#### 모델 구조: FourierNet

**정현파 필터**를 사용합니다:[1]

$$
g(x; \theta^{(i)}) = \sin(\omega^{(i)}x + \phi^{(i)})
$$

여기서 $$\theta^{(i)} = \{\omega^{(i)} \in \mathbb{R}^{d_i \times n}, \phi^{(i)} \in \mathbb{R}^{d_i}\}$$.[1]

**핵심 수학적 성질**은 두 정현파의 곱이 정현파의 합으로 표현된다는 삼각함수 항등식입니다:[1]

$$
\sin(a) \sin(b) = \frac{1}{2}\cos((a-b)) - \frac{1}{2}\cos((a+b))
$$

이를 통해 Corollary 1에서 정확한 계수 형태를 도출할 수 있습니다:[1]

$$
\begin{align}
\bar{\alpha} &= \frac{1}{2^{k-1}} W^{(k-1)}_{i_{k-1}, i_{k-2}} \cdots W^{(2)}_{i_3, i_2} W^{(1)}_{i_2, i_1} \\
\bar{\omega} &= s_k \omega^{(k)}_{i_k} + \cdots + s_2 \omega^{(2)}_{i_2} + \omega^{(1)}_{i_1} \\
\bar{\phi} &= s_k \phi^{(k)}_{i_k} + \cdots + s_2 \phi^{(2)}_{i_2} + \phi^{(1)}_{i_1} + \frac{\pi}{2} \sum_{i=2}^{k} s_k
\end{align}
$$

#### 모델 구조: GaborNet

**Gabor 필터**는 공간 국소성과 주파수 정보를 동시에 포착합니다:[1]

$$
g_j(x; \theta^{(i)}) = \exp\left(-\frac{\gamma^{(i)}_j}{2}\|x - \mu^{(i)}_j\|^2_2\right) \sin(\omega^{(i)}_j x + \phi^{(i)}_j)
$$

여기서 $$\gamma^{(i)}_j$$는 스케일 항, $$\mu^{(i)}_j$$는 중심 좌표입니다. Theorem 2는 GaborNet의 출력도 Gabor 기저 함수들의 선형 결합으로 표현됨을 증명합니다.[1]

### 3. 성능 향상 및 실험 결과

#### 이미지 표현 작업

**이미지 재구성** 실험에서 GABORNET은 10,000 반복 후 73.98 dB의 PSNR을 달성하여 SIREN의 56.54 dB를 크게 상회했습니다. 특히 초기 수렴 속도에서 GABORNET은 1,000 반복만에 다른 모든 모델이 10,000 반복으로도 도달하지 못하는 성능을 보였습니다.[1]

**동영상 재구성**에서는 SIREN이 30.58±0.93 dB로 최고 성능을 보였지만, GABORNET은 29.83±0.71 dB로 SIREN 성능 내 1 dB 이내이면서 프레임 간 일관성이 더 우수했습니다.[1]

#### 이미지 일반화 작업

25% 픽셀만 사용하여 학습하고 전체 이미지에 대해 평가하는 과제에서:[1]

| 방법 | Natural | Text |
|------|---------|------|
| FF Gaussian | 25.57±4.18 | 30.46±1.97 |
| **FOURIERNET** | **26.03±2.77** | **31.02±2.04** |
| **GABORNET** | **26.18±2.95** | **31.19±2.00** |

MFN은 모든 Fourier 특성 기반 네트워크를 초과했습니다. 특히 텍스트 데이터셋에서 기존 방법들이 놓친 글자 부분을 완전히 재구성했습니다.[1]

#### 미분방정식 풀이

Poisson 방정식의 Laplacian 기반 재구성에서 GABORNET은 손실값 107.129로 SIREN의 3432.56을 극적으로 개선했습니다. Helmholtz 방정식(50,000 반복)에서도 GABORNET의 손실(18,845.75)이 SIREN(84,949.92)을 크게 상회했습니다.[1]

#### 3D 형상 표현 및 신경 방사 필드

부호있는 거리 함수(SDF) 표현에서 MFN은 ReLU MLP가 놓친 세부사항(문, 액자, 베개)을 정확히 재구성했으며, SIREN만큼 매끄러운 표면은 아니었지만 뛰어난 성능을 보였습니다. NeRF 기반 3D 역 렌더링 작업에서는 GABORNET이 25.81±0.76 dB의 PSNR으로 다른 방법들과 경쟁적 성능을 보였습니다.[1]

### 4. 일반화 성능 향상 가능성 (중점)

#### 이론적 근거

MFN의 일반화 성능 향상은 **저차원 복잡 함수의 특성**과 **기저 함수 표현의 효율성**에 기인합니다. 전체 함수가 선형 결합으로 표현되기 때문에:[1]

1. **샘플 복잡성 감소**: 다항식 개의 계수만 학습하면 되므로, 과적합 위험이 감소합니다.[1]

2. **주파수 스펙트럼 제어**: Corollary 1의 명시적 계수 형태를 통해 초기화 시 $$\sqrt{k}$$로 스케일링하여 네트워크 깊이에 무관하게 최종 주파수의 분산을 일정하게 유지할 수 있습니다.[1]

3. **지수적 기저 함수**: 곱셈 깊이를 통해 지수 개의 기저 함수를 생성하면서도 파라미터는 다항식적으로만 증가하므로, 효율적인 표현 학습이 가능합니다.[1]

#### 실험적 증거: 이미지 일반화

가장 명확한 증거는 **이미지 일반화 작업**입니다. Table 2에서 보듯이 FOURIERNET(26.03±2.77)과 GABORNET(26.18±2.95)이 FF Gaussian(25.57±4.18)보다 일관되게 높은 평균 성능과 **더 낮은 표준편차**를 보입니다. 이는 MFN이 더 안정적인 일반화를 제공함을 시사합니다.[1]

특히 자연 이미지(Natural)에서 표준편차 감소가 두드러집니다:
- FF Gaussian: ±4.18
- FOURIERNET: ±2.77 (34% 감소)
- GABORNET: ±2.95 (29% 감소)

#### 구조적 우위

MFN의 일반화 우위는 다음과 같이 설명됩니다:[1]

1. **Gabor 필터의 공간 국소성**: 개별 특성은 입력 공간의 특정 영역에만 영향을 미치므로, 외삽(extrapolation) 시 국소적 구조만 학습하면 됩니다.[1]

2. **명시적 주파수 제어**: Theorem 1과 2의 명시적 기저 함수 표현으로 인해, 하이퍼파라미터 튜닝이 직관적이고 재현성이 높습니다.[1]

3. **낮은 유효 파라미터 복잡도**: 비록 W 행렬 계수로 정의되는 계수가 다항식 개지만, 이들은 강하게 구조화된 (저랭크) 텐서 형태이므로 학습 가능한 자유도가 낮습니다.[1]

### 5. 모델의 한계

논문은 MFN의 한계도 명시합니다:[1]

1. **SIREN의 매끄러움(Smoothness) 편향 부족**: SIREN은 함수와 그 기울기 모두에서 더 매끄러운 영역으로의 편향이 있는데, MFN은 이를 갖지 못합니다. 특히 3D 형상 표현에서 MFN은 평면 표면에서 시각적 아티팩트를 보입니다.[1]

2. **깊이에 따른 성능 증가의 불확실성**: MFN이 깊이나 너비 증가에 따라 일관되게 성능이 개선되지만, SIREN은 특정 하이퍼파라미터 조정(예: ω 값)에 매우 민감합니다.[1]

3. **적용 범위의 제한성**: MFN은 저차원 좌표 기반 함수에 최적화되어 있으며, 고차원 입력에서의 성능은 검증되지 않았습니다.[1]

### 6. 최신 연구 기반 향후 영향과 고려사항

#### 최신 연구 동향

**Residual Multiplicative Filter Networks (Residual MFN)**: 2022년 발표된 이 연구는 MFN의 한계를 해결하기 위해 스킵 연결을 도입했습니다. 다중 스케일 재구성이 필요한 역문제에서 MFN의 원래 구조가 세밀한 스케일에 맞추면서 거친 스케일 정보를 잃는 문제를 해결했습니다. 이는 MFN의 실용적 적용을 크게 확대합니다.[2]

**Implicit Neural Representations (INR)의 일반화**: 2024년의 포괄적인 조사에서, MFN이 저차원 함수 근사의 표준 벤치마크로 인정되었습니다. INR 분야에서 기억 효율성, 해상도 독립성, 그리고 **이산 데이터 구조 너머로의 일반화** 능력이 주요 이점으로 강조되었습니다.[3]

**Quantum Implicit Representation Networks (QIREN)**: 2024년 ICML에서 발표된 이 연구는 고전 FNN에 양자 이점을 제공하며, MFN 계열 방법의 최신 확장을 보여줍니다. QIREN은 이미지 표현 작업에서 파라미터 수 대비 최고 성능을 달성하며, 고전 방법 대비 최대 34.8% 오류 감소를 보였습니다.[4]

**신경 필드의 필터링 및 정규화**: 2023년 연구는 신경 필드 일반화를 개선하기 위해 평활 연산자와 복구 연산자를 제안했습니다. 이는 MFN이 기본적으로 필터 기반 설계이므로, 이러한 정규화 기법과의 결합이 효과적일 수 있음을 시사합니다.[5]

#### 앞으로의 연구에서 고려할 점

1. **다중 스케일 표현 개선**: Residual MFN의 성공 사례를 따라, 계층적 주파수 제어와 적응적 대역폭 할당이 중요한 연구 방향입니다. 특히 BACON 초기화 기법과의 결합이 유망합니다.[6][2]

2. **일반화 이론의 강화**: 현재 MFN은 경험적으로 우수한 일반화를 보이지만, 이론적 일반화 한계(generalization bounds)에 대한 분석이 필요합니다. 무작위 특성 관점에서 Rademacher 복잡도 분석이 도움이 될 수 있습니다.[7]

3. **신경 방사 필드(NeRF) 일반화**: NeRF 분야에서 최근 NeuGen 같은 뇌 영감 정규화 기법이 다양한 장면으로의 일반화를 개선하고 있습니다. MFN 기반 NeRF에 이러한 정규화 기법을 적용하면, 더욱 강력한 교차 장면 일반화가 가능할 것으로 예상됩니다.[8][9]

4. **혼합 전문가(Mixture of Experts) 적용**: 2024년 연구는 MoE를 INR에 적용하여 국소 분할 학습 능력을 추가했습니다. MFN도 각 필터 영역에 대해 전문화된 선형 계수를 학습하는 MoE 변형을 통해 성능을 더욱 향상시킬 수 있습니다.[10]

5. **시간 의존적 PDE와 동적 신호**: 최근 연구는 INR을 시간에 따라 변하는 PDE 풀이에 적용하고 있습니다. MFN의 명시적 주파수 제어 특성이 시간-공간 주파수 분리에 특히 유용할 수 있습니다.[11]

6. **고차원으로의 확장**: 현재 MFN은 저차원 입력에서만 검증되었으나, 최신 연구의 멀티해상도 해시 그리드나 컨볼루션 보강 INR 같은 기법과 결합하면 고차원 신호에도 확장 가능할 것으로 보입니다.[12]

7. **의료 영상 응용**: 2024년 연구에서 전체 슬라이드 이미지(WSI) 표현에 INR이 성공적으로 적용되었습니다. MFN의 높은 효율성과 로컬 표현 능력이 의료 영상 압축 및 초고해상도 재구성에 큰 이점을 제공할 수 있습니다.[12]

### 결론

Multiplicative Filter Networks는 저차원 복잡 함수 근사에 대한 획기적 관점을 제시합니다. 기존의 깊은 신경망 구조를 포기하고 명시적으로 계산 가능한 선형 Fourier/Gabor 기저 표현을 채택함으로써, **단순성과 해석가능성을 유지하면서도 우수한 성능을 달성**합니다. 특히 이미지 일반화에서의 안정성과 초기 수렴 속도에서의 우위는 이 접근법의 근본적인 강점을 드러냅니다.[1]

최신 연구의 확장(Residual MFN, QIREN, MoE INR 등)은 MFN이 단순한 개별 아키텍처가 아니라 **혁신적인 신경 표현 패러다임**의 기초임을 보여줍니다. 앞으로의 연구는 MFN의 주파수 제어 능력과 다양한 정규화 기법의 결합, 고차원 및 동적 신호로의 확장, 그리고 의료·과학 영상 등 실제 응용 분야로의 적용에 초점을 맞춰야 할 것으로 판단됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/8b565a83-a6df-444e-badf-7b8516e46e68/2666_multiplicative_filter_networks.pdf)
[2](https://arxiv.org/abs/2206.00746v2)
[3](https://arxiv.org/abs/2411.03688)
[4](https://icml.cc/media/icml-2024/Slides/34996_oOoMq1A.pdf)
[5](https://arxiv.org/abs/2201.13013)
[6](https://proceedings.neurips.cc/paper_files/paper/2022/file/38e491559eb9e4cf31b8cd3a4e222436-Paper-Conference.pdf)
[7](http://proceedings.mlr.press/v108/chen20d/chen20d.pdf)
[8](https://openreview.net/forum?id=YxLxrWkwsX)
[9](https://arxiv.org/abs/2505.06894)
[10](https://openreview.net/pdf/29a5178e806f04207f02516fcb74d8395ed9af42.pdf)
[11](http://arxiv.org/pdf/2210.00124.pdf)
[12](https://papers.miccai.org/miccai-2024/163-Paper4034.html)
