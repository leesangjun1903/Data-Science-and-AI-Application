# ProxyNCA++: Revisiting and Revitalizing Proxy Neighborhood Component Analysis

**핵심 주장:**  
ProxyNCA++는 기존 ProxyNCA의 계산 효율성과 간편함을 유지하면서, 여섯 가지 주요 개선(프록시 할당 확률, 저온 스케일링, 계층 정규화, 클래스 균형 샘플링, 전역 맥스 풀링, 고속 이동 프록시)을 통해 제로샷 이미지 검색 성능을 대폭 향상시킨다.  

**주요 기여:**  
1. ProxyNCA를 NCA와 정식으로 정렬시키는 **프록시 할당 확률 최적화**  
2. **저온(低溫) 스케일링** 도입으로 더 뾰족한 소프트맥스 분포 획득  
3. **계층 정규화(Layer Norm)** 및 **클래스 균형 샘플링** 결합으로 안정적 학습  
4. 전통적 GAP 대신 **전역 맥스 풀링(Global Max Pooling)** 활용  
5. 프록시 학습률을 높인 **고속 이동 프록시(Fast Moving Proxies)**  
6. 네 가지 제로샷 벤치마크(CUB200, Cars196, SOP, InShop)에서 평균 Recall@1을 22.9pp 향상  

***

## 1. 해결하고자 하는 문제  
- **거리 메트릭 학습(DML)** 기반 이미지 검색에서, ProxyNCA는 클래스별 프로토타입(프록시)을 활용해 연산량을 줄였으나 최신 쌍 기반 방법 대비 성능이 뒤처짐  
- **제로샷** 환경(학습 시 보지 못한 클래스를 검색)에서 더 강력한 일반화 메트릭이 요구됨  

## 2. 제안 방법

### 2.1 프록시 할당 확률 최적화  
기존 ProxyNCA 손실은  

$$
L_{\text{ProxyNCA}}
= -\log \frac{\exp\bigl(-d(x_i, p_{y_i})\bigr)}{\sum_{c}\exp\bigl(-d(x_i, p_c)\bigr)}
$$  

여기서 $$d$$는 정규화된 유클리드 거리.  
ProxyNCA++는 이를 **프록시 할당 확률** $$P_i$$ 최적화로 해석하여,  

$$
L = -\log P_i, \quad
P_i = \frac{\exp\bigl(-d(x_i, p_{y_i})/T\bigr)}{\sum_{c}\exp\bigl(-d(x_i, p_c)/T\bigr)}
$$  

로 확장하며, 프록시 전반에 대한 할당 확률을 일관되게 최적화.

### 2.2 저온 스케일링  
- **온도 $$T<1$$** 로 소프트맥스 분포를 더 날카롭게 만들어, 클래스 간 경계가 뚜렷해지고 과적합을 억제하면서 테스트 성능을 최대로 끌어올림  
- CUB200에서 최적 $$T=1/9$$ 적용 시 Recall@1 10.8pp 향상  

### 2.3 전역 맥스 풀링  
- GAP 대비 주요 특징만 강조하는 맥스 풀링이 Retrieval Recall@1을 평균 2.1pp 개선  
- 다양한 모델(Margin, Triplet, MS Loss)에도 동일한 성능 향상 관찰  

### 2.4 고속 이동 프록시  
- 프록시 L2 정규화로 인한 기울기 소실 문제 해결을 위해 **프록시 학습률**을 백본 대비 수백 배 높여 빠르게 업데이트  
- 저온 스케일링·맥스 풀링과 시너지 효과로 Recall@1 약 2pp 추가 향상  

### 2.5 기타: 계층 정규화·클래스 균형 샘플링  
- Layer Norm 비어파인 파라미터版 적용, 배치 내 클래스 균형 유지로 안정적 수렴  

## 3. 모델 구조  
- 백본: ImageNet 사전학습 ResNet-50  
- 특성 추출 후 전역 맥스 풀링 → 2048차원 임베딩층 → L2 정규화  
- 학습: 두 단계(학습/튜닝, 전체 훈련)  
- 프록시 수 = 학습 클래스 수  

## 4. 성능 향상 및 한계  

| 데이터셋   | ProxyNCA Recall@1 | ProxyNCA++ Recall@1 |
|-----------|-------------------|---------------------|
| CUB200    | 59.3 → 72.2       | +12.9pp             |
| Cars196   | 62.6 → 90.1       | +27.5pp             |
| SOP       | 62.1 → 81.4       | +19.3pp             |
| InShop    | 59.1 → 90.9       | +31.8pp             |

- **한계:**  
  - 최적의 온도 $$T$$ 및 학습률 설정이 민감하며, 소규모 데이터셋 일반화에는 과적합 우려  
  - GAP 대비 GMP 장점 원인에 대한 명확한 이론적 해석 미비  

## 5. 일반화 성능 향상 가능성  
- 저온 스케일링과 계층 정규화가 과적합 억제 및 분류 경계 강화에 기여 → **드문 클래스에서도 더 안정적으로 분포 학습**  
- 클래스 균형 샘플링으로 배치 다이버시티 확보 → **희소 클래스 대표성 보강**  
- 고속 이동 프록시로 프록시-임베딩 협력적 적응 가속 → **신규 클래스 적응력** 강화  

***

## 향후 연구에 미치는 영향 및 고려 사항  
1. **이론적 분석:** GMP 우수성 및 온도 스케일링 효과에 대한 수학적 근거 확보  
2. **동적 온도 학습:** 훈련 중 적응적 $$T$$ 스케일링으로 과적합 방지와 일반화 조화  
3. **프록시 공간 구조화:** 프록시 간 계층·군집 관계 학습을 통한 더 풍부한 클래스 분포 모델링  
4. **저자원 시나리오:** 소수 샷·소수 클래스 환경에서의 안정성 및 효율성 검증  

ProxyNCA++는 경량 프록시 기반 DML의 새로운 기준을 제시하며, **제로샷 일반화**를 위한 다양한 연구 방향을 열어준다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/9f3be7f6-d6a0-4257-af23-b06482c55ccb/2004.01113v2.pdf
