# Mixed Precision Training

## 1. 핵심 주장 및 주요 기여  
**혼합 정밀도(Mixed Precision) 훈련**을 통해 FP32(32비트 부동소수점) 대비 메모리 사용량과 연산 시간을 거의 2배가량 절감하면서도, **모델 정확도 손실 없이** 동일한 수준의 성능을 달성할 수 있음을 보였다.  
- FP16(16비트 반정밀도) 저장·연산 적용  
- FP32 마스터 가중치 복사본 유지  
- 손실 스케일링(loss-scaling) 도입  
- FP16 곱셈 결과 누적 시 FP32로 연산  

## 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계  

### 2.1 해결하고자 하는 문제  
- 대규모 신경망 훈련 시 메모리(bandwidth)와 연산(arithmetic) 병목  
- FP16의 좁은 동적 범위로 인한 그라디언트 하위 비트 손실 및 정확도 저하  

### 2.2 제안 방법  
1) **FP32 마스터 가중치 유지**  
   - 가중치 업데이트는 FP32 마스터에 적용.  
   - 순전파·역전파는 FP16 복제본 사용.  
2) **손실 스케일링 (Loss-Scaling)**  
   - 역전파 전 손실 $$L$$에 스케일 $$S$$를 곱해 $$L' = S \times L$$.  
   - 그라디언트 $$\nabla_w L$$은 $$\nabla_w L' = S \times \nabla_w L$$로 스케일링되어 FP16 언더플로우 방지.  
   - 업데이트 전 다시 $$\nabla_w L = \nabla_w L' / S$$로 복원.  
3) **FP16 곱셈 누적(FP16×FP16→FP32 누적)**  
   - 내적과 대규모 합산(reduction)은 FP32로 누적하여 정밀도 확보.  

### 2.3 모델 구조  
- 다양한 아키텍처에 적용: CNN(AlexNet, VGG, Inception, ResNet-50), 객체 탐지(Faster R-CNN, SSD), RNN/GRU 기반 음성인식(DeepSpeech2), LSTM 번역·언어모델, DCGAN 등  
- 핵심은 기존 구조·하이퍼파라미터 변경 없이 정밀도만 조정  

### 2.4 성능 향상  
- **메모리 사용량 약 50% 절감** (활성화·가중치 저장)  
- **Tensor Core 활용 시 연산 속도 2×–6× 향상**  
- **Top-1 정확도/Perplexity/CER/ mAP** 모두 FP32와 대등 or 소폭 상회  
  - ILSVRC 분류: e.g. ResNet-50 FP32 75.92% → MP 76.04%  
  - SSD mAP: 76.9% → 77.1% (손실 스케일링)  
  - 음성인식 CER: 15.82% → 15.01% (일부 정밀도 규제 효과)  

### 2.5 한계  
- 손실 스케일링 인자 $$S$$를 경험적으로 선택해야 함  
- 스케일 인자로 인한 오버플로우 감지 및 업데이트 스킵 로직 필요  
- 프레임워크·라이브러리 최적화 작업 필수  

## 3. 일반화 성능 향상 관점  
- FP16 저장 포맷이 **일종의 정규화(regularization)** 효과를 주어 과적합 감소 가능성  
- 음성 및 언어 모델에서 MP 훈련 결과가 FP32 대비 **5–10%** CER·Perplexity 개선  
- 불필요한 작은 그라디언트 소실 억제로 훈련 안정성 상승  

## 4. 향후 연구 영향 및 고려 사항  
- **자동 손실 스케일링** 메커니즘 개발: 오버플로우 감지 기반 동적 스케일  
- **강화 학습·텍스트-투-스피치** 등 추가 도메인 적용 검증  
- **라이브러리 최적화**로 Mixed Precision 전 구간 가속화  
- FP16 이외의 저정밀도(bfloat16·INT8) 확장 및 하드웨어 지원 발전 방향 모색

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/cea9038e-c0a9-40f6-a641-b702ac347b26/1710.03740v3.pdf
