# Self-training Avoids Using Spurious Features Under Domain Shift

### 1. 논문의 핵심 주장과 주요 기여

**"Self-training Avoids Using Spurious Features Under Domain Shift"** 논문은 비지도 도메인 적응 환경에서 자기 학습(self-training)과 엔트로피 최소화가 **거짓 특징(spurious features)**을 회피할 수 있음을 이론적으로 증명한 첫 번째 연구입니다.[1]

핵심 주장은 다음과 같습니다:

- 소스 도메인에서 특정 특징들이 라벨과 상관관계를 가지지만, 타겟 도메인에서는 이 상관관계가 존재하지 않는 구조화된 도메인 이동 상황에서, 초기 소스 분류기의 성능이 충분할 경우 자기 학습이 이러한 거짓 특징의 사용을 자동으로 제거할 수 있다는 것입니다.[1]

주요 기여는 다음과 같습니다:

1. **이론적 증명**: 비볼록 손실 함수를 가진 자기 학습이 유한한 라벨 없는 데이터로도 거짓 특징의 계수를 0으로 수렴시킬 수 있음을 증명했습니다.[1]

2. **현실적 가정**: 로그-오목 분포의 혼합과 잘 분리된 클래스 구조라는 현실적인 분포 가정을 제시하여 기존 이론의 한계를 극복했습니다.[1]

3. **실증적 검증**: 반합성 Colored MNIST와 CelebA 데이터셋 실험을 통해 이론적 통찰력이 다층 신경망에도 적용 가능함을 보였습니다.[1]

***

### 2. 해결 문제, 제안 방법, 모델 구조 및 성능 분석

#### 2.1 해결하는 문제

기존 도메인 적응 이론은 소스와 타겟 도메인이 충분히 가까운 경우만 분석했습니다. 하지만 실무에서는 훨씬 큰 도메인 이동이 발생하며, 특히 **거짓 상관관계** 문제가 심각합니다. 예를 들어:[1]

- 의료 진단에서 병원의 특정 기구(metal tokens)가 실제 질병 특징처럼 작용
- 인종정보가 재범 예측 모델에서 인과적이지 않은 상관을 형성
- 성별 분류에서 머리 색깔이 성별을 대리하는 특징으로 작용[1]

#### 2.2 제안하는 방법 및 수식

논문은 **엔트로피 최소화(Entropy Minimization)** 알고리즘을 제안합니다:[1]

$$
\min_w \mathcal{L}(w) = \mathbb{E}_{x \sim D_{tg}} \ell_{\exp}(w^T x)
$$

여기서 $$\ell_{\exp}(t) = \log(1 + e^{-t})$$는 로지스틱 손실을 근사한 지수 손실입니다.[1]

**Projected Gradient Descent** 업데이트:

$$
w^{t+1} = \Pi_R(w^t - \eta \nabla \mathcal{L}(w^t))
$$

여기서 $$\Pi_R$$는 단위 공(unit ball)으로의 투영, $$\eta$$는 스텝 사이즈입니다.[1]

**핵심 수학적 인사이트**: 데이터가 신호 특징 $$x_1$$과 거짓 특징 $$x_2$$로 분해될 때:

$$
x = [x_1, x_2], \quad y = \text{unif}(-1,1), \quad x_1 \sim D_{tg,1}, \quad x_2 \sim \mathcal{N}(0, \sigma^2 I)
$$

타겟 도메인에서 $$x_2 \perp y$$이면, 그래디언트 방향이 $$w_2$$를 감소시킵니다:[1]

$$
\nabla_{w_2} \mathcal{L}(w) = -\sigma^2 w_2 \cdot \mathbb{E}[q(\eta)] < 0
$$

여기서 $$\eta = w_1^T x_1$$이고 $$q(\eta) = \mathbb{E}\_{z \sim \mathcal{N}(\eta, \sigma^2)}[\ell_{\exp}(z)]$$입니다.[1]

#### 2.3 모델 구조

선형 분류기를 기본 설정으로 사용:

$$
f(x) = w^T x = w_1^T x_1 + w_2^T x_2
$$

실험에서는 이를 **3층 신경망**으로 확장하여 비선형 설정에서도 작동함을 보였습니다.[1]

**핵심 가정**:

1. **분리 가정(Separation Assumption)**: 신호 특징 $$x_1$$이 $$K$$개의 슬라이스-로그-오목 및 로그-평활 분포의 혼합으로 충분히 분리됨[1]

2. **초기 조건**: 소스 분류기가 타겟 도메인에서 적절한 정확도를 가지며, 거짓 특징에 지나치게 의존하지 않음[1]

$$
\|w_s^1\|_2 \geq \frac{1}{2}\|\tilde{w}\|_2, \quad \sigma^2 \|w_s^2\|_2^2 \leq c \min(1, \lambda^2, \pi_{\min}, \log(1/\epsilon))
$$

#### 2.4 성능 향상

**Theorem 3.1** (메인 결과): 위 가정 하에서, $$\mathcal{O}(\log(1/\epsilon))$$ 반복 후 거짓 특징의 계수는 $$\|w_2\|_2 \leq \epsilon$$로 수렴합니다. 유한 샘플 보장은 $$\mathcal{O}(\epsilon^{-4}\log(1/\epsilon))$$ 개의 라벨 없는 샘플을 필요로 합니다.[1]

**Theorem 3.2** (가우시안 특수 경우): 신호 특징이 1차원 가우시안 혼합일 때, 자기 학습이 **베이즈 최적 분류기**로 수렴합니다.[1]

**실증적 결과**:
- **CelebA**: 81% → 88% 정확도 (7% 향상)[1]
- **CMNIST10** (p=0.95): 94% → 96% 정확도[1]
- **CMNIST2**: 72% → 비정상 (초기 정확도 부족시 실패 확인)[1]

***

### 3. 모델 일반화 성능 향상 가능성

#### 3.1 강건성 측면의 개선

논문은 **대규모 다양한 라벨 없는 데이터 수집의 중요성**을 강조합니다. 이는 최근 Google의 "Noisy Student" 연구 등과 일치하며, 실제로 대규모 데이터셋에서 자기 학습이 ImageNet 정확도를 크게 향상시켰음이 입증되었습니다.[1]

#### 3.2 도메인 이동 하에서의 일반화

거짓 특징을 제거함으로써 모델은 **인과적 특징**에만 의존하게 되어 도메인 이동에 대한 안정성이 향상됩니다. 실험에서 90%의 테스트 예제가 이론적 예측과 일치하는 거동을 보였습니다.[1]

#### 3.3 한계점

**중요한 제약 조건**들:

1. **초기 소스 정확도 필요**: 소스 정확도가 충분하지 않으면 자기 학습이 오히려 성능을 악화시킵니다 (CMNIST10, p=0.97: 72% → 67%)[1]

2. **구조화된 도메인 이동 가정**: 거짓 특징이 정확히 타겟에서 라벨과 독립적이어야 합니다. 부분적 독립성이나 연속적 이동에는 적용 불가[1]

3. **유한 샘플 복잡성**: $$\mathcal{O}(\epsilon^{-4})$$의 높은 샘플 복잡도로 매우 작은 $$\epsilon$$에서는 실용성 제한[1]

4. **선형 모델 분석의 한계**: 이론은 선형 분류기 기반이며, 깊은 신경망에서의 정확한 동작은 완전히 이해되지 않음[1]

***

### 4. 앞으로의 연구에 미치는 영향과 향후 고려사항

#### 4.1 학계에 미친 영향

이 논문은 **자기 학습의 이론적 기초**를 제공하여 많은 후속 연구를 촉발했습니다:[2][3][4][5]

1. **도메인 적응 이론 발전**: 기존의 "거리 기반" 접근에서 "구조 기반" 접근으로의 전환을 이끌었습니다.[2]

2. **인과학과의 연결**: 거짓 특징 문제를 인과추론 관점에서 분석하는 연구 증대[5]

3. **공정성 및 강건성 개선**: 알고리즘 편향 제거 방법으로 활용[1]

#### 4.2 최신 연구 방향 (2023-2025)

**연속적 거짓 이동(Continuous Spurious Shift)**: 최근 연구는 거짓 특징이 도메인 인덱스에 따라 **연속적으로 변화**하는 경우를 다루고 있습니다. 예: 환자 나이에 따라 의료 영상의 거짓 특징이 변할 때, 이 논문의 가정은 무너집니다.[3]

**인과 이동성(Causal Transportability)**: 단순한 엔트로피 최소화를 넘어 **인과적으로 전달 가능한 인코딩**을 찾는 연구로 확장되었습니다.[3][5]

**군 불변 학습(Group Invariant Learning)**: 여러 환경에서 불변인 특징을 학습하는 SCILL 같은 방법이 제안되어, 이 논문의 두 도메인 설정을 다중 도메인으로 확장했습니다.[5]

**자기 학습의 동적 가중치**: 최근 STDW(Self-Training with Dynamic Weighting) 등이 제안되어 단순 엔트로피 최소화의 불안정성을 개선하려 하고 있습니다.[4]

#### 4.3 향후 연구 시 고려할 점

**1. 더 약한 가정 하에서의 수렴 분석**
- 로그-오목 가정을 완화하여 일반적인 분포에 대한 이론 개발
- 거짓 특징이 부분적으로만 독립적인 경우 분석

**2. 다중 도메인 및 연속 이동 확장**
- 2개 도메인이 아닌 여러 도메인에서의 수렴성
- 연속적으로 변하는 도메인 인덱스에서의 일반화[3]

**3. 깊은 신경망에서의 이론**
- 현재 이론은 선형 모델 기반인데, 비선형성이 거짓 특징 제거에 미치는 영향 분석
- 표현 학습 측면에서의 거짓 특징 제거 메커니즘 이해

**4. 실용적 알고리즘 개선**
- 초기 정확도 요구 조건 완화
- 신뢰 임계값 자동 설정 방법
- 거짓 특징 식별 가능 시 명시적 페널티 추가[6]

**5. 특정 응용 분야 확장**
- 의료 영상, 자연어 처리 등 구체적 도메인에서의 거짓 특징 특성화
- 공정성 제약이 있는 도메인 적응[1]

#### 4.4 현재의 한계 극복 방향

최신 연구들은 다음 방향으로 진전하고 있습니다:[7][8][2]

- **정보 이론적 기초**: 상대 엔트로피 정규화를 이용한 더 안정적인 도메인 적응[9]
- **소스 없는 도메인 적응**: 소스 데이터 접근 불가 환경에서의 자기 학습 개선[7]
- **특징 분리(Disentanglement)**: 도메인-불변 특징과 도메인-특이적 특징의 명시적 분리[10][11]

***

### 결론

**"Self-training Avoids Using Spurious Features Under Domain Shift"** 논문은 비지도 도메인 적응 문제에 대한 **첫 번째 수렴 이론**을 제공함으로써, 자기 학습이 단순히 경험적으로 작동하는 휴리스틱이 아니라 **수학적으로 정당화될 수 있음**을 보였습니다. 이는 도메인 적응 이론의 기초를 마련했으며, 최근의 연속적 이동, 인과 이동성, 다중 도메인 설정 등의 연구들이 이를 확장하고 있습니다.[4][5][3][1]

향후 연구에서는 더 약한 가정, 더 복잡한 분포 설정, 그리고 깊은 신경망에서의 거짓 특징 제거 메커니즘에 대한 이론적 이해가 필요하며, 실무적으로는 초기 정확도 요구 조건을 완화하고 자동 알고리즘 설정을 개선하는 방향이 중요합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/522d3681-07d0-4f1b-b985-ad87a1b932cc/2006.10032v3.pdf)
[2](https://arxiv.org/html/2502.06272v1)
[3](https://openreview.net/forum?id=G2AMCTTpCc)
[4](https://arxiv.org/html/2510.13864v1)
[5](https://arxiv.org/pdf/2206.14534.pdf)
[6](https://openaccess.thecvf.com/content/CVPR2025/papers/Chai_Identifying_and_Mitigating_Spurious_Correlation_in_Multi-Task_Learning_CVPR_2025_paper.pdf)
[7](http://arxiv.org/pdf/2406.01658.pdf)
[8](https://arxiv.org/html/2402.12715v3)
[9](https://www.mdpi.com/1099-4300/27/4/426)
[10](https://pmc.ncbi.nlm.nih.gov/articles/PMC6759585/)
[11](https://www.sciencedirect.com/science/article/abs/pii/S0893608025006379)
[12](https://arxiv.org/pdf/1811.05443.pdf)
[13](http://arxiv.org/pdf/2403.06424.pdf)
[14](https://arxiv.org/pdf/1502.02791.pdf)
[15](http://arxiv.org/pdf/2403.10834.pdf)
[16](https://proceedings.neurips.cc/paper/2020/file/f1298750ed09618717f9c10ea8d1d3b0-Paper.pdf)
[17](https://lgmoneda.github.io/2021/01/12/spurious-correlation-ml-and-causality.html)
[18](https://pmc.ncbi.nlm.nih.gov/articles/PMC12235040/)
[19](https://proceedings.nips.cc/paper/2021/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf)
