# Transferable Adversarial Training: A General Approach to Adapting Deep Classifiers

### 1. 핵심 주장 및 주요 기여

본 논문은 **도메인 적응(domain adaptation)에서 기존 적대적 특성 적응(adversarial feature adaptation) 방법들의 숨겨진 한계를 지적**하고, 이를 극복하기 위해 **전이가능한 적대적 훈련(Transferable Adversarial Training, TAT)**을 제안합니다.[1]

**핵심 주장:**
- 기존 적대적 특성 적응 방법들이 도메인 불변 표현(domain-invariant representations)을 학습하는 과정에서 원본 특성 분포(feature distributions)를 왜곡하여, 오히려 도메인 적응의 핵심 이론적 전제조건인 **적응능력(adaptability) λ를 악화시킨다**는 점
- 도메인 적응 이론의 일반화 오차 상한식에서 $$\epsilon_t(C) \leq \epsilon_s(C) + d_{H\Delta H}(X_s, X_t) + \lambda$$로 표현되는데, 기존 방법들이 λ를 증가시켜 이론적 보장을 약화시킨다는 주장[1]

**주요 기여:**
- 특성 표현을 고정하고 **전이가능한 예제(transferable examples)를 생성하여 도메인 간의 격차를 채우는 새로운 패러다임** 제시
- 이론적 적응능력 λ를 보존하면서도 도메인 불변성을 달성하는 방법론 개발
- 시각(vision) 및 자연언어처리(NLP) 작업 모두에서 최첨단 성능 달성

---

### 2. 문제 정의, 제안 방법론 및 모델 구조

#### 2.1 문제 정의

**도메인 적응 문제:**
- 소스 도메인에서 레이블된 데이터 $$\{x_s^{(i)}, y_s^{(i)}\}_{i=1}^{n_s}$$ ($$P(x_s, y_s)$$ 분포)
- 타겟 도메인에서 레이블이 없는 데이터 $$\{x_t^{(i)}\}_{i=1}^{n_t}$$ ($$Q(x_t, y_t)$$ 분포)
- 목표: 타겟 도메인에서 낮은 오차를 보장하는 분류기 $$y = C(f)$$ (여기서 $$f = F(x)$$) 학습

**기존 방법의 문제점:**
- 도메인 불변 특성을 학습하는 과정에서 특성 분포 왜곡
- 이상적 결합 가설의 오차 λ 증가
- 감정 분류(sentiment classification) 등 잘 정의된 아키텍처가 없는 작업에서 성능 저하[1]

#### 2.2 제안 방법: Transferable Adversarial Training (TAT)

**핵심 아이디어:**
특성 표현을 고정한 채 전이가능한 예제를 생성하고, 이 예제들에 대해 적대적 훈련을 수행

**Step 1: 전이가능 예제의 적대적 생성**

도메인 판별기 손실:
$$\ell_d(\theta_D, f) = -\frac{1}{n_s}\sum_{i=1}^{n_s}\log[D(f_s^{(i)})] - \frac{1}{n_t}\sum_{i=1}^{n_t}\log[1-D(f_t^{(i)})]$$

분류기 손실:
$$\ell_c(\theta_C, f) = \frac{1}{n_s}\sum_{i=1}^{n_s}\ell_{ce}(C(f_s^{(i)}), y_s^{(i)})$$

전이가능 예제 생성 (K번 반복, k=0,1,...,K-1):
$$f_t^{k+1} \leftarrow f_t^k + \beta\nabla_{f_t^k}\ell_d(\theta_D, f_t^k) - \gamma\nabla_{f_t^k}\ell_2(f_t^k, f_t^0)$$

$$f_s^{k+1} \leftarrow f_s^k + \beta\nabla_{f_s^k}\ell_d(\theta_D, f_s^k) - \gamma\nabla_{f_s^k}\ell_2(f_s^k, f_s^0) + \beta\nabla_{f_s^k}\ell_c(\theta_C, f_s^k)$$

여기서:
- β: 도메인 판별기 영향도 제어 파라미터
- γ: 원본 예제로부터의 L2 거리 제약 (안정성 보장)
- K: 전이가능 예제 생성 반복 횟수 (일반적으로 K=10)[1]

**Step 2: 전이가능 예제를 이용한 적대적 훈련**

분류기 적대적 훈련 손실:

```math
\ell_{c,adv}(\theta_C, f^*) = \frac{1}{n_s}\sum_{i=1}^{n_s}\ell_{ce}(C(f_s^{*(i)}), y_s^{*(i)}) + \frac{1}{n_t}\sum_{i=1}^{n_t}||C(f_t^{*(i)}) - C(f_t^{(i)})||
```

도메인 판별기 적대적 훈련 손실:

```math
\ell_{d,adv}(\theta_D, f^*) = -\frac{1}{n_s}\sum_{i=1}^{n_s}\log[D(f_s^{*(i)})] - \frac{1}{n_t}\sum_{i=1}^{n_t}\log[1-D(f_t^{*(i)})]
```

최종 최적화 문제:

```math
\min_{\theta_D,\theta_C} \ell_d(\theta_D, f) + \ell_c(\theta_C, f) + \ell_{d,adv}(\theta_D, f^*) + \ell_{c,adv}(\theta_C, f^*)
```

#### 2.3 모델 구조

**구성 요소:**

| 요소 | 설명 |
|------|------|
| 특성 추출기 (F) | 사전학습된 ResNet-50 (ImageNet)로 고정 |
| 분류기 (C) | 2층 완전연결층 (2048×256×#classes) |
| 도메인 판별기 (D) | 2층 완전연결층 + BatchNorm + LeakyReLU |
| 생성 모듈 | 전이가능 예제를 K번 반복 생성 |

**훈련 알고리즘 (Algorithm 1):**[1]
- 입력: 원본 특성 $$\{f\_s^{(i)}, y_s^{(i)}\}\_{i=1}^{n_s}$$와 $$\{f_t^{(i)}\}_{i=1}^{n_t}$$
- 최대 반복: MaxIter
- 각 반복에서:
  1. 미니배치 샘플링
  2. K번 전이가능 예제 생성 (식 6-7)
  3. 분류기 업데이트: $$\theta_C \leftarrow \theta_C - \alpha\nabla_{\theta_C}[\ell_c(\theta_C, f) + \ell_{c,adv}(\theta_C, f^*)]$$
  4. 도메인 판별기 업데이트: $$\theta_D \leftarrow \theta_D - \alpha\nabla_{\theta_D}[\ell_d(\theta_D, f) + \ell_{d,adv}(\theta_D, f^*)]$$

**계산 효율성:**
- 특성 추출기를 고정하므로 그래디언트 계산이 2-3층 완전연결층에만 필요
- 기존 특성 적응 방법보다 **약 10배 이상 빠른 훈련 속도**

***

### 3. 모델의 일반화 성능 향상 가능성

#### 3.1 이론적 보장

**도메인 적응 이론 (Ben-David et al., 2010):**
$$\epsilon_t(C) \leq \epsilon_s(C) + d_{H\Delta H}(X_s, X_t) + \lambda$$

여기서 λ는 적응능력으로 정의됨:

```math
\lambda = \epsilon_s(h^*) + \epsilon_t(h^*)
```

**TAT의 이론적 기여:**
1. **적응능력 λ 보존**: 특성 표현 F를 고정하므로 λ가 원본 값으로 유지됨[1]
2. **소스 오차 최소화**: $$\ell_c(\theta_C, f)$$를 통해 $$\epsilon_s(C)$$ 최소화
3. **도메인 불변성 개선**: 생성된 전이가능 예제들이 도메인 간 격차를 메우면서 $$d_{H\Delta H}(X_s, X_t)$$ 감소[1]
4. **적대적 훈련 이론**: Sinha et al. (2018)의 견고한 적대적 훈련 이론 활용
   - Wasserstein 거리 ρ 내의 최악 사례 분포에 대한 훈련
   - 타겟 도메인이 거리 ρ 내에 있으면 좋은 성능 보장

#### 3.2 실증적 성능 향상

**Office-31 (주요 벤치마크):**

| 작업 | ResNet-50 | DANN | CDAN | TAT |
|------|-----------|------|------|-----|
| A→W | 68.4% | 82.6% | 93.1% | 92.5% |
| D→W | 96.7% | 96.9% | 98.6% | 99.3% |
| W→D | 99.3% | 99.3% | 100.0% | 100.0% |
| A→D | 68.9% | 81.5% | 92.9% | 93.2% |
| D→A | 62.5% | 68.4% | 71.0% | **73.1%** |
| W→A | 60.7% | 67.5% | 69.3% | **72.1%** |
| **평균** | 76.1% | 82.7% | 87.5% | **88.4%** |

특히 **D→A와 W→A에서 큰 도메인 불일치가 있을 때 최고의 성능** 달성[1]

**VisDA-2017 (합성-실제 적응):**[1]
- ResNet-50: 40.2%
- DANN: 63.7%
- MCD: 69.2%
- CDAN: 70.0%
- **TAT: 71.9%** ✓

**ImageCLEF-DA 결과:**[1]
- CDAN: 87.1%
- **TAT: 88.9%** (모든 작업에서 최고 성능)

**Office-Home (고난이도 벤치마크):**[1]
- CDAN: 63.8%
- **TAT: 65.8%** (큰 도메인 불일치에서 우수)

#### 3.3 이상적 결합 가설의 오차 (적응능력 λ) 분석

**Figure 2(b)의 실증적 증거:**
- ResNet-50 (기준): λ ≈ 0.18
- DANN: λ ≈ 0.22 (+22% 증가 - **악화**)
- MCD: λ ≈ 0.20 (+11% 증가 - **악화**)
- TAT: λ ≈ 0.18 (유지 - **보장됨**)[1]

이는 TAT가 **기존 적대적 특성 적응 방법들이 야기하는 적응능력 악화 문제를 해결**함을 입증합니다.

#### 3.4 A-Distance를 통한 도메인 불변성 측정

**테이블 7 - 교차 도메인 A-거리:**[1]

| 방법 | D→W | W→A |
|------|-----|-----|
| ResNet-50 | 1.27 | 1.86 |
| DANN | 1.23 | 1.44 |
| MCD | 1.22 | 1.60 |
| **TAT** | **1.06** | **1.04** |

TAT가 가장 **낮은 A-거리(즉, 가장 작은 도메인 불변성 필요)** 달성

***

### 4. 성능 향상의 메커니즘

#### 4.1 전이가능 예제의 역할

**Figure 4 (Two Moon 실험):**
- (a) 소스만 훈련: 타겟 예제 분류 실패
- (b) TAT: 결정 경계가 타겟 데이터에 적응
- (c) 전이가능 예제: **소스와 타겟 간 격차를 메우는 중간 점들** 분포[1]

**핵심 효과:**
- 전이가능 예제들이 도메인 판별기와 분류기 모두를 속이면서, 자연스럽게 도메인 간의 "다리" 역할 수행
- 기존 방법처럼 특성 공간 전체를 변형하지 않으면서도 적응 달성

#### 4.2 Ablation Study (표 6)

| 방법 | A→W | D→W | W→D | A→D | D→A | W→A | 평균 |
|------|-----|-----|-----|-----|-----|-----|------|
| TAT (w/o C) | 87.2% | 98.3% | 99.9% | 88.4% | 71.0% | 69.8% | 85.8% |
| TAT (w/o D) | 88.5% | 98.7% | 100.0% | 90.6% | 71.4% | 72.0% | 86.8% |
| **TAT (완전)** | **92.5%** | **99.3%** | **100.0%** | **93.2%** | **73.1%** | **72.1%** | **88.4%** |

- 분류기 기울기 제거 (w/o C): -2.6% 하락
- 도메인 판별기 기울기 제거 (w/o D): -1.6% 하락
- **두 성분 모두 필수**적임을 입증[1]

---

### 5. 모델의 한계 및 과제

#### 5.1 내재적 한계

**1. 하이퍼파라미터 민감성**
- β, γ, K 값이 성능에 영향을 미침
- 데이터셋별 최적값 찾기 필요 (역검증 사용)
- 다양한 도메인 쌍에 대한 일반화 부족

**2. 계산량**
- 각 예제마다 K번 반복 생성 필요 (K=10)
- 특성 추출기는 고정되지만, 여전히 추가 계산 오버헤드 존재

**3. 특성 추출기의 선택**
- 사전학습된 특성의 질에 크게 의존
- ImageNet 사전학습이 효과적이지 않은 도메인 (예: 의료 영상)에서 성능 제한

**4. 도메인 불일치 처리**
- 매우 큰 도메인 불일치(예: 스케치↔사진)에서 제한된 개선 (ImageCLEF-DA에서 상대적으로 작은 향상)
- 부분적 도메인 적응(partial domain adaptation) 미지원

#### 5.2 방법론적 한계

**1. 특성 공간 제약**
- 특성 표현 고정으로 인해 원본 특성의 판별력에 제한됨
- 타겟 도메인에서 필요한 새로운 특성 표현 학습 불가능

**2. 이론과 실제의 간격**
- 적응능력 λ 보존을 보장하지만, 실제로는 복잡한 특성 공간에서의 정확한 계산 불가능
- Ben-David 이론의 $$d_{H\Delta H}$$ 항과 적대적 훈련 이론의 통합 미완성 (논문에서 명시)[1]

**3. 타겟 도메인 라벨 부재 활용 제한**
- 타겟 도메인 라벨이 전혀 없는 완전 비지도 학습 시나리오만 고려
- 소량의 타겟 라벨 활용 시나리오 미다룸

***

### 6. 최신 연구 동향 및 미래 연구 방향

#### 6.1 TAT 이후의 진화 방향

**1. 대조학습(Contrastive Learning) 결합**
2023년 이후 연구에서 **대조적 적대적 훈련(Contrastive Adversarial Training, CAT)**이 제안되었습니다. 이는 TAT의 아이디어를 확장하여:[2]
- 레이블이 있는 소스 샘플을 앵커로 사용
- 같은 클래스의 타겟 샘플을 끌어당기고, 다른 클래스는 밀어내기
- VisDA-2017: 93.3% (TAT의 71.9% 대비 상당한 개선)
- Office-Home: 85.1% (TAT의 65.8% 대비 개선)
- DomainNet: 58.0% (새로운 벤치마크에서 강력한 성능)[2]

**2. 메타학습 통합**
2023년 "Domain Adversarial Learning Enhanced Meta Self-Training (DaMSTF)"은 자기학습(self-training)에 적대적 학습을 결합하여 의사 레이블 노이즈 문제를 해결합니다.[3]

**3. 기초 모델(Foundation Model) 활용**
최근 CLIP 등 대규모 사전학습 모델이 도메인 적응에 활용되고 있습니다.[4]
- 멀티모달 도메인 적응의 새로운 패러다임 제시
- TAT의 단일 모달리티 제한 극복 가능성

#### 6.2 일반화 성능 향상을 위한 고려사항

**1. 적응능력 λ의 명시적 최적화**
향후 연구에서는:
- 이상적 결합 가설의 오차를 직접 최소화하는 손실함수 설계
- 소스-타겟 동시 최적화에서 λ를 계산하고 제약 조건으로 활용
- Ben-David 이론과 적대적 훈련 이론의 완전한 통합

**2. 도메인 불일치 정도의 적응적 조절**
- 도메인 불일치 정도를 측정하고 하이퍼파라미터(β, γ, K) 자동 조정
- 메타학습을 통한 동적 파라미터 최적화

**3. 다중 도메인 및 개방 집합 시나리오**
- 3개 이상의 도메인 동시 적응 (Multi-Domain Adaptation)
- 타겟 도메인의 새로운 클래스 처리 (Open-Set Domain Adaptation)
- Partial Domain Adaptation에서의 부정적 전이(negative transfer) 해결

**4. 테스트 시간 적응(Test-Time Adaptation)**
- 추론 단계에서 타겟 도메인 데이터에 실시간 적응
- 온라인 도메인 적응 시나리오 지원

#### 6.3 응용 분야별 고려사항

**의료 영상 도메인 적응:**[5]
- 스킨 병변 탐지에서 도메인 적대적 신경망 사용 시 18.47% 정확도 개선
- 이미지 기반 특성 사전학습의 한계 극복 필요
- 도메인 특화 아키텍처 개발

**강건성(Robustness) 향상:**[6]
- 적대적 훈련이 표준 정확도와 강건성 사이의 트레이드오프 개선
- 서로 다른 적대적 공격에 대한 일반화 강화

**NLP 작업:**[1]
- TAT가 감정 분류에서 기존 방법보다 우수함을 입증
- 문자 수준(BoW) 및 높은 차원 표현 모두에서 효과적

#### 6.4 이론적 진전 필요 영역

**1. 특성 왜곡 메커니즘의 정량적 분석**
- 도메인 판별기가 특성 분포를 어느 정도 왜곡하는지 정확히 측정
- 특이값(singular value) 분석 확장

**2. 전이가능 예제의 최적성 보장**
- 생성된 예제가 실제로 최적의 "중간 점"인지 증명
- 기하학적 해석 강화

**3. 수렴성 보장**
- 제안된 minimax 게임의 수렴 조건 분석
- 학습률, 반복 횟수의 이론적 선택 가이드라인

***

### 7. 결론 및 권장사항

#### 7.1 TAT의 기여 요약

**Transferable Adversarial Training은 다음을 달성합니다:**

1. **이론적 진전**: 기존 적대적 특성 적응의 적응능력 저하 문제를 명시적으로 지적하고, 특성 표현 고정을 통한 해결책 제시

2. **실무적 효과성**: 
   - 시각 작업(Office-31, ImageCLEF, Office-Home, VisDA-2017)에서 최첨단 성능
   - NLP 작업(감정 분류)에서도 강력한 성능 (처음으로 두 영역 모두 통일적 성능)

3. **계산 효율성**: 기존 방법 대비 10배 이상 빠른 훈련

4. **일반성**: 복잡한 도메인 아키텍처가 필요 없는 일반적 접근법

#### 7.2 향후 연구 시 고려사항

**단기 (1-2년):**
- TAT와 대조학습의 결합 심화 연구
- 의료, 자율주행 등 실제 응용 도메인에서의 성능 평가
- 하이퍼파라미터 자동 선택 메커니즘 개발

**중기 (2-5년):**
- 기초 모델(CLIP, 대형 언어모델) 기반 도메인 적응
- 다중 도메인 및 개방 집합 문제에 대한 확장
- 테스트 시간 적응과의 통합

**장기 (5년 이상):**
- 적응능력 λ의 명시적 최적화 이론 개발
- 적대적 훈련 이론과 도메인 적응 이론의 완전한 통합
- 연속 도메인 이동(continual domain shift) 환경에서의 적응

#### 7.3 도메인 적응 분야에 미치는 영향

TAT는 **특성 표현 학습 대신 분류기 적응에 초점**을 맞춤으로써, 도메인 적응 연구의 패러다임 전환을 제시합니다. 특히:

- 기존 특성 추출기의 활용 극대화
- 적응 계층의 단순화로 인한 해석 가능성 증대
- 리소스 제한적 환경에서의 실용성 향상

이는 향후 도메인 적응 연구에서 **특성 공간 변형보다는 결정 경계 조정**에 더 많은 관심을 가지게 할 것으로 예상됩니다.

***

### 참고문헌 및 인용

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/0b9e5e12-3e8b-4f6b-b651-b8eec0f4b2df/liu19b.pdf)
[2](https://aclanthology.org/2023.acl-long.92.pdf)
[3](https://arxiv.org/pdf/2112.00428.pdf)
[4](https://arxiv.org/pdf/1702.05464.pdf)
[5](https://arxiv.org/pdf/1809.02176.pdf)
[6](https://arxiv.org/abs/2109.05751)
[7](https://arxiv.org/pdf/1904.05801.pdf)
[8](https://arxiv.org/abs/2301.03826)
[9](https://arxiv.org/abs/1808.04205)
[10](https://arxiv.org/html/2407.12782v1)
[11](https://openaccess.thecvf.com/content_CVPR_2020/papers/Zheng_Efficient_Adversarial_Training_With_Transferable_Adversarial_Examples_CVPR_2020_paper.pdf)
[12](https://arxiv.org/html/2501.18592v1)
[13](https://pmc.ncbi.nlm.nih.gov/articles/PMC11355601/)
[14](https://arxiv.org/html/2506.18516v1)
[15](https://proceedings.neurips.cc/paper_files/paper/2024/file/6b7e1e96243c9edc378f85e7d232e415-Paper-Conference.pdf)
[16](https://www.sciencedirect.com/science/article/abs/pii/S095219762300578X)
[17](https://openreview.net/forum?id=T6RygOFZ6B)
[18](https://www.sciencedirect.com/science/article/abs/pii/S0952197624019845)
[19](https://www.sciencedirect.com/science/article/pii/S0950705125009979)
