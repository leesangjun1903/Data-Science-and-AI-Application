# Learned Initializations for Optimizing Coordinate-Based Neural Representations

## 1. 핵심 주장 및 주요 기여

본 논문의 **핵심 주장**은 좌표 기반 신경망(coordinate-based neural representations)을 최적화할 때 무작위 초기화(random initialization)는 매우 비효율적이라는 점에서 출발합니다. 논문은 메타러닝(meta-learning) 알고리즘을 활용하여 신호의 클래스에 특화된 학습된 초기 가중치를 찾으면, 테스트 시간 최적화에서 **빠른 수렴(faster convergence)**과 **더 우수한 일반화(better generalization)**를 모두 달성할 수 있다고 주장합니다.[1]

**주요 기여**는 다음과 같습니다:[1]

1. **실용적 단순성**: 기존 좌표 기반 신경망 프레임워크에 MAML 또는 Reptile의 외부 루프를 추가하는 몇 줄의 코드만으로 구현 가능합니다.[1]

2. **광범위한 적용성**: 2D 이미지 회귀, CT 스캔 재구성, 3D 물체 재구성, 3D 장면 복원 등 다양한 신호 유형에 성공적으로 적용됩니다.[1]

3. **강력한 사전 정보(Prior)**: 메타 학습된 초기 가중치는 신호 클래스에 대한 강력한 사전 정보로 작동하여, 부분 관찰 상황에서 단일 뷰 재구성도 가능하게 합니다.[1]

## 2. 해결하고자 하는 문제 및 제안 방법

### 2.1 문제 정의

좌표 기반 신경망 표현 $$f_\theta$$는 입력 좌표 $$x \in \mathbb{R}^d$$를 신호 값 $$y \in \mathbb{R}^n$$으로 매핑하는 완전 연결 신경망입니다. 그러나 이러한 네트워크의 주요 제약은:[1]

- 새로운 신호마다 무작위 초기화에서 시작하여 많은 경사 하강 단계를 거쳐야 함
- 작은 이미지 인코딩은 초 단위, 고해상도 3D 장면 복원(예: NeRF)은 시간 단위의 계산 시간 필요[1]

기존 해결책(잠재 벡터 연결, 하이퍼네트워크 사용)은 학습된 잠재 공간 내의 신호만 표현 가능하여 미지의 신호에 제한이 있습니다.[1]

### 2.2 제안하는 방법: 메타러닝 기반 초기화

논문은 신호 분포 $$\mathcal{T}$$에서 나오는 신호들에 대해, 다음을 최소화하는 초기 가중치 $$\theta_0^*$$를 찾는 메타 최적화 문제를 설정합니다:[1]

$$\theta_0^* = \arg\min_{\theta_0} \mathbb{E}_{T \sim \mathcal{T}} L_m(\theta_m(\theta_0, T))$$

여기서 $$\theta_m(\theta_0, T)$$는 신호 $$T$$에 대해 $$\theta_0$$에서 시작하여 $$m$$단계의 경사 하강을 수행한 후의 가중치입니다.[1]

**MAML (Model-Agnostic Meta-Learning)**을 사용할 때의 업데이트 규칙:[1]

$$\theta_0^{j+1} = \theta_0^j - \beta \nabla_{\theta_0} L_m(\theta_m(\theta_0^j, T_j))\big|_{\theta_0=\theta_0^j}$$

여기서 $$\beta$$는 메타 학습 단계 크기입니다. 이는 이계 미분(second-order gradient)이 필요합니다.[1]

**Reptile** (더 메모리 효율적인 대안):[1]

$$\theta_0^{j+1} = \theta_0^j - \beta(\theta_0^j - \theta_m(\theta_0^j, T_j))$$

Reptile은 이계 미분을 필요로 하지 않아 더 복잡한 작업에서 더 많은 내부 루프 단계를 펼칠 수 있습니다.[1]

### 2.3 손실 함수

직접 포인트 관찰이 있는 경우:[1]

$$L = \sum_i \|f_\theta(x_i) - T(x_i)\|_2^2$$

간접 관찰(측정 모델 $$\mathcal{M}$$을 통해)인 경우:[1]

$$L_\mathcal{M} = \sum_i \|\mathcal{M}(f_\theta, p_i) - \mathcal{M}(T, p_i)\|_2^2$$

## 3. 모델 구조

### 3.1 기본 아키텍처

모든 실험에서 **완전 연결 다층 퍼셉트론(MLP)** 사용:[1]

- **이미지 회귀**: 5개 층, 각각 256개 채널, 사인 활성화 함수(SIREN 기반)[1]
- **CT 재구성**: 5개 층, 256개 채널, ReLU 활성화, 랜덤 푸리에 특성 적용[1]
- **ShapeNet 3D 합성**: 6개 층, 256개 채널, ReLU, 위치 인코딩 사용[1]
- **Phototourism 장면**: 6개 층, 256개 채널, ReLU, 위치 인코딩[1]

### 3.2 위치 인코딩

입력 좌표 $$x$$는 다음과 같은 형태의 위치 인코딩을 받습니다:[1]

$$[\cos(2^0 \pi x), \sin(2^0 \pi x), \cos(2^1 \pi x), \sin(2^1 \pi x), \ldots, \cos(2^{N-1} \pi x), \sin(2^{N-1} \pi x)]$$

이는 신경망이 고주파 함수를 학습하도록 돕습니다.[1]

## 4. 성능 향상

### 4.1 빠른 수렴

**이미지 회귀 (CelebA 데이터셋)**:[1]

메타 학습된 초기화를 사용하면 단 2 단계 후 PSNR 30.37을 달성하는데, 표준 초기화는 이를 달성하기 위해 **10~20배 많은 반복**이 필요합니다:[1]

| 초기화 방법 | 2단계 PSNR | 표준과 일치하는데 필요한 반복 횟수 |
|-----------|----------|--------------------------|
| 표준 | 10.88 | 37.92 |
| 평균 | 14.48 | 25.59 |
| 매칭됨 | 13.73 | 26.32 |
| 셔플됨 | 16.29 | 25.80 |
| 메타 | 30.37 | - |

### 4.2 부분 관찰에서의 일반화

**CT 재구성 (제한된 뷰)**:[1]

메타 학습된 초기화는 표준 초기화에 비해 절반의 뷰로 동일한 성능 달성:

| 뷰 수 | 표준 PSNR | 메타 PSNR |
|------|---------|---------|
| 1 | 13.63 | 15.09 |
| 2 | 14.15 | 18.70 |
| 4 | 16.31 | 22.00 |
| 8 | 24.77 | 27.34 |

**단일 뷰 3D 재구성**:[1]

표준 초기화는 단일 뷰에서 의미 있는 형태를 복원할 수 없지만, 메타 학습된 초기화는 클래스 특화 형태 사전을 활용하여 단일 이미지에서 3D 기하학 복원 가능합니다.[1]

### 4.3 데이터셋 간 전이

메타 학습된 초기화는 학습에 사용된 데이터셋에 대한 특화된 사전을 형성합니다:[1]

| 초기화 원본 | CelebA | Imagenette | 텍스트 | SDF |
|-----------|--------|----------|------|-----|
| CelebA | 30.37 | 26.44 | 21.53 | 36.45 |
| Imagenette | 28.51 | 27.07 | 22.63 | 34.80 |
| 텍스트 | 14.65 | 15.83 | 27.85 | 23.14 |
| SDF | 19.80 | 20.05 | 17.23 | 51.73 |

유사한 데이터셋 간(예: CelebA ↔ Imagenette)에는 초기화가 잘 전이되지만, 주파수 특성이 크게 다른 데이터셋 간에는 전이가 제한적입니다.[1]

## 5. 주요 한계

논문에서 명시된 한계점:[1]

1. **데이터셋 요구**: 메타 학습 단계에 목표 신호 분포에서 충분한 샘플이 필요합니다.

2. **테스트 시간 최적화 여전히 필요**: 메타 학습이 빠른 시작점을 제공하지만, 여전히 테스트 시간에 최적화 단계가 필요합니다(즉, 추론 시간이 추가됨).

3. **클래스 특화**: 초기화는 특정 신호 클래스에 특화되어 있어, 매우 다른 신호 유형으로의 일반화는 제한적입니다.

4. **메모리 제약**: MAML은 이계 미분 계산으로 인해 메모리 제약이 있어 더 복잡한 작업에서는 Reptile 사용 필요.[1]

## 6. 일반화 성능 향상 메커니즘

### 6.1 데이터 의존적 사전 학습

메타 학습된 가중치는 신호 클래스의 **데이터 의존적 사전**으로 작동합니다. 이는 다음과 같이 작동합니다:[1]

- 메타 학습 중 다양한 신호 인스턴스에서 공통 구조적 특성을 학습
- 이러한 특성을 초기 가중치에 인코딩
- 테스트 시 불완전한 관찰에서도 이 사전이 가이드로 작용

### 6.2 가중치 공간 vs 신호 공간

중요한 발견은 **평균(Mean)** 및 **매칭됨(Matched)** 기준선 비교에서 드러납니다:[1]

- 평균 기준선: 신호 공간에서 평균을 예측하도록 네트워크를 최적화
- 매칭됨 기준선: 메타 학습 네트워크의 신호 공간 출력과 일치하도록 최적화
- 둘 다 메타 학습된 가중치보다 성능이 낮음

이는 가중치 공간에서의 좋은 초기화가 신호 공간에서의 좋은 초기 예측보다 더 중요함을 시사합니다.[1]

### 6.3 부분 관찰에서의 강화

부분 관찰(예: CT에서의 제한된 뷰)에서 메타 학습 초기화의 이점이 더 두드러집니다. 이는:[1]

- 불완전한 정보로부터 신호를 복원해야 할 때 사전 정보의 가치 증가
- 메타 학습이 인코딩한 클래스별 기하학적 특성이 모호성 해결에 도움

## 7. 최신 연구 동향과 앞으로의 영향

### 7.1 메타러닝의 발전

최근 메타러닝 연구는 다음 방향으로 진화하고 있습니다:[2][3][4]

**신경장 메타러닝의 확장**: 2024-2025년 연구는 메타러닝을 신경장(neural fields)에 더 깊이 통합하고 있습니다. 특히 **메타-연속 학습 신경장(Meta-Continual Learning of Neural Fields, MCL-NF)**은 기존 방법의 재앙적 망각(catastrophic forgetting)과 느린 수렴을 해결하며, 이미지, 오디오, 비디오 재구성 및 뷰 합성 작업에서 빠른 적응을 달성합니다.[4]

**도메인 일반화의 고도화**: 메타러닝 기반 도메인 일반화는 2024-2025년에 더욱 정교해졌습니다. 예를 들어 **산술 메타러닝(Arithmetic Meta-Learning)**은 기울기 매칭 이론을 넘어, 도메인별 최적 매개변수의 중심을 더 정확히 추정하여 균형 잡힌 매개변수를 찾습니다.[5][6]

### 7.2 신경 표현의 향상

**암묵적 신경 표현(INR)의 고도화**: 최근 연구는 주기적 활성화 함수, 위치 인코딩, 법선 데이터를 통합하여 고주파 세부 정보를 더 잘 포착하도록 발전했습니다. 또한 **확산 모델과 INR의 결합(MicroDiffusion)**은 3D 재구성에서 전역 구조 일관성과 세부 정보 향상을 동시에 달성합니다.[7][8]

**메타러닝 기반 최적화**: 2023-2024년 연구에서는 **대규모 신경장의 맥락 기반 메타러닝**이 자동 온라인 맥락점 선택을 통해 메모리 절감을 달성하며, 거의 즉각적인 전역 구조 모델링을 가능하게 합니다.[9]

### 7.3 구성성과 체계적 일반화

**인간 수준의 체계적 일반화**: 2023년 Nature 논문에서 제시된 **메타러닝 합성성(Meta-Learning for Compositionality, MLC)**은 신경망이 인간 수준의 체계적 일반화를 달성할 수 있음을 보여줍니다. 이는 메타러닝이 단순한 효율성 도구를 넘어 근본적인 일반화 능력을 향상시킨다는 점을 시사합니다.[10]

### 7.4 산업적 영향과 미래 방향

**다중 도메인 학습**: 최근 연구는 다중 도메인에서 학습하여 미지의 도메인으로 일반화하는 능력에 초점을 맞추고 있습니다. 논문의 접근법을 확장하여, 여러 신호 클래스에서 동시에 메타 학습하는 것이 더욱 강력한 기본 표현을 산출할 가능성이 있습니다.[11][10]

**효율성과 실시간성**: 신경 표현이 점점 더 복잡한 작업(도시 규모 NeRF, 고해상도 3D 재구성)에 적용되면서, 메타 학습된 초기화는 실시간 또는 준실시간 애플리케이션에서 중요한 역할을 할 것입니다.[4]

## 8. 향후 연구 시 고려할 점

### 8.1 메타 학습 개선

1. **더 정교한 메타 알고리즘**: MAML과 Reptile 이상의 최신 메타 학습 알고리즘(예: 기울기 기반 메타러닝의 변형, 임플리시트 메타 러닝)을 탐색합니다.[12]

2. **하이퍼 매개변수 최적화**: 학습률, 내부 루프 단계 수, 메타 배치 크기 등의 체계적 최적화가 필요합니다.[13]

### 8.2 데이터와 확장성

1. **다중 클래스 메타 학습**: 여러 신호 클래스에서 동시에 학습하여 더 일반적인 초기화 달성.[10]

2. **자기 지도 학습 통합**: 라벨이 없는 신호 분포에서 메타 학습하는 방법 탐구.[14]

### 8.3 이론적 이해

1. **일반화 경계**: 메타 학습된 초기화의 일반화 성능에 대한 이론적 보장 개발.[15]

2. **가중치 공간 기하학**: 메타 학습이 가중치 공간에서 어떤 기하학적 구조를 유도하는지 더 깊이 이해.[10]

### 8.4 응용 확대

1. **적응 네트워크**: 메타 학습을 동적 환경에서 신경 표현이 지속적으로 적응할 수 있도록 확장.[4]

2. **교차 도메인 전이**: 다양한 신호 모달리티(2D 이미지, 3D 기하학, 오디오, 포인트 클라우드) 간의 초기화 전이 연구.[10]

3. **제로샷 및 퓨샷 시나리오**: 완전히 새로운 신호 클래스에 대해 최소한의 또는 제로 샷 적응을 가능하게 하는 메타 초기화.[16]

## 결론

본 논문은 메타러닝과 좌표 기반 신경 표현을 결합한 우아하고 실용적인 접근법을 제시하며, 366회 이상의 인용을 받아 이 분야의 영향력 있는 기여입니다. 특히 단순한 구현으로도 상당한 성능 향상을 달성할 수 있다는 점에서, 신경 표현을 기반으로 하는 다양한 응용(NeRF, 3D 재구성, 의료 영상)의 실용성을 크게 높입니다.[17]

향후 연구는 더 복잡한 메타 학습 알고리즘, 다중 도메인 시나리오, 이론적 일반화 이해, 그리고 완전히 새로운 신호 클래스로의 전이 학습을 탐구할 필요가 있습니다. 특히 메타-연속 학습과 체계적 일반화 분야에서의 최신 진전은 이 기초 논문이 제시한 경로를 지속적으로 발전시키고 있음을 보여줍니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b7045ec2-62f8-4ca0-a378-094123d7d92a/2012.02189v2.pdf)
[2](https://pmc.ncbi.nlm.nih.gov/articles/PMC6519722/)
[3](http://arxiv.org/pdf/2502.01830.pdf)
[4](https://openreview.net/forum?id=OCpxDSn0G4)
[5](https://arxiv.org/html/2503.18987v1)
[6](https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_Balanced_Direction_from_Multifarious_Choices_Arithmetic_Meta-Learning_for_Domain_Generalization_CVPR_2025_paper.pdf)
[7](https://arxiv.org/html/2410.12725v1)
[8](https://openaccess.thecvf.com/content/CVPR2024/papers/Hui_MicroDiffusion_Implicit_Representation-Guided_Diffusion_for_3D_Reconstruction_from_Limited_2D_CVPR_2024_paper.pdf)
[9](https://arxiv.org/pdf/2302.00617.pdf)
[10](https://www.nature.com/articles/s41586-023-06668-3)
[11](https://arxiv.org/html/2404.02785v1)
[12](https://arxiv.org/html/2410.01655)
[13](https://scholarworks.bwise.kr/cau/bitstream/2019.sw.cau/72909/1/Enhancing%20Model%20Agnostic%20Meta-Learning%20via%20Gradient%20Similarity%20Loss.pdf)
[14](https://research.samsung.com/blog/Meta-Learning-in-Neural-Networks)
[15](https://arxiv.org/pdf/2102.06589.pdf)
[16](https://arxiv.org/html/2210.00379v6)
[17](https://openaccess.thecvf.com/content/CVPR2021/papers/Tancik_Learned_Initializations_for_Optimizing_Coordinate-Based_Neural_Representations_CVPR_2021_paper.pdf)
