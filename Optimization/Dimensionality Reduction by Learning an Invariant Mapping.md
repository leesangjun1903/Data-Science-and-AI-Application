# Dimensionality Reduction by Learning an Invariant Mapping  

## 핵심 주장 및 주요 기여  
**핵심 주장**  
고차원 데이터를 저차원 매니폴드로 사영할 때, 전통적 기법은 새로운 샘플에 대한 매핑 함수가 없고, 입력 공간의 거리 측정에 의존한다. 본 논문(DrLIM)은 이 두 가지 한계를 극복하여  
- 이산적 레이블(유사/비유사)만으로 학습 가능한 비선형 매핑 함수를 제안하고  
- 학습된 함수는 새로운 샘플에도 적용 가능하며  
- 변형(invariance) 학습을 통해 조명·기하학적 왜곡에도 강건한 저차원 표현을 얻는다.  
이 네 가지 특징이 DrLIM의 주요 기여이다.[1]

## 해결하고자 하는 문제  
1. **함수 부재**: LLE, ISOMAP 등 스펙트럴 기법은 학습 후 새로운 샘플을 매핑하려면 전체 임베딩을 재계산해야 한다.  
2. **거리 측정 의존**: 대부분 방법이 입력 공간에서 유클리드 거리나 커널 함수를 전제한다.  
3. **군집화·차집합 문제**: 비유사 샘플이 서로 가까이 뭉치는 퇴화 현상(점 밀집) 발생.  

## 제안 방법  
### 1) 쌍 기반 대립 손실 함수 (Contrastive Loss)  
샘플 쌍 $$(\mathbf{x}_1,\mathbf{x}_2)$$과 레이블 $$Y\in\{0,1\}$$을 사용해 다음과 같이 정의한다:  

$$
D_W(\mathbf{x}_1,\mathbf{x}_2) = \|G_W(\mathbf{x}_1) - G_W(\mathbf{x}_2)\|_2
$$  

$$
L(W) = \sum_{i} \Bigl[(1-Y_i)\tfrac12 D_W^2 \;+\; Y_i\tfrac12\max(0,\,m - D_W)^2\Bigr]
$$  

- $$Y=0$$인 유사(same) 쌍은 거리를 최소화하고,  
- $$Y=1$$인 비유사(different) 쌍은 일정 마진 $$m$$ 이상 유지하도록 학습한다.[1]

### 2) 네트워크 구조 (Siamese Architecture)  
- **쌍둥이 분기(twin branches)**: 매개변수 $$\!W\!$$를 공유하는 두 개의 동일 신경망 $$G_W$$.  
- **거리 연산 코스트 모듈**: 두 branch 출력의 유클리드 거리 $$D_W$$ 계산.  
- **손실 모듈**: 위 대립 손실 함수로 $$W$$를 경사 하강법으로 갱신.  

#### 예시 모델  
- MNIST: 2개의 합성곱 계층 + 풀링 + 완전연결 후 2차원 출력 (shift-invariant 학습)  
- NORB: 2-layer fully connected, 은닉 20→출력 3 유닛 (조명 불변 학습)  

## 성능 향상 및 한계  
| 실험 환경              | 기존 기법 한계                              | DrLIM 성과                             |
|-----------------------|-------------------------------------------|-----------------------------------------|
| MNIST (숫자 4 vs 9)   | 새로운 샘플 매핑 불가, 클러스터링 과도 발생         | 2D 상 균등 분포, 테스트 샘플 정확히 매핑  |
| Shift-invariance MNIST| 유클리드 이웃만 사용 시 변형별 클러스터링(5개 군집) | 변형 관계 주입 시 위치 독립적 매핑 달성  |
| NORB (조명·포즈)      | LLE: 조명별 분리, 전역 구조 왜곡                  | 3D 실린더형 매니폴드 획득, 조명 완전 불변 |

- **한계**  
  - 손실 계산 쌍 수 $$O(N^2)$$ → 대규모 데이터셋에선 쌍 샘플링 전략 필요  
  - $$G_W$$ 표현력에 의존 → 복잡한 변형·도메인엔 더 깊거나 특수 구조 요구  
  - 하이퍼파라미터(마진 $$m$$, 네트워크 크기) 민감  

## 일반화 성능 향상 관점  
- **대립 손실의 균형 효과**: 유사·비유사 두 힘(스프링 모델 비유)이 출력 공간을 과도한 수렴·확산 없이 안정적 평형으로 이끈다.  
- **파라미터 공유 구조**: Siamese 네트워크가 동일함수로 학습 → 도메인 편향 감소, 새로운 샘플에 즉시 적용 가능.  
- **사전 지식 삽입**: 유클리드 거리 외에도 시계열 연속성, 데이터 증강(pairing) 이용하여 일반화 대상 변형 추가 학습 가능.  

## 향후 연구 영향 및 고려 사항  
- **응용 확장**: 로봇 내비게이션(카메라 연속 이미지로 위치·방향 추정), 자율주행, 의료 영상 등 변형 불변 특징 학습에 활용 가능.  
- **효율화 과제**: 대규모 데이터셋 대응을 위한 스마트 쌍 선택, 온라인 학습, 지식 증류(distillation) 기법 연구 필요.  
- **모델 구조 발전**: 트랜스포머·그래프 신경망 등 최신 구조로 $$G_W$$ 대체 시 복잡한 도메인 변형에도 일반화 성능 향상 기대.  
- **하이퍼파라미터 자동화**: 마진, 네트워크 깊이·폭, 샘플링 비율 최적화를 위한 자동 튜닝 및 메타러닝 도입 고려.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/e87827ba-9188-4d54-a542-a15e65efe8a8/Dimensionality_Reduction_by_Learning_an_Invariant_Mapping.pdf
