# On the Effects of Batch and Weight Normalization in Generative Adversarial Networks

# 논문의 핵심 요약 및 기여

**핵심 주장:**  
이 논문은 **배치 정규화(Batch Normalization, BN)** 가 GAN(Generative Adversarial Networks) 훈련 초기에는 학습 속도를 높이지만, 장기적으로는 불안정성과 생성 품질 저하를 초래할 수 있음을 보인다. 대신, **가중치 정규화(Weight Normalization, WN)** 를 변형하여 적용하면 GAN의 안정성, 효율성, 및 생성 샘플 품질이 모두 향상된다고 주장한다.[1]

**주요 기여:**  
- GAN에 특화된 WN 기법을 제안하여 BN 대비 **약 10% 낮은 재구성 오류**(reconstruction loss)를 달성.[1]
- GAN 모델 비교를 위한 **테스트 세트 기반 평균 제곱 유클리드 재구성 오차**라는 새로운 객관적 평가지표 제안.[1]
- 표준 DCGAN 및 21층 ResNet 구조 실험을 통해 WN의 **안정성, 속도, 품질 우위**를 정량·정성적으로 입증.[1]

***

## 1. 문제 정의 및 동기

GAN은 샘플 품질은 우수하나 **학습 불안정성(mode collapse, 진동 등)** 문제를 겪는다.  
기존에는 BN을 통해 훈련을 가속화하고 안정성을 부여하려 했으나,  
- 생성 샘플의 **시각적 인공물** 발생  
- **다양성 감소**(mode collapse)  
- **테스트 성능 악화**  

등의 부작용이 관찰된다.[1]

***

## 2. 제안 방법

### 2.1. WN 변형 수식

기존 WN은 가중치 벡터를 정규화만 하지만, 평균 보정이 부족하다. 논문은 다음과 같이 **Translated ReLU (TReLU)** 를 결합한 변형 WN을 제안한다.

Strict WN 레이어:  

$$
y = \frac{w^T x}{\lVert w\rVert}
$$  

TReLU 활성화:  

$$
\mathrm{TReLU}_\alpha(x) = \max(x - \alpha,\,0) + \alpha
$$  

이를 통해 선형 변환 후 평균·분산을 제어하고, 불필요한 배치 통계 의존성을 제거했다.[1]

### 2.2. 모델 구조

- **DCGAN 기반:**  
  - 판별자와 생성자 모두 각 층에 **SWNConv** (strict WN) 및 마지막 층에 **AWNConv** (affine WN) 적용  
  - 활성화로 TPReLU 사용  
- **21-layer ResNet GAN:**  
  - 잔차 블록 내 모든 합연산에 대해 **weight-normalized addition** 적용  
  - TPReLU 및 strict WN으로 대체  

배치 크기, 코드 길이, RMSProp 최적화 등은 표준 DCGAN 설정을 따르며, BN 모델과의 비교를 위해 동일 조건 하에 실험을 진행했다.[1]

***

## 3. 성능 향상 및 한계

### 3.1. 성능 향상

- **재구성 오차:** WN 모델이 BN 대비 약 **10.5% 낮은 최종 재구성 오차** 달성.[1]
- **학습 안정성:** WN은 700k iteration까지도 안정적이며, 샘플 진폭 변동이 적음.[1]
- **학습 속도:** 초기 수백 iteration 내에도 의미 있는 샘플 생성 가능, BN과 유사한 가속 효과 확보.[1]
- **심층 네트워크 적용:** 21-layer ResNet GAN에서도 WN만으로 안정적 훈련·고품질 생성 달성.[1]

### 3.2. 한계

- **완전한 불안정성 해결 아님:** 장기 훈련 시 일부 샘플 품질 저하 관찰.[1]
- **평가 비용:** Latent code에 대한 gradient descent 기반 재구성 평가는 계산량이 많아 **모니터링에는 제한적**임.[1]
- **구현 복잡도:** TReLU, weight-normalized addition 등 세부 구현이 번거로울 수 있음.

***

## 4. 일반화 성능 향상 가능성

- **테스트 세트 기반 재구성:** 학습 데이터 외의 샘플을 재구성할 수 있어 **과적합 방지** 및 분포 학습 여부 평가에 유리하다.  
- **배치 의존성 제거:** BN의 배치 통계 의존을 배제해, **미니배치 크기 변화에도 일관된 성능** 발휘 가능성이 크다.  
- **다양한 아키텍처와 호환:** TReLU와 strict WN 레이어는 다양한 GAN 변형(LSGAN, WGAN, EBGAN 등)에 **추가로 적용**할 수 있어, 일반화 성능 향상을 위한 **모듈식 도구** 역할을 한다.

***

## 5. 향후 연구에의 영향 및 고려 사항

- **다른 안정화 기법과의 결합:** WN은 손실 함수 변경(LSGAN) 또는 판별자 구조 변경(gradient penalty)과 **상호보완적**으로 적용 가능하다.  
- **효율적 평가 지표 개발:** 계산 비용이 큰 재구성 평가를 대체할 **경량화된 정량 지표** 연구가 필요하다.  
- **하이퍼파라미터 민감도 분석:** WN 기반 GAN의 **learning rate, TReLU 초기값** 등에 따른 성능 변동성을 체계적으로 조사해야 한다.  
- **적은 데이터 및 소형 배치 환경:** 배치 통계가 불안정한 설정에서 WN의 진가가 더욱 부각될 수 있으므로, **저자원 학습 시나리오**에서의 적용성을 검증할 필요가 있다.

***

 1704.03971v4.pdf[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/9c0a2c76-1cc7-4473-948b-4722dab6be97/1704.03971v4.pdf)
