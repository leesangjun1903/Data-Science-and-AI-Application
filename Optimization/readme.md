# Optimization
> ## Learning Hyperparameter Tuning, Loss function(Cost function), Objective function optimization, Meta-Learning, Few-Shot Learning, Multi-task Learning, Domain Adaption, Metric Learning, PU-Learning(Positive-Unlabeled Learning), Curriculum Learning, Activation function, Layer Fine-Tuning, Topos and stacks(Not verified)

## Awesome Series
- Awesome Machine Learning for Combinatorial Optimization Resources : https://github.com/Thinklab-SJTU/awesome-ml4co
- Awesome Pruning : https://github.com/he-y/Awesome-Pruning


## Papers
- Cross-Domain Few-Shot Classification via Learned Feature-Wise Transformation | 2020 · 552회 인용, Few-Shot Learning, Learning-to-Learn Approach (Meta-Learning)
- DMTRL : Deep Multi-task Representation Learning: A Tensor Factorisation Approach | 2016 · 341회 인용, Domain adaption
- Efficient Parametrization of Multi-Domain Deep Neural Networks | 2018 · 488회 인용, Domain apaption
- Episodic Training for Domain Generalization | 2019 · 585회 인용, Domain Generalization, Episodic Training, Meta-Learning
- Incremental Learning Through Deep Adaptation | 2017 · 342회 인용, Incremental Learning
- Incremental Multi-domain Learning with Network Latent Tensor Factorization | 2019 · 45회 인용, Domain adaption
- Latent Domain Learning with Dynamic Residual Adapters | 2020 · 5회 인용, Domain adaption, Dynamic Residual Adapters
- Learning multiple visual domains with residual adapters | 2017 · 1158회 인용, Domain adaption, residual apapters
- PT-MAP : Leveraging the Feature Distribution in Transfer-based Few-Shot Learning | 2020 · 233회 인용, Few-shot learning
- Matching Networks for One Shot Learning | 2016 · 9832회 인용, Image classification, One shot learning, Meta-Learning
- MetaReg: Towards Domain Generalization using Meta-Regularization | 2018 · 897회 인용, Domain Generalization, Meta-Learning
- Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights | 2018 · 876회 인용, Domain adaption
- Prototype Rectification for Few-Shot Learning | 2019 · 379회 인용, Image classification
- Prototypical Networks for Few-Shot Learning | 2017 · 11667회 인용, Image classification, Few-shot Learning
- Transductive Information Maximization for Few-Shot Learning | 2020 · 245회 인용, Image classificaiton, Few-Shot Learning
- LCM-LoRA: A Universal Stable-Diffusion Acceleration Module | 2023 · 180회 인용, Accelerate Sampling
- Agent Attention: On the Integration of Softmax and Linear Attention | 2023 · 216회 인용, Attention Mechanism
- Active Learning for Domain Adaptation: An Energy-Based Approach | 2021 · 172회 인용, Image Classification, Domain Adaption
- Oops I Took A Gradient: Scalable Sampling for Discrete Distributions | 2021 · 123회 인용. Discrete Sampling
- Learning the Stein Discrepancy for Training and Evaluating Energy-Based Models without Sampling | 2020 · 114회 인용, Sampling Technique
- ICE-BeeM: Identifiable Conditional Energy-Based Deep Models Based on Nonlinear ICA | 2020 · 172회 인용
- On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models | 2019 · 206회 인용
- Your GAN is Secretly an Energy-based Model and You Should Use Discriminator Driven Latent Sampling | 2020 · 151회 인용, GAN Technique
- Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks | 2019 · 335회 인용, Tree-based Model Robustness
- Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning | 2016 · 1513회 인용, Semi-Supervised Learning
- A Tutorial on Bayesian Optimization | 2018 · 3458회 인용, Optimization
- Taking the Human Out of the Loop: A Review of Bayesian Optimization | 2015 · 7154회 인용, Optimization
- Maxout Networks | 2013 · 3283회 인용
- Representation Learning with Contrastive Predictive Coding | 2018 · 13290회 인용




## Reference
- Advancing medical Imaging Informatics by Deep Learning-Based Domain Adaptation : https://ballentain.tistory.com/m/69
- MMD : https://bommbom.tistory.com/entry/Maximum-Mean-DiscrepancyMMD-%EC%B5%9C%EB%8C%80-%ED%8F%89%EA%B7%A0-%EB%B6%88%EC%9D%BC%EC%B9%98-%EA%B0%9C%EB%85%90-%EC%A0%95%EB%A6%AC
- VAE 계열 모델의 ELBO(Evidence Lower Bound) 분석 : https://glanceyes.com/entry/VAE-%EA%B3%84%EC%97%B4-%EB%AA%A8%EB%8D%B8%EC%9D%98-ELBOEvidence-Lower-Bound-%EB%B6%84%EC%84%9D

## 모두를 위한 컨벡스 최적화
https://convex-optimization-for-all.github.io/

1	Introduction	 

2	Convex Sets	 

3	Convex Functions	 

4	Convex Optimization Basis	 

5	Canonical Problems	 

6	Gradient Descent	 

7	Subgradient	Page	 

8	Subgradient Method	 

9	Proximal Gradient Descent and Acceleration	 

10	Duality in Linear Programs	 

11	Duality in General Programs	 

12	KKT Conditions	Page	 

13	Duality uses and correspondences	Page	 

14	Newton’s Method	Page	 

15	Barrier Method	Page	 

16	Duality Revisited	Page	 

17	Primal-Dual Interior-Point Methods	 

18	Quasi-Newton Methods	 

19	Proximal Netwon Method	 

20	Dual Methods	 

21	Alternating Direction Method of Mulipliers	 

22	Conditional Gradient Method	 

23	Coordinate Descent	 

24	Mixed Integer Programming 1	 

25	Mixed Integer Programming 2	 

## Large-Scale Convex Optimization, Ernest K. Ryu
-- Monotone operator methods  
Monotone operators and base splitting schemes  
Primal-dual splitting methods  
Parallel computing  
Randomized coordinate update methods  
Asynchronous coordinate update methods  

-- Additional topics
Stochastic optimization  
ADMM-type methods  
Duality in splitting methods  
Maximality and monotone operator theory  
Distributed and decentralized optimization  
Acceleration  
Scaled relative graphs  

## BAYESIAN OPTIMIZATION, ROMAN GARNETT
The Bayesian Approach  
Gaussian Processes  
Modeling with Gaussian Processes  
Model assessment, Selection, and Averaging  
Decision Theory for Optimization  
Utility Functions for Optimization  
Common Bayesian Optimization Policies  
Computing Policies with Gaussian Processes  
Implementation  
Theoretical Analysis  
Extensions and Related Settings  
A brief history of Bayesian Optimization 

## Machine Learning A Bayesian and Optimization Perspective, Sergios Theodoridis
What Machine Learning is About
Probability and Stochastic Processes
Learning in Parametric Modeling: Basic Concepts and Directions
Mean-Square Error Linear Estimation
Stochastic Gradient Descent: The LMS Algorithm and its Family
The Least-Squares Family
Classification: A Tour of the Classics
Parameter Learning: A Convex Analytic Path
Sparsity-Aware Learning: Concepts and Theoretical Foundations
Sparsity-Aware Learning: Algorithms and Applications
Learning in Reproducing Kernel Hilbert Spaces
Bayesian Learning: Inference and the EM Algorithm
Bayesian Learning: Approximate Inference and Nonparametric Models
Monte Carlo Methods
Probabilistic Graphical Models
Particle Filtering
Neural Networks and Deep Learning
Dimensionality Reduction

## Linear Algebra and Optimization for Machine Learning
Linear Algebra and Optimization: An Introduction  
Linear Transformations and Linear Systems  
Eigenvectors and Diagonalizable Matrices  
Optimization Basics: A Machine Learning View  
Advanced Optimization Solutions  
Constrained Optimization and Duality  
Singular Value Decomposition  
Matrix Factorization  
The Linear Algebra of Similarity  
The Linear Algebra of Graphs  
Optimization in Computational Graphs  
