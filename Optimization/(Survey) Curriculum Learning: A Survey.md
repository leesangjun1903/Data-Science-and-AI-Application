# (Survey) Curriculum Learning: A Survey

# 핵심 요약

**“Curriculum Learning: A Survey”**는 머신러닝 모델 학습 시 데이터를 *쉬운(easy)* 샘플부터 *어려운(hard)* 샘플 순으로 점진적으로 투입하는 **커리큘럼 학습(Curriculum Learning, CL)** 패러다임을 체계적으로 정리한다.  
주요 기여:
- 커리큘럼 학습의 **일반화**된 알고리즘 공식화  
- 데이터·모델·과제·성능 측정의 **네 가지 수준(level)** 에서의 커리큘럼 관점 제시  
- 손수 만든 **분류 체계(taxonomy)** + TF–IDF 기반 **군집 분석**(hierarchical clustering)을 통한 연구 동향 고찰  
- 다양한 도메인(컴퓨터 비전·자연어 처리·강화학습 등)에서의 **성능 향상** 사례와 한계 제시  

# 상세 설명

## 1. 해결하고자 하는 문제  
기존 딥러닝은 **랜덤 셔플**된 미니배치로 학습하므로,  
- 느린 수렴 속도  
- 불안정한 최적화  
- 국소해에 빠질 위험  
를 안고 있다.  
인간이 *쉬운 개념→어려운 개념* 순으로 학습하는 것처럼, 머신도 **난이도 순서**를 따라 학습하면 수렴 속도·정확도가 개선될 것이라는 가설을 다룬다.

## 2. 제안 방법

### 2.1 일반화된 알고리즘  
Algorithm 1: 커리큘럼 학습 루프  
```
Input:
  M: 모델, E: 데이터셋, P: 성능 측정, n: 에폭 수  
  C: 커리큘럼 기준(Criterion), l: 적용 수준(level), S: 스케줄러  
for t = 1 … n:
  p ← P(M)
  if S(t,p) == true:
    (M, E, P) ← C(l, M, E, P)      # 순서 재조정
  E* ← select(E)                  # 미니배치 선택
  M ← train(M, E*, P)             # 학습 업데이트
end for
```
– $$C$$는 난이도 기준(예: 샘플 손실, 텍스트 길이, 모델 규모)으로 데이터·모델·과제·성능함수 중 하나를 **재정렬(re-rank)**.  
– 스케줄러 $$S(t,p)$$는 **선형·로그 페이스** 또는 **자기 성과 기반(self-paced)** 으로 재순서를 결정.

### 2.2 수식  
일반화 손실 함수 $$\mathcal{L}(M;E)$$ 위에 **가중치 변수** $$v_i\in$$를 도입한 SPL의 대표 예:[1]

$$
\min_{M,\mathbf v} \sum_{i=1}^N v_i \,\ell\bigl(M;x_i,y_i\bigr)+\lambda\,R(\mathbf v),
$$

여기서  
- $$\ell(\cdot)$$은 예제별 손실  
- $$R(\mathbf v)$$는 자기 주도 페이싱 규제항(self-paced regularizer)  
- $$\lambda$$는 단계별 난이도 조절 파라미터  

### 2.3 모델 구조  
본 조사는 특정 모델 제안이 아니라, **커리큘럼 전략**을  
1) 데이터 수준: 샘플 정렬 → 미니배치 샘플링  
2) 모델 수준: 채널 수·레이어 수·드롭아웃 비율 조정  
3) 과제 수준: 하위 과제 점진적 학습  
4) 성능 측정 수준: 손실함수 스무딩  
으로 적용한 사례들을 **포괄적**으로 정리한다.

## 3. 성능 향상 및 한계  
- **수렴 속도**: 초반 학습 단계에서 손실곡선의 매끄러움 → 빠른 최적화  
- **최종 정확도**: 난이도별 정보 획득 순서화 → 성능↑  
- **일반화**: 쉬운·어려운 샘플 경계부터 학습 → 과적합 감소  
- **한계**:  
  - 난이도 측정의 **편향(bias)** 문제 → 특정 클래스·도메인 편중  
  - 과도한 커리큘럼 제약 → 데이터 다양성 감소  
  - 모델·도메인마다 최적 스케줄·기준이 **이질적** → 일반적 솔루션 부재  

## 4. 일반화 성능 관련 논의  
- **스무딩 관점**: 쉬운 샘플만 사용할 때 손실함수는 더 볼록에 가까워져 국소해 탈출이 쉬움  
- **다양성 균형**: BCL(균형 CL)·SPCL(자기주도 CL)로 **대표 클래스·영역** 모두 학습  
- **모델 수준 커리큘럼**: 필터 해상도·드롭아웃 강도 점진 조정 → 초기 저차원 구조에서 복잡 구조로 확장  
- **도전**: 난이도 편향이 심할수록 **OOD(Out-of-Distribution)** 예측 성능 저하 가능성

# 향후 영향 및 고려사항

커리큘럼 학습은  
- 대용량·다양성 높은 데이터 학습에서 **효율성** 개선  
- **Vision Transformer**, **Self-Supervised Learning** 등 최첨단 모델에도 적용 여지  
- **자동 스케줄링**·**난이도 측정** 보편화 메커니즘 필요  

향후 연구 시  
1. **난이도 편향 제어**: 클래스·도메인·샘플 분포 균형 유지  
2. **적응형 스케줄러**: 학습 동력에 따라 자동 조절  
3. **범용성**: 비지도·자기지도 학습, 대규모 프리트레인·파인튜닝 워크플로우에 통합  
4. **이론적 분석**: SGD 외 최적화 관점에서의 안정성 보장  

이 논문은 커리큘럼 학습의 **이론·응용·과제**를 종합적으로 고찰함으로써, 향후 **자기주도적·자동화된 학습 전략** 개발의 초석을 제시했다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/d7540b0b-ed6d-40ae-bcf0-d837f271d345/2101.10382v3.pdf)
