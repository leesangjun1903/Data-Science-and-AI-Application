# Alternating Optimization of Decision Trees, with Application to Learning Sparse Oblique Trees

## 1. 핵심 주장과 주요 기여

이 논문의 핵심 주장은 기존의 탐욕적(greedy) 의사결정 트리 학습 알고리즘의 한계를 극복하기 위해 **Tree Alternating Optimization (TAO)** 알고리즘을 제안한다는 것입니다.[1]

**주요 기여는 다음과 같습니다:**

- **직접적인 오분류 오차 최적화**: 기존 방법들이 불순도(impurity) 측정을 사용하는 대신, 실제 관심사인 오분류 오차를 직접 최적화합니다[1]
- **교대 최적화 접근법**: 서로 후손 관계가 아닌 노드들의 매개변수를 분리하여 병렬로 최적화할 수 있는 분리성 조건을 활용합니다[1]
- **희소 비스듬한 트리 도입**: 각 노드에서 소수의 특성만을 사용하는 새로운 유형의 트리를 제안하여 축 정렬 트리와 비스듬한 트리의 장점을 결합합니다[1]

## 2. 해결하고자 하는 문제와 제안 방법

### 문제 정의

논문은 의사결정 트리 학습의 근본적인 문제를 다룹니다. 최적의 의사결정 트리를 찾는 것은 NP-hard 문제이며, 기존의 CART나 C4.5 같은 알고리즘들은 다음과 같은 한계가 있습니다:[1]

1. **탐욕적 최적화**: 한 번 최적화된 노드는 영원히 고정됩니다[1]
2. **대리 목적함수 사용**: 실제 목표인 오분류 오차 대신 불순도 측정을 사용합니다[1]
3. **비스듬한 트리의 부정확한 최적화**: 좌표 하강법이 좋지 않은 지역 최적해에 빠지는 경향이 있습니다[1]

### 제안 방법: TAO 알고리즘

**수학적 정식화:**

목표 함수는 정규화된 손실 함수로 표현됩니다:

$$E(T) = L(T) + \alpha C(T), \quad \alpha > 0$$

여기서 $$L$$은 훈련 세트에서의 오분류 오차이고, $$C$$는 트리의 복잡도입니다.[1]

고정된 트리 구조에 대해 최적화할 때, 다음과 같이 단순화됩니다:
$$L(\Theta) = \sum_{n=1}^{N} L(y_n, T(x_n; \Theta))$$

**핵심 이론적 기반:**

**정리 3.1 (분리성 조건)**: 서로 후손 관계가 아닌 노드 $$i$$와 $$j$$에 대해, 목적함수 $$L(\Theta)$$는 다음과 같이 분리됩니다:

$$L(\theta_i, \theta_j) = L_i(\theta_i) + L_j(\theta_j) + \text{상수}$$[1]

**정리 3.2 (축소 문제)**: 내부 노드 $$i$$에 대한 최적화는 다음의 이진 분류 문제로 축소됩니다:

$$\min_{\theta_i} \sum_{n \in C_i} L(y_n, f_i(x_n; \theta_i))$$

여기서 $$C_i$$는 "관심 있는" 훈련점들의 집합입니다.[1]

### 모델 구조

TAO는 다음과 같은 구조로 작동합니다:

1. **깊이별 교대 최적화**: 잎(leaves)에서 뿌리(root)로 향하는 순서로 각 깊이 레벨의 모든 노드를 병렬로 최적화합니다[1]
2. **간접적 가지치기**: 최적화 과정에서 죽은 가지(dead branches)와 순수한 부분트리(pure subtrees)가 자동으로 제거됩니다[1]
3. **희소성 정칙화**: ℓ1 페널티 $$\lambda \sum_{\text{nodes } i} \|\mathbf{w}_i\|_1$$를 추가하여 희소한 비스듬한 트리를 학습합니다[1]

## 3. 성능 향상 및 일반화 성능

### 실험 결과

**MNIST 데이터셋에서의 성능:**
- 초기 CART 트리: 훈련/테스트 오차 1.95%/11.03%
- TAO 적용 후: 훈련/테스트 오차 0.09%/5.66%로 드라마틱한 개선[1]
- 트리 크기도 410개에서 230개 내부 노드로 감소[1]

### 일반화 성능 향상 메커니즘

1. **구조적 가지치기**: TAO는 최적화 과정에서 자동으로 트리 크기를 줄여 더 적은 매개변수로 더 나은 일반화를 달성합니다[1]

2. **균형잡힌 트리**: TAO로 최적화된 트리는 더 균형잡혀 있으며, 샘플들이 트리 가지에 더 균등하게 분포됩니다[1]

3. **희소성의 효과**: 희소 비스듬한 트리는 각 노드에서 소수의 특성만 사용하여 과적합을 방지하면서도 복잡한 데이터 상관관계를 모델링할 수 있습니다[1]

4. **정칙화 경로**: LASSO와 유사하게, 희소성 하이퍼파라미터 $$C$$의 함수로서 트리들의 경로를 제공하여 최적의 일반화 성능을 가진 트리를 선택할 수 있습니다[1]

## 4. 한계점

1. **지역 최적해**: TAO는 여전히 지역 최적해로 수렴하며, 전역 최적해에 대한 보장이 없습니다[1]

2. **초기 트리 의존성**: TAO의 성능은 초기 트리의 품질에 의존적입니다[1]

3. **근사 보장 부재**: 현재로서는 근사 보장이 없습니다[1]

4. **NP-hard 본질**: 기본적인 오분류 손실 최적화 문제는 여전히 NP-hard입니다[1]

## 5. 향후 연구에 미치는 영향과 고려점

### 연구에 미치는 영향

1. **의사결정 트리 최적화의 새로운 패러다임**: 불순도 측정 대신 직접적인 오분류 오차 최적화로의 패러다임 전환을 제시합니다[1]

2. **희소 비스듬한 트리의 실용화**: 축 정렬 트리의 해석 가능성과 비스듬한 트리의 유연성을 결합한 새로운 접근법을 제공합니다[1]

3. **IoT 및 자원 제약 환경에서의 응용**: 매우 빠른 추론 시간과 작은 모델 크기로 인해 자원 제약 환경에서의 활용 가능성을 보여줍니다[1]

### 향후 연구 시 고려점

1. **확장 가능한 방향들**:
   - 회귀 트리로의 확장[1]
   - 선형 이분기 이외의 다른 노드 유형[1]
   - 트리 앙상블과의 결합[1]
   - 트리 구조 탐색과의 결합[1]

2. **이론적 개선 필요성**:
   - 근사 보장 개발
   - 수렴 속도 분석
   - 일반화 오차 경계 도출

3. **실용적 고려사항**:
   - 대규모 데이터셋에서의 확장성
   - 다양한 도메인에서의 성능 검증
   - 하이퍼파라미터 선택 자동화

이 논문은 의사결정 트리 학습에서 근본적인 최적화 문제를 해결하는 새로운 접근법을 제시하며, 특히 모델의 해석 가능성과 성능을 동시에 향상시킬 수 있는 희소 비스듬한 트리라는 혁신적인 개념을 도입했습니다. 이는 향후 설명 가능한 AI와 자원 효율적인 머신러닝 연구에 중요한 기여를 할 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/df9ee945-2883-4a6a-ab26-935c4edf1f58/NeurIPS-2018-alternating-optimization-of-decision-trees-with-application-to-learning-sparse-oblique-trees-Paper.pdf)
