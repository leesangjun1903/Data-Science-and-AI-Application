# MODeL: Memory Optimizations for Deep Learning

## 1. 핵심 주장 및 주요 기여  
**MODeL**은 딥 뉴럴 네트워크 훈련 시 발생하는 메모리 병목을 해소하기 위해  
- **텐서의 수명(lifetime)과 메모리 상 위치(location)를 최적화**  
- 이를 **정수 선형 계획법(ILP)** 으로 수식화하여, 오프라인으로 최적 스케줄과 메모리 할당 계획을 계산  
- 평균 **30% 이상의 메모리 사용량 절감**, 최적화 시간 수 초 내외  

주요 기여  
1. 텐서 할당 시점과 해제 시점을 동시에 최적화하는 ILP 공식화  
2. 메모리 단편화(fragmentation)를 완전 제거하는 주소 배치(address generation) 기법  
3. 최첨단 대규모 네트워크(ResNet, Transformer, BERT 등)에 수초 내 적용 가능한 확장성  

## 2. 해결 과제, 제안 기법, 모델 구조, 성능 및 한계  

### 2.1 해결 과제  
- DNN 파라미터 수는 2012–2022년 사이에 100,000배 급증한 반면, GPU 메모리는 단 10배 증가  
- 기존 프레임워크(Pytorch/TensorFlow)의 동적 할당 방식은  
  - **메모리 단편화** 유발  
  - **텐서 수명 최적화 미흡**  
→ 결과: 대형 모델이나 큰 배치(batch) 학습 불가

### 2.2 제안 기법: ILP 기반 수식화  
#### 2.2.1 변수 정의  
- Ce,s ∈{0,1}: 텐서 e를 단계 s에 생성(create)  
- Pe,s ∈{0,1}: 텐서 e를 단계 s에 메모리에 유지(preserve)  
- Ae ∈[0,M]: 텐서 e의 메모리 버퍼 내 오프셋  
- ai,j, bi,j ∈{0,1}: e_i, e_j가 동시에 메모리에 있으면 단편화 방지 제약에 사용  

#### 2.2.2 제약식 (LaTeX)  
1) 생성·유지 상호 배타:  

$$
\forall e,s:\;C_{e,s} + P_{e,s} \le 1
$$  

2) 유지 지속성:  

$$
\forall e,s:\;P_{e,s} \le P_{e,s-1} + C_{e,s-1}
$$  

3) 일회성 생성 강제:  

$$
\forall e:\;\sum_s C_{e,s} = 1
$$  

4) 의존성 보장:  

$$
\forall e,f\in \mathrm{fanin}(e),s:\;C_{e,s} \le P_{f,s}
$$  

5) 형제 텐서 동시 생성:  

$$
\forall e,f\in \mathrm{siblings}(e),s:\;C_{f,s} = C_{e,s}
$$  

6) 단편화 방지(공유 배제):  

$$
\forall i,j:\;a_{i,j}+b_{i,j}\le1,\quad a_{i,j}+b_{i,j}\ge \mathrm{live}\_{i,t}+\mathrm{live}_{j,t}-1
$$  

$$
A_i + S_i \le A_j + M\,(1-a_{i,j}),\quad A_i \ge A_j + S_j - M\,(1-b_{i,j})
$$  

7) 최대 메모리 (peak_mem) 최소화:  

$$
\forall e:\;A_e + S_e \le \mathrm{peak\_mem}
$$  

전체 최적화 문제:  

$$
\min_{C,P,A}\;\mathrm{peak\_mem}
\quad\text{s.t. 위 모든 제약식}
$$

### 2.3 모델 구조 통합  
- **torch.FX**를 이용해 PyTorch 모델을 데이터플로우 그래프 형태로 변환  
- Gurobi ILP 솔버로 최적화  
- 최적 실행 순서(sequence) 및 주소(offset) 매핑 정보로 런타임 전처리  

### 2.4 성능 향상  
- **메모리 사용량**: 배치 크기 1에서 8–45% 절감, 배치 32에서 12–68% 절감; 평균 31.4%/32.8% 절감  
- **단편화**: PyTorch 단편화 7.8%→0, 배치32 단편화 21.3%→0  
- **최적화 시간**: 평균 7.4초, 최악 18.1초, 최단 0.1초  
- **연산 스케줄링만** 고려 시에도 최대 38% 절감 (batch1 평균 23.9%)

### 2.5 한계  
1. **데이터플로우 그래프 필요**: 순환 구조 RNN 등 비정형 제약  
2. **텐서 크기 고정**: 가변 길이 입력의 경우 최악 길이로 최적화  
3. **단일 메모리 공간 가정**: 다중 디바이스 지원 추가 필요  

## 3. 일반화 성능 향상 가능성 검토  
- MODeL은 **정확도 손상 없이** 메모리 최적화  
- 메모리 여유 확보로 더 큰 배치, 더 깊은 네트워크 학습 가능  
- 이는 **모델 일반화** 향상에 기여:  
  - 대형 배치 학습 시 **배치 정규화** 안정화  
  - 더 깊은·넓은 모델 탐색 여지  
- 향후 메모리 제약 해소를 통해 과적합 방지 기법 및 대규모 데이터 학습에 긍정적  

## 4. 향후 연구 영향 및 고려 사항  
- ILP 기반 최적화의 **확장성**: 멀티 GPU·분산 학습 시 메모리 공간 분할 고려  
- **가변 입력 처리**: 동적 그래프 크기에 대한 온라인 최적화 연구  
- **다기준 최적화**: 메모리 외 연산 비용, 통신 비용과의 트레이드오프 통합  
- MODeL과 기존 기법(체크포인팅, 양자화 등) **결합** 연구로 전방위 메모리 절감  

---  
MODeL은 메모리 병목을 해소해 딥 러닝 모델의 확장과 일반화 가능성을 높이며, ILP 기반 최적화 연구의 새 지평을 열었다. 앞으로 멀티 디바이스, 동적 그래프, 통합 비용 최적화 등 실환경 제약을 반영한 확장 연구가 필요하다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/9cf29dd9-a8ad-48b4-b15a-0e20a0412508/steiner23a.pdf
