# Domain Conditioned Adaptation Network

### 1. 핵심 주장과 주요 기여

Domain Conditioned Adaptation Network(DCAN)의 핵심 주장은 **기존 도메인 적응 방법의 완전히 공유된 합성곱 네트워크(shared-convnets) 구조가 원천적 한계를 지닌다**는 것입니다. 연구자들은 소스 도메인과 타겟 도메인의 분포 차이가 클 때, 모든 필터를 동일하게 활성화하는 구조가 도메인 특정 정보 학습을 방해한다고 주장합니다.[1]

DCAN의 주요 기여는 세 가지입니다:[1]

- **도메인 조건 채널 어텐션 모듈**: 소스와 타겟 도메인 각각을 위한 분리된 채널 활성화 경로를 제공하여, 각 도메인이 필요한 채널을 선택적으로 활성화할 수 있도록 함
- **도메인 조건 특징 수정 블록**: 고수준 특징 분포의 불일치를 명시적으로 줄이기 위해 작업 특정 계층 이후에 배치됨
- **광범위한 실험적 검증**: 특히 도메인 간 차이가 큰 도메인넷 등 대규모 벤치마크에서 우수한 성능 입증

### 2. 해결하는 문제와 제안하는 방법

#### 2.1 기본 문제점

기존 도메인 적응 방법들은 다음 두 가지 문제를 안고 있습니다:[1]

**첫째**, 깊은 신경망에서 완전히 공유된 합성곱 계층으로 인해 도메인 특정 정보가 손실됩니다. 예를 들어, 아마존 이미지(깨끗한 배경)와 실생활 이미지(복잡한 배경) 간의 큰 시각적 패턴 차이가 있을 때, 공유 필터는 양쪽 도메인을 동일하게 처리하려고 하므로 도메인 고유의 특징을 제대로 포착하지 못합니다.[1]

**둘째**, 상위 계층에만 도메인 불일치 페널티를 적용하면, 역전파 과정에서 그래디언트가 감소하여 하위 합성곱 계층에 효과적으로 영향을 미치지 못합니다.[1]

#### 2.2 제안 방법

**도메인 조건 채널 어텐션 메커니즘**

전체 입력 특징 텐서를 다음과 같이 정의합니다:

$$X^s = [X^s_1, ..., X^s_C], X^t = [X^t_1, ..., X^t_C] \in \mathbb{R}^{H \times W \times C}$$

여기서 $$C$$는 채널 수, $$H \times W$$는 공간 차원입니다.[1]

**단계 1**: 글로벌 평균 풀링을 통해 각 채널의 전역 공간 정보를 추출합니다:

$$g \in \mathbb{R}^{1 \times 1 \times C}$$

**단계 2**: 감소 비율 $$r=16$$을 사용하여 차원을 $$1 \times 1 \times \frac{C}{r}$$로 축소합니다. 여기서 소스와 타겟은 **서로 다른 FC 계층**을 통과합니다:[1]

$$f_t = \text{FC}_{\text{reduce}}(g), \quad f_s = \text{FC}'_{\text{reduce}}(g)$$

**단계 3**: ReLU를 적용한 후, 공유된 가중치 행렬 $$W \in \mathbb{R}^{C \times \frac{C}{r}}$$을 통해 원래 차원으로 복원합니다:[1]

$$v_s = \sigma(W(\text{ReLU}(f_s))), \quad v_t = \sigma(W(\text{ReLU}(f_t)))$$

여기서 $$\sigma$$는 시그모이드 함수입니다.[1]

**단계 4**: 채널별 어텐션을 원본 특징에 적용합니다:[1]

$$\tilde{X}^s = v_s \odot X^s = [v^1_s \cdot X^1_s, ..., v^C_s \cdot X^C_s]$$

$$\tilde{X}^t = v_t \odot X^t = [v^1_t \cdot X^1_t, ..., v^C_t \cdot X^C_t]$$

여기서 $$\odot$$는 원소별 곱셈입니다.[1]

**도메인 조건 특징 수정 블록**

$$L$$개의 작업 특정 계층 각각 후에 수정 블록을 배치합니다. $$l$$번째 계층의 소스와 타겟 출력을 $$H_l(x_s)$$, $$H_l(x_t)$$라 하면:[1]

수정된 타겟 표현은 다음과 같이 정의됩니다:

$$\tilde{H}_l(x_t) = H_l(x_t) + \Delta H_l(x_t)$$

여기서 $$\Delta H_l(x_t)$$ FC-ReLU-FC 계층으로 구성된 수정 블록이 학습한 불일치입니다.[1]

도메인 간 분포 차이를 최소화하기 위해 최대 평균 불일치(MMD) 기준을 사용합니다:[1]

$$L^l_M = \left\| \frac{1}{n_s}\sum^{n_s}_{i=1}\phi(H_l(x^s_i)) - \frac{1}{n_t}\sum^{n_t}_{j=1}\phi(\tilde{H}_l(x^t_j)) \right\|^2_{H_\kappa}$$

여기서 $$\phi$$는 재현 커널 힐베르트 공간(RKHS)에서의 특징 맵이고, $$\kappa$$는 특성 커널입니다.[1]

**정규화 손실**

과도한 보정을 방지하기 위해, 소스 데이터를 무작위 부분집합과 비교하는 정규화 손실을 도입합니다:[1]

$$L^l_{reg} = \sum^{C_n}_{k=1} \left\| \frac{1}{n^s_k}\sum_{x^s_i \in S_k}\phi(H_l(x^s_i)) - \frac{1}{|R|}\sum_{x^s_j \in R}\phi(\tilde{H}_l(x^s_j)) \right\|^2_{H_\kappa}$$

여기서 $$R$$은 무작위 부분집합, $$|R|$$은 집합 크기입니다[1].

#### 2.3 전체 목적 함수

전체 손실 함수는 다음과 같이 구성됩니다:[1]

$$L = L_s + \alpha\sum^L_{l=1}(L^l_M + L^l_{reg}) + \beta L_e$$

여기서:
- $$L_s = \frac{1}{n_s}\sum^{n_s}_{i=1}E(G(x^s_i), y^s_i)$$ : 소스 분류 손실(교차 엔트로피)
- $$L_e = -\frac{1}{n_t}\sum^{n_t}\_{j=1}\sum^{C_n}_{k=1}G^{(k)}(x^t_j)\log G^{(k)}(x^t_j)$$ : 타겟 엔트로피 최소화 손실
- $$\alpha = 1.5, \beta = 0.1$$ : 가중치 파라미터

### 3. 모델 구조 상세 설명

DCAN의 구조는 다음과 같이 구성됩니다:[1]

**기본 아키텍처**: ResNet-50을 백본 네트워크로 사용하며, 모든 잔차 블록에 도메인 조건 채널 어텐션 모듈을 삽입합니다.[1]

**특징 수정 모듈 배치**: 
- 풀링 계층 이후 하나의 수정 블록 배치 (저수준 특징 정렬)
- 소프트맥스 계층 이후 하나의 수정 블록 배치 (범주 상관 지식 전이)[1]

**약한 공유 파라미터 구조**: 각 도메인이 채널 어텐션 계산 시 서로 다른 감소 경로를 사용하여 도메인 특정 학습을 가능하게 합니다.[1]

**추가 계산 비용**: 도메인 조건 채널 어텐션 모듈은 최소한의 추가 파라미터와 계산량만 필요하므로, 기존 도메인 적응 모델에 쉽게 통합 가능합니다.[1]

### 4. 성능 향상 및 실험 결과

#### 4.1 벤치마크 성능

**Office-31 데이터셋**:[1]
- A→W: 95.0% (이전 최고: 94.5%)
- D→A: 92.6% (2.6% 향상)
- W→A: 74.9% (2.7% 향상)
- 평균 정확도: 89.5% (88.9% 대비 0.6% 향상)

**Office-Home 데이터셋**:[1]
- 평균 정확도: 70.5% (이전 최고 68.1% 대비 2.4% 향상)
- 특히 도메인 간 차이가 큰 작업에서 더 큰 개선

**DomainNet 데이터셋 (대규모, 0.6백만 이미지, 345 카테고리)**:[1]
- ResNet-50 기반: 19.9% (MDD 16.2% 대비 3.7% 향상)
- ResNet-101 기반: 22.3% (MCD 15.4% 대비 6.9% 향상)
- ResNet-152 기반: 24.8% (SE 11.7% 대비 13.1% 향상)

#### 4.2 절제 연구 (Ablation Study)

Office-31에서의 성분별 기여도:[1]

| 방법 | 평균 정확도 |
|------|-----------|
| ResNet-50 베이스라인 | 76.1% |
| DCAN (w/o L₁ᴹ + L₁ᵣₑ𝓰) | 84.4% |
| DCAN (w/o L₂ᴹ + L₂ᵣₑ𝓰) | 87.8% |
| DCAN (w/o Lₑ) | 88.4% |
| DCAN (w/o CA) | 87.5% |
| DCAN (완전) | 89.5% |

결과 해석:
- 정규화 손실 제거: 평균 1.4% 성능 저하
- 채널 어텐션 제거: 2.0% 성능 저하
- 엔트로피 손실 제거: 1.1% 성능 저하

이는 각 구성 요소가 성능에 기여함을 보여줍니다.[1]

#### 4.3 시각화를 통한 분석

**t-SNE 특징 시각화**:[1]
- ResNet-50: 소스와 타겟 도메인이 완전히 분리됨
- DAN: 범주 간 정렬이 충분하지 않음
- DCAN: 클래스 내 응집도가 높고 클래스 간 분리가 명확함

**채널 어텐션 차이 히트맵**:[1]
- Stage 1-3: 밝은 색 (작은 차이 = 공통 저수준 특징)
- Stage 4: 어두운 색 (큰 차이 = 도메인 특정 고수준 특징)
- Office-Home의 더 어려운 작업(Ar→Cl)이 Office-31의 더 쉬운 작업(A→W)보다 더 큰 채널 어텐션 차이를 나타냄

### 5. 모델의 한계

DCAN이 가진 주요 한계는 다음과 같습니다:[1]

**1. 유사한 도메인에서의 제한된 개선**: 도메인이 유사할 때(정확도 90% 이상)는 도메인 특정 특징 학습이 추가 이점을 제공하지 않습니다. 예를 들어, D→W와 A→D 작업에서 DAAA나 SymNets와 비교해 약간 낮은 성능을 보입니다.[1]

**2. 복잡한 소스 도메인의 부정적 전이**: DomainNet에서 소스 도메인이 "rel"(실제)일 때 자주 성능 저하가 발생합니다. 이는 복잡한 소스 정보가 강제 정렬 시 타겟 판별 특징 학습을 방해할 수 있음을 시사합니다.[1]

**3. 배치 크기 제약**: 배치 크기를 32로 제한하고 배치 정규화 계층을 동결했으므로, 더 큰 배치 크기에서의 효과는 불명확합니다.[1]

**4. 초파라미터 민감도**: 정규화 계수 $$\alpha$$와 $$\beta$$, 무작위 샘플링 확률 $$p$$ 등의 선택이 성능에 영향을 미칩니다.[1]

### 6. 일반화 성능 향상의 핵심 메커니즘

DCAN이 일반화 성능을 향상시키는 방식은 다음과 같습니다:[1]

**약한 공유 구조의 역할**: 소스와 타겟이 채널 수준에서 선택적으로 특징을 활성화함으로써, 도메인 불변 특징과 도메인 특정 특징을 동시에 학습할 수 있습니다. 이는 기존의 "도메인 불변성만 추구"하는 방식과 다릅니다.[1]

**다층적 정렬**: 저수준 합성곱에서는 채널 어텐션을 통해 도메인 특정 정보를 선택하고, 고수준 작업 특정 계층에서는 MMD를 통해 판별 분포를 정렬합니다. 이중 전략이 더 효과적인 적응을 가능하게 합니다.[1]

**정규화 손실의 안정화 효과**: 소스 데이터를 무작위 부분집합과 비교함으로써, 과도한 보정을 방지하면서도 효과적인 특징 수정을 유지합니다.[1]

### 7. 최신 연구에 미치는 영향 및 향후 연구 고려사항

#### 7.1 DCAN의 학계 영향

DCAN(2020년 AAAI 발표)은 도메인 적응 분야에서 **144회의 인용**을 받았으며, 이후 여러 중요한 연구의 토대가 되었습니다.[2][3]

**직접적 후속 연구**: 
- **GDCAN (Generalized DCAN, 2021 T-PAMI)**: DCAN의 저자들이 발표한 확장판으로, 각 채널 어텐션 모듈에서 도메인별 활성화 분리 여부를 자동으로 결정하는 기능을 추가했습니다. 이는 매우 유사한 도메인에서도 효과적으로 작동하도록 개선했습니다.[3]

- **도메인 조건 채널 어텐션의 확산**: 이후 많은 연구들이 DCAN의 약한 공유 파라미터 아이디어를 채택했습니다. 예를 들어 Domain Attention Consistency Network (DAC-Net, 2021)도 특징 채널 어텐션 모듈을 사용하여 전이 가능한 특징을 식별합니다.[4]

#### 7.2 Vision Transformer 시대의 진화

**Vision Transformer(ViT)의 도메인 적응으로의 확장**:[5][6]
최근 연구(2024)는 DCAN과 같은 채널 기반 적응 아이디어를 ViT에 적용하고 있습니다. ViT의 자기 주의(self-attention) 메커니즘이 채널 어텐션과 유사한 역할을 수행할 수 있다는 점이 발견되었습니다.[5]

- **Transferable Vision Transformer (TVT, 2021)**: Transferability Adaption Module (TAM)을 통해 ViT의 어텐션 블록에 학습된 전이성을 주입합니다.[7]
- **ViT 도메인 일반화 연구 (2024-2025)**: 최신 연구는 ViT가 전통적인 CNN보다 높은 일반화 능력을 보여줌을 입증합니다.[6]

#### 7.3 Source-Free Domain Adaptation (SFDA)의 부상

**새로운 패러다임**:[8][9][10]
최근 실용적 요구에 따라 SFDA가 중요하게 대두되고 있습니다. SFDA는 소스 데이터에 접근할 수 없이 사전 학습 모델이 새 도메인에 적응하도록 하는 방식입니다. DCAN의 특징 수정 아이디어가 SFDA 맥락에서도 활용되고 있습니다.[10][8]

#### 7.4 도메인 일반화(Domain Generalization) 방향

**Unsupervised Domain Generalization (UDG, 2022)**:[11]
DCAN과 달리 UDG는 라벨 없는 여러 소스 도메인에서 보이지 않은 타겟 도메인으로 일반화하는 문제입니다. Domain-Aware Representation Learning (DARLING, 2022)은 대조 학습과 도메인 인식을 결합합니다.[11]

이는 DCAN의 도메인 조건 접근법과 자기 지도 학습(self-supervised learning)을 통합하는 방향으로의 진화를 보여줍니다.[11]

#### 7.5 의료 이미징과 실제 응용

**Cross-Population Domain Shift (2025)**:[12]
최근 의료 이미징 연구(2025)는 흉부 X선 분류에서 인종 간 도메인 차이를 다루고 있습니다. DCAN과 같은 특징 수준 적응 기법이 90%+ 정확도를 달성하며, CNN보다 37%까지 높은 성능 향상을 보여줍니다.[12]

#### 7.6 향후 연구 시 고려할 점

**1. ViT와의 통합**:
DCAN의 채널 어텐션 개념을 ViT의 토큰 기반 어텐션과 통합하는 방법을 탐색해야 합니다. ViT의 본질적 유연성이 DCAN의 도메인 특정 학습을 더 강력하게 할 수 있습니다.[6][5]

**2. 계산 효율성**:
대규모 모델(ViT-Large, ViT-Huge)에서 도메인 조건 어텐션 모듈의 계산 오버헤드를 최소화하는 방안이 필요합니다. 현재 DCAN은 구현이 간단하지만, 대규모 사전 학습 모델에서의 효율성은 미검증입니다.[6]

**3. Source-Free 시나리오**:
SFDA 맥락에서 DCAN의 정규화 손실을 재설계하는 연구가 필요합니다. 소스 데이터가 없을 때, 타겟 데이터만으로 과도한 보정을 방지하면서 도메인 특정 특징을 학습하는 방법을 개발해야 합니다.[9][8][10]

**4. 도메인 유사성의 적응적 조정**:
GDCAN이 도입한 자동 선택 메커니즘을 더 정교하게 발전시켜, 각 계층과 각 채널에서 도메인 특정성의 정도를 동적으로 조정하는 방법을 탐색할 가치가 있습니다.[3]

**5. 다중 소스 도메인 적응**:
DCAN을 다중 소스 적응으로 확장할 때, 어떻게 여러 도메인 간 채널 선택을 조정할지에 대한 연구가 필요합니다. Domain Attention Consistency Network의 접근이 참고가 될 수 있습니다.[4]

**6. 도메인 시프트 측정 및 진단**:
최근 연구는 CKA(Centered Kernel Alignment)를 사용하여 도메인 시프트를 정량화하는 방법을 제안합니다. DCAN과 같은 적응 방법이 실제로 어느 정도의 도메인 시프트에 대처 가능한지 체계적으로 측정하는 프레임워크 개발이 필요합니다.[13]

**7. 분포 외 데이터(Out-of-Distribution) 및 노이즈 견고성**:
최신 ViT 기반 연구(2025)는 폐색과 노이즈에 대한 견고성을 평가합니다. DCAN의 도메인 조건 채널 어텐션이 이러한 분포 외 조건에서도 효과적인지 검증해야 합니다.[14]

### 결론

DCAN은 도메인 적응 분야에서 **"도메인 불변성만이 아닌 도메인 특정성도 함께 학습해야 한다"**는 중요한 통찰을 제시했습니다. 약한 공유 파라미터 구조와 다층적 정렬 전략을 통해 특히 도메인 간 큰 차이가 있을 때 우수한 성능을 달성했습니다.[1]

최신 연구 동향은 DCAN의 아이디어가 Vision Transformer, 자기 지도 학습, Source-Free 적응 등 새로운 패러다임으로 확장되고 있음을 보여줍니다. 앞으로의 연구는 ViT 기반 모델로의 통합, 실제 의료 이미징 등 고위험 응용 분야로의 확대, 그리고 분포 외 데이터에 대한 견고성 확보에 초점을 맞춰야 견고성 확보에 초점을 맞춰야 할 것입니다.[28][22][27]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/e1b04b14-257e-4378-94d7-97cfac8c9567/2005.06717v1.pdf)
[2](https://arxiv.org/abs/2005.06717)
[3](https://pubmed.ncbi.nlm.nih.gov/33646945/)
[4](https://arxiv.org/abs/2111.03911)
[5](https://arxiv.org/pdf/2404.04452.pdf)
[6](https://arxiv.org/abs/2404.04452)
[7](https://arxiv.org/pdf/2108.05988.pdf)
[8](https://www.sciencedirect.com/science/article/abs/pii/S0925231223010445)
[9](https://openreview.net/forum?id=kUCgHbmO11)
[10](https://proceedings.iclr.cc/paper_files/paper/2025/file/e85454a113e8b41e017c81875ae68d47-Paper-Conference.pdf)
[11](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Towards_Unsupervised_Domain_Generalization_CVPR_2022_paper.pdf)
[12](https://www.nature.com/articles/s41598-025-95390-3)
[13](https://arxiv.org/html/2404.08184v1)
[14](https://arxiv.org/html/2504.04225v1)
[15](http://arxiv.org/pdf/2108.06383.pdf)
[16](http://arxiv.org/pdf/2103.12339.pdf)
[17](https://arxiv.org/pdf/2202.13310.pdf)
[18](https://arxiv.org/html/2502.06272v1)
[19](https://www.aclweb.org/anthology/2020.coling-main.603.pdf)
[20](https://arxiv.org/html/2403.17958)
[21](https://www.sciencedirect.com/science/article/pii/S0924271625001224)
[22](https://www.sciencedirect.com/science/article/pii/S0951832022003179/pdf)
[23](https://pure.ewha.ac.kr/en/publications/deep-unsupervised-domain-adaptation-a-review-of-recent-advances-a)
[24](https://ieeexplore.ieee.org/iel8/6287639/10820123/11039780.pdf)
[25](https://arxiv.org/abs/2208.07422)
[26](https://github.com/BIT-DA/GDCAN)
[27](https://arxiv.org/pdf/2502.14214.pdf)
[28](http://arxiv.org/pdf/2308.03322.pdf)
[29](http://arxiv.org/pdf/2205.13535.pdf)
[30](https://arxiv.org/html/2403.09394v1)
[31](https://arxiv.org/abs/2407.02900)
[32](https://arxiv.org/pdf/2207.11860v2.pdf)
[33](https://www.themoonlight.io/ko/review/vision-transformers-in-domain-adaptation-and-domain-generalization-a-study-of-robustness)
[34](https://www.osti.gov/servlets/purl/1833783)
[35](https://dl.acm.org/doi/abs/10.1007/s00521-024-10353-5)
