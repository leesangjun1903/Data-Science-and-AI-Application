# Distilling the Knowledge in a Neural Network

## 핵심 주장 및 주요 기여  
“Distilling the Knowledge in a Neural Network” 논문은 **큰 규모의 모델 또는 여러 모델(ensemble)** 이 학습한 **일반화 능력**을 **작고 경량화된 모델**에 전이하여, 배포 시 계산 비용을 크게 줄이면서도 성능 저하를 최소화하는 **지식 증류(distillation)** 기법을 제안한다.  
- **Soft targets**: 큰 모델이 출력하는 확률 분포(softmax의 온도를 높여 얻은 부드러운 확률)를 작은 모델의 학습 목표로 삼아, 단일 hard label보다 풍부한 일반화 정보를 제공  
- **Temperature 조절**: 소프트맥스의 온도 $$T$$를 조절하여 작은 확률 간의 상대적 크기 정보도 학습에 반영  
- **Ensemble compression**: 여러 모델의 예측을 평균하여 생성한 소프트 타깃으로 단일 모델에 앙상블 효과를 전이  

## 1. 해결하고자 하는 문제  
대규모 데이터셋과 복잡한 네트워크를 학습할 때,  
- **학습 단계**에서는 수많은 연산량과 높은 자유도를 통해 우수한 성능 달성 가능  
- 그러나 **배포 단계**에서는 대기시간 및 자원 제약으로 대규모 모델 운용이 어려움  
이를 해결하기 위해, **앙상블 모델의 일반화 성능을 소형 모델에 효율적으로 압축**하는 방법이 필요하다.

## 2. 제안 방법  
### 2.1 Soft Targets 기반 지식 증류  
- 큰(또는 앙상블) 모델의 로짓(logits) $$v_i$$를 소프트맥스 온도 $$T$$로 변환하여 소프트 타깃 확률 $$p_i$$ 생성:  

$$
p_i = \frac{\exp(v_i / T)}{\sum_j \exp(v_j / T)}
$$

- 작은 모델의 예측 로짓 $$z_i$$에 동일한 $$T$$ 적용하여 예측 분포 $$q_i$$ 계산  
- **목적 함수**: 소프트 타깃과의 교차 엔트로피 $$C_{\text{soft}}$$와 실제 정답 하드 타깃과의 교차 엔트로피 $$C_{\text{hard}}$$를 가중 합산  

$$
C = \alpha \, T^2 \sum_i p_i \ln q_i \;+\; (1-\alpha) \sum_i y_i \ln \hat{q}_i
$$

($$\hat{q}$$는 $$T=1$$인 소프트맥스)  

- $$T^2$$로 스케일링하여 온도 변화에도 두 손실 기여도가 일정하도록 조정  

### 2.2 Matching Logits의 특수 사례  
- 온도가 매우 클 때, 증류 손실은 로짓 간 제곱 오차와 동등해짐:  

$$
\frac{\partial C}{\partial z_i}\approx \frac{1}{N T^2}(z_i - v_i)
$$

## 3. 모델 구조 및 실험 성능  
### 3.1 MNIST  
- **큰 모델**: 히든 유닛 1200×2, dropout, 입력 데이터 왜곡  
- **작은 모델**: 히든 유닛 800×2 → 증류 적용 시 테스트 오류 146 → 74로 대폭 개선  
- **온도 민감도**: 은닉 유닛 300×2 이상일 때 $$T\ge8$$ 양호, 작은 모델(30×2)일 때 $$T=2.5\sim4$$ 최적  

### 3.2 음성 인식(Deep Acoustic Model)  
- **기본 모델**: 8층×2560유닛, 출력 14,000 상태, 2,000시간 데이터 → WER 10.9%  
- **10개 앙상블**: 프레임 정확도 61.1%, WER 10.7%  
- **증류 단일 모델**: 프레임 정확도 60.8%, WER 10.7% → 앙상블 이득 80% 이상 전이  

## 4. 일반화 성능 향상  
- **Soft targets**는 작은 모델이 앙상블이 학습한 **미묘한 클래스 간 관계**(예: 잘못 분류된 경우의 상대적 확률)를 학습하도록 하여, **표준 hard label 학습 대비** 적은 데이터로도 **우수한 일반화** 달성  
- **데이터 부족 실험**(원 데이터의 3%만 사용): hard label만으로 학습 시 심각한 과적합(WER 급락) 발생, soft targets 적용 시 **전체 학습 세트 성능의 98% 이상** 회복  

## 5. 한계 및 고려 사항  
- **온도 및 손실 가중치 튜닝** 필요: 작은 모델의 크기나 복잡도에 따라 최적 $$T$$와 $$\alpha$$ 상이  
- **전이 데이터 선택**: 원본 학습 데이터 외 별도 전이 셋이 필요할 수도 있으며, 전이 데이터의 분포가 원본과 다르면 성능 저하 위험  
- **로짓 표현 한계**: 매우 음수인 로짓은 정보량이 적어 무시될 수 있으나, 일부 유의미한 정보 손실 가능  

## 6. 향후 연구 영향 및 고려 사항  
- **경량화 모델 배포**: 모바일·임베디드 딥러닝에 핵심 기술로 자리매김  
- **교차 도메인 전이 학습**: 소프트 타깃을 활용한 도메인 적응(domain adaptation) 가능성  
- **연속 학습 및 온라인 학습**: 증류 기법을 통해 이전 모델 지식 유지와 신규 지식 학습의 균형  
- **다중 전문가(specialists) 증류**: 다양한 소규모 전문가 모델과 일반 모델 앙상블 → 단일 모델로 압축 및 지식 융합 연구  

위 기법은 **앙상블 효과를 단일 신경망으로 압축**해 효율적인 배포와 **강력한 일반화**를 동시 달성하는 혁신적 접근으로, 이후 경량화·도메인 적응·연속 학습 등 다양한 분야에 응용될 전망이다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/118983ba-1469-4f6f-aefd-3f70cca1c7f0/1503.02531v1.pdf)
