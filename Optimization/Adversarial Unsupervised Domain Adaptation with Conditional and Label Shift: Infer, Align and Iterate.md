# Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate

### 1. 핵심 주장 및 주요 기여

본 논문은 **비지도 도메인 적응(Unsupervised Domain Adaptation, UDA)에서 조건부 시프트(conditional shift)와 라벨 시프트(label shift)를 동시에 처리**하는 새로운 방법론을 제시한다. 기존의 UDA 방법들은 공변량 시프트(covariate shift)만 가정하고 특정 클래스의 분포 변화를 무시했는데, 이 논문은 현실의 도메인 간 분포 불일치가 훨씬 복잡함을 논증한다.[1]

**주요 기여는 다음과 같다:**[1]
- 조건부 시프트와 라벨 시프트가 공존하는 UDA 환경에서 기존 적대적 학습 방법의 한계를 이론적으로 분석
- **"Infer, Align, Iterate"** 전략을 기반으로 한 새로운 최적화 방안 제시
- 훈련 단계에서 목표 라벨 분포 $$p_t(y)$$를 추정하고 조건부 분포 $$p_s(x|y)$$와 $$p_t(x|y)$$를 반복적으로 정렬
- 테스트 단계에서 사후 분포 $$p_t(y|x)$$를 정확하게 정렬

***

### 2. 해결하고자 하는 문제 및 이론적 배경

#### 2.1 기존 방법의 한계

기존의 적대적 UDA 방법들은 다음의 베이즈 정리에 기반한다:[1]

$$p(f(x)|y) = \frac{p(y|f(x))p(f(x))}{p(y)}$$

이들은 판별자(discriminator)를 통해 $$p_s(f(x)) = p_t(f(x))$$를 강제함으로써 도메인 불변 표현을 학습한다고 가정했다. 그러나 이 접근 방식은 두 가지 비현실적인 가정에 기반한다:

1. **개념 시프트 부재**: $$p_s(y|f(x)) = p_t(y|f(x))$$
2. **라벨 시프트 부재**: $$p_s(y) = p_t(y)$$

#### 2.2 핵심 이론적 발견

**정리 2 (Label Shift 효과):**[1] $$p_s(x|y) = p_t(x|y)$$이고 $$p_s(y) \neq p_t(y)$$일 때, 적절한 특징 추출기 $$f(\cdot)$$가 $$p_s(f(x)) = p_t(f(x))$$를 만족하면:

$$p_s(y|f(x)) \neq p_t(y|f(x))$$

즉, **라벨 시프트 상황에서 기존의 도메인 불변 정렬은 개념 시프트를 야기한다.** 극단적으로 모든 샘플이 같은 점 $$f_0$$으로 매핑되면 분류기의 판별력이 완전히 상실된다.[1]

**조건부 시프트와 라벨 시프트의 충돌:** 법칙의 전체 확률에 따라:[1]

$$\sum_{i=1}^{c} p_s(f(x)|y=i)p_s(y=i) = \sum_{i=1}^{c} p_t(f(x)|y=i)p_t(y=i)$$

만약 각 클래스의 샘플이 특징 공간에서 완벽하게 구분 가능하다면, 이 정렬은 $$p_s(y=i) = p_t(y=i)$$를 강제하게 되어 라벨 시프트 가정과 모순된다.[1]

---

### 3. 제안 방법 및 모델 구조

#### 3.1 조건부 시프트 정렬 (Section 3.2)

논문은 **클래스별 가중치 파라미터** $$\gamma \in \mathbb{R}^c$$를 도입한다. 목표는 $$\gamma_i p_s(y=i) = p_t(y=i)$$를 만족하는 $$\gamma^*$$를 찾는 것이다.[1]

수정된 최적화 목표는:

```math
\max_{D\&C} \left\{\sum_{i=1}^{c} \gamma_i p_s(y=i) \mathbb{E}_{x \sim p_s(x|y=i)} \log C(f(x), y|Y) + \mathbb{E}_{x \sim p_s} \log(1-C(f(x), c+1)) + \mathbb{E}_{x \sim p_t} \log(C(f(x), c+1))\right\}
```

```math
\max_{f} \left\{\sum_{i=1}^{c} \gamma_i p_s(y=i) \mathbb{E}_{x \sim p_s(x|y=i)} \log C(f(x), y|Y) + \lambda \mathbb{E}_{x \sim p_t} \log(1-C(f(x), c+1))\right\}
```

여기서 **Joint Dis&Cls 파라미터화**를 채택한다. 이는 특정 클래스에 대한 판별기의 인식도를 높인다.[1]

**명제 1:** 결합 파라미터화는 정리 1의 $$L_t(h)$$를 $$L_s(h)$$로 더 나은 근사를 제공하여, 도메인 시프트의 영향을 감소시킨다.[1]

#### 3.2 목표 라벨 분포 추정 (Section 3.3)

조건부 분포가 정렬되었다고 가정하면, 평균 매칭(mean matching)을 통해 $$p_t(y)$$를 추정할 수 있다:[1]

$$L_M(p_t(y)) = ||M_s p_t(y) - \mu_t||_2^2$$

여기서:
- $$M_s = [\mu_s(f(x)|y=1), \mu_s(f(x)|y=2), \ldots, \mu_s(f(x)|y=c)]$$: 소스 도메인의 클래스별 평균
- $$\mu_t$$: 목표 도메인의 인코딩된 특징 평균

**정리 3 (점근 일관성):**[1] $$p_s(f(x)|y) = p_t(f(x)|y)$$를 가정하고, 특징 공간의 분산이 무한하지 않으며 모든 라벨 비율이 0이 아니라면:

$$\text{샘플 수} \to \infty \text{일 때, 추정된 } p_t(y) \rightarrow \text{실제 } p_t(y)$$

이는 $$M_s^T M_s$$가 가역성을 만족할 때 성립한다.[1]

#### 3.3 대체 최적화와 Fade-in $$p_t(y)$$ (Section 3.4)

훈련 단계에서 세 개의 학습 플레이어(특징 추출기 $$f(\cdot)$$, 결합 Dis&Cls, $$p_t(y)$$ 추정기)를 반복적으로 업데이트한다:[1]

**[Infer]:** $$f(\cdot)$$ 고정, 목표 라벨 분포 추정기 업데이트
$$\frac{\partial L_M(p_t(y))}{\partial p_t(y)}$$

**[Align]:** $$p_t(y)$$ 추정기 고정, 특징 추출기와 결합 Dis&Cls를 GAN 프로토콜에 따라 업데이트

**Fade-in 전략:** 훈련 초반에 $$p_t(y)$$의 정확한 추정이 어렵기 때문에:[1]
$$\frac{1}{1+\alpha}\{p_t(y) + \alpha p_s(y)\}$$
형태로 초기화하며, $$\alpha = \frac{1}{1+N}$$ (에포크 $$N \leq 5$$), $$\alpha = 0$$ ($$N > 5$$)로 점진적으로 감소시킨다.

#### 3.4 사후 분포 정렬 (Section 3.5)

테스트 단계에서 목표 도메인 분류기를 다음과 같이 보정한다:[1]

$$p_t(y=i|f(x)) \leftarrow \frac{\gamma_i p_s(y=i|f(x))}{||\gamma||_1} \sum_{j=1}^{c} p_s(y=j|f(x))$$

여기서 추정된 $$p_t(y)$$를 사용하여 $$\gamma$$를 계산한다.[1]

***

### 4. 성능 향상 분석

#### 4.1 분류 작업 (VisDA17 벤치마크)

표 1 결과에 따르면:[1]
- **CLS (전체 방법)**: 81.6% ± 0.4% 평균 정확도
- **CLS + TDDA**: 82.9% ± 0.5% (추가 정규화 손실 적용)
- **CLS (ResNet152)**: 83.8% ± 0.5% (더 강한 백본)

주요 비교:
- DANN (기준선): 57.4% → **CLS: 81.6%** (+24.2%p)
- TDDA (최근 방법): 74.03% → **CLS: 81.6%** (+7.57%p)
- CBST (자기학습): 78.1% → **CLS: 81.6%** (+3.5%p)

**라벨 시프트에 대한 견고성:** Figure 1에서 CLS는 KL(p_s(y), p_t(y))이 증가해도 다른 방법들보다 성능 저하가 적다.[1]

#### 4.2 의미론적 분할 작업 (GTA5 → Cityscapes)

표 2 결과:[1]
- **CLS (ResNet-101)**: 46.5% mIoU
- **CLS + APODA**: 51.2% mIoU
- **CLS + IAST-MST**: 53.0% mIoU (자기학습 결합)

- CRST (자기학습): 46.6% → **CLS: 46.5%** (동등)
- AdaptSegNet 기반: 43.3% → **CLS+: 51.2%** (+7.9%p)

#### 4.3 부분 UDA (Partial UDA) 작업

**Office-31 (Table 3)**: 평균 97.94% (Joint Dis+Cls 없이) → **98.24%** (전체)
- BAUS 이전 SOTA: 97.81% → **CLS: 98.24%** (+0.43%p)

**Cityscapes → NTHU (Table 4, Partial Segmentation)**:
- ResNet-101 백본: **50.7% mIoU** (GCAA: 47.6%, CBST: 48.2%)

***

### 5. 모델 일반화 성능 향상 가능성

#### 5.1 이론적 기초

CLS의 일반화 성능 향상은 여러 이론적 기반 위에 있다:

**1) 도메인 적응 이론의 개선 (정리 1 기반):**[1]
정리 1에서 목표 손실은:
$$L_t(h) \leq L_s(h) + d[p_s(f(x)), p_t(f(x))] + \min[..., ...]$$

명제 1에 의해, 결합 파라미터화를 통해 $$L_t(h)$$를 $$L_s(h)$$로 더 정확하게 근사하므로 도메인 시프트의 영향이 감소한다.

**2) 클래스 수준의 가중치 메커니즘:**[1]
예시 수준의 가중치보다 클래스 수준의 가중치 $$\gamma_i$$를 사용하면:
- 계산 효율성 향상 (픽셀 수준 분류인 분할에서 특히 중요)
- 더 안정적인 가중치 추정
- 대규모 작업에서 확장성 향상

**3) 사후 정렬의 정확성:**[1]
테스트 시 $$p_t(y|x) = \frac{\gamma_i p_s(y=i|f(x))||\gamma||_1}{\sum_j p_s(y=j|f(x))}$$ 형태로 정렬하면, 소스 도메인의 의사 편향을 제거하고 목표 도메인의 실제 분포에 맞춘다.

#### 5.2 실험적 증거

**KL-발산 분석 (Figure 5):** 추정된 $$p_t(y)$$와 실제 $$p_t(y)$$ 간의 KL-발산이:[1]
- VisDA17: 훈련 초기 0.7에서 테스트 시 0.1 이하로 감소
- GTA5 → Cityscapes: 유사한 패턴으로 수렴

**절제 연구:**[1]
- CLS w/o Joint Dis+Cls: 80.8% ± 0.6% (VisDA17) → -0.8%p
- CLS w/o Fade-in p_t(y): 81.3% ± 0.5% → -0.3%p
- 전체 CLS: 81.6% ± 0.4%

각 컴포넌트의 기여도를 확인할 수 있다.

#### 5.3 부분 UDA에서의 일반화

부분 UDA는 라벨 시프트의 극단적 경우(일부 클래스 비율이 0)이다. CLS가 이 설정에서도 우수한 성능을 보이는 것은:[1]
- 조건부 시프트와 라벨 시프트를 동시에 처리하는 능력
- 클래스 수준의 문제 진단으로 음의 전이 완화

***

### 6. 한계 및 제한사항

#### 6.1 이론적 한계

1. **조건부 정렬 가정:** Theorem 3의 점근 점근 일관성은 $$p_s(f(x)|y) = p_t(f(x)|y)$$를 가정하지만, 실제로는 이것이 정확하게 만족되지 않는다. 따라서 추정 오류가 축적될 수 있다.

2. **선형성 가정 회피:** 논문은 기존 방법들의 선형성 가정(특히 Zhang et al., 2013)을 피했지만, 평균 매칭도 암묵적으로 일차 모멘트만 활용한다.[1]

3. **단일 소스 도메인:** 정리 3은 단일 소스 도메인만 고려하며, 다중 소스로의 확장은 선형 미정이다.[1]

#### 6.2 실험적 한계

1. **백본 의존성:** 더 강한 백본(ResNet152)을 사용하면 성능이 크게 향상되지만, 이는 CLS 자체의 기여도를 명확하게 분리하기 어렵게 만든다.[1]

2. **훈련 시간:** CLS의 훈련 시간은 DANN의 1.5배이며, 추론 속도는 동일하다. 반복적 최적화의 계산 비용이 증가한다.[1]

3. **Fade-in 파라미터:** $$\alpha = \frac{1}{1+N}$$ (N ≤ 5), $$\alpha = 0$$ (N > 5)의 선택은 휴리스틱이며, 데이터셋마다 최적화되지 않을 수 있다.[1]

4. **라벨 분포 추정의 신뢰성:** 초기 훈련 단계에서 $$p_t(y)$$의 추정이 정확하지 않으면 피드백 루프가 조기에 수렴할 수 있다.[1]

#### 6.3 기술적 한계

1. **대체 최적화의 수렴성:** 정합성이나 수렴 조건에 대한 이론적 보장이 없다. 세 개의 플레이어를 반복적으로 업데이트할 때 진동할 가능성이 있다.[1]

2. **분포 불일치 측정:** KL-발산을 분포 불일치의 주요 척도로 사용하지만, Wasserstein 거리나 다른 f-발산이 더 나을 수 있다.[1]

3. **결합 Dis+Cls 설계:** c+1개 출력 유닛을 사용하는 설계는 직관적이지만, 재파라미터화(식 7)의 수치적 안정성이 명확하지 않다.[1]

***

### 7. 최신 연구 기반 미래 영향 및 고려사항

#### 7.1 현재 연구 트렌드와의 연계

**2024년 최신 발전:** UDA 분야의 최근 연구들은 CLS의 접근 방식을 다양하게 확장하고 있다:[2][3][4]

1. **라벨 시프트 보정의 정교화:** CCA-LSC(Contrastive Conditional Alignment based on Label Shift Calibration)는 CLS의 클래스별 가중치 개념을 샘플 가중 이동 평균 중심 정렬과 결합하여 불균형 도메인 적응에서 성능을 향상시킨다.[2]

2. **확률 모델을 통한 확장:** SPDIM(Source-Free UDA for EEG)은 CLS의 라벨 분포 추정 아이디어를 Riemannian 기하학과 결합하여 신경 신호 처리에 적용한다. 이는 CLS가 의료 영상뿐만 아니라 생의학 신호 처리까지 확장 가능함을 보여준다.[3]

3. **조건부 지원 정렬:** CASUAL(Conditional Support Alignment)은 조건부 특징 분포의 지원(support)을 정렬하는 이론적 틀을 제시하며, CLS의 조건부 시프트 정렬 개념을 이론적으로 강화한다.[4]

#### 7.2 일반화 성능 향상을 위한 향후 연구 방향

**1) 다중 소스 도메인 확장:**
현재 CLS는 단일 소스만 고려하지만, 향후 연구는 다음과 같이 확장할 수 있다:
- 소스-별 가중치 추가: $$\gamma_{i,s} = \frac{p_t(y=i)}{\bar{p}_s(y=i)}$$
- 소스 간 분포 차이를 명시적으로 모델링

**2) 고차 모멘트 매칭:**
평균 매칭(식 12)을 넘어:
- 공분산 매칭으로 특징 공간의 기하학적 구조 보존
- Wasserstein 또는 MMD 손실로 더 강력한 분포 정렬

**3) 연속적 도메인 시프트:** 단일 대상 도메인이 아닌 연속적으로 변하는 도메인 시퀀스 처리:[5]
- 온라인 업데이트 전략으로 $$p_t(y)$$ 지속적 추정
- 메모리 효율적인 클래스별 가중치 추적

**4) 개념 시프트 처리:**
정리 2에서 도출된 개념 시프트 $$p_s(y|f(x)) \neq p_t(y|f(x))$$의 명시적 처리:
- 판별자 앙상블로 개념 시프트 감지
- 신뢰도 기반 샘플 선택

**5) 자기학습과의 시너지:** CLS의 조건부 정렬과 자기학습 결합:[6]
- 신뢰도 높은 의사 레이블만 선택적 사용
- 동적 임계값 조정으로 거짓 긍정 최소화

#### 7.3 의료 영상 처리 응용 고려사항

당신의 골 억제(bone suppression) 연구 관점에서:[7][8][9]

1. **도메인 시프트 특성:**
   - 촬영 조건 변화(배경, 조명): 조건부 시프트
   - 골 구조 빈도 변화(환자마다): 라벨 시프트
   - 즉, CLS의 이중 시프트 가정이 매우 현실적

2. **해부학적 구조 보존:**
   - CLS의 조건부 정렬이 뼈-폐 조직 경계를 유지
   - 클래스별 가중치로 구조가 드문 클래스(예: 갈비뼈)도 과소 적응 방지

3. **효율성 개선:**
   - 의료 영상은 픽셀 수가 매우 많아 예시 수준 가중치(IWAN 등)가 비효율적
   - CLS의 클래스 수준 접근이 GPU 메모리 효율성 향상

#### 7.4 앞으로의 연구 방향 제안

**단기 (1-2년):**
- [ ] 가변 조건부 시프트 처리 (클래스마다 다른 정도의 조건부 시프트)
- [ ] 신경망 기반 $$p_t(y)$$ 추정기로 평균 매칭 대체
- [ ] 시퀀스 데이터(시간-도메인 시프트)로 확장

**중기 (2-3년):**
- [ ] 개념 시프트 감지 및 적응 메커니즘 추가
- [ ] 다중 소스 도메인 일반화로 확대
- [ ] 생의학 신호(EEG, ECG) 처리로 응용 분야 확장

**장기 (3년 이상):**
- [ ] 도메인 적응 이론 통합: 일반화 한계(generalization bounds) 강화
- [ ] 메타-학습과 결합한 빠른 적응(few-shot adaptation)
- [ ] 인과 추론 기반 도메인 시프트 모델링

#### 7.5 최신 방법론과의 비교

| 측면 | CLS (본 논문) | 최신 확산 모델 기반[6][10] | 최신 대조 학습[2] | 최신 그래프 적응[7] |
|------|---------------|------------------------|------------------|-------------------|
| 조건부 시프트 | ✓ (명시적) | ✓ (암묵적) | ✓ (명시적) | ✗ |
| 라벨 시프트 | ✓ (명시적) | ✗ | ✓ (보정) | ✗ |
| 계산 효율 | 높음 | 중간 | 중간 | 높음 |
| 대규모 작업 | ✓ (클래스별 가중치) | ✓ (생성 샘플링 비용) | ✓ | ✗ (그래프 제약) |
| 의료 영상 적합도 | 매우 높음 | 높음 | 높음 | 낮음 |

---

### 결론

CLS는 **조건부 시프트와 라벨 시프트를 동시에 처리**하는 UDA의 핵심 문제를 이론적으로 분석하고, 실질적인 해결책을 제시한다. 특히:[1]

- **이론적 기여**: 정리 2-3을 통해 라벨 시프트의 개념 시프트 유도, 점근 일관성 보장
- **실용적 기여**: 클래스별 가중치 $$\gamma_i$$를 통한 간단하면서 효과적인 구현
- **범용성**: 분류, 분할, 부분 UDA 등 다양한 작업에서 SOTA 달성

최신 연구 동향(2023-2025)과의 연계에서, CLS의 아이디어는 **자기학습, 대조 학습, 확산 모델** 등과 결합되어 더욱 강력한 적응 프레임워크로 진화하고 있다. 특히 의료 영상 처리에서는 해부학적 구조 보존과 계산 효율성 측면에서 CLS의 접근 방식이 매우 적합하므로, 당신의 뼈 억제 연구에 직접적으로 적용할 가치가 있다.[3][2][1]

***

**주요 인용:**

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/4574ee71-9bec-4800-adcc-86e6cd191850/2107.13469v2.pdf)
[2](https://link.springer.com/10.1007/978-3-031-78195-7_2)
[3](https://arxiv.org/abs/2411.07249)
[4](https://arxiv.org/abs/2305.18458)
[5](https://arxiv.org/pdf/2303.15833.pdf)
[6](https://arxiv.org/abs/2309.14360)
[7](https://www.nature.com/articles/s41598-024-59890-y)
[8](https://arxiv.org/abs/2401.15952)
[9](https://ieeexplore.ieee.org/document/10130291/)
[10](https://link.springer.com/10.1007/978-3-031-73281-2_2)
[11](https://arxiv.org/abs/2405.16102)
[12](https://link.springer.com/10.1007/978-3-031-96202-8_18)
[13](https://arxiv.org/html/2412.20337v1)
[14](https://arxiv.org/pdf/2211.14960.pdf)
[15](http://arxiv.org/pdf/1903.09734.pdf)
[16](https://arxiv.org/pdf/2103.03571.pdf)
[17](https://arxiv.org/html/2503.02506v1)
[18](https://www.mdpi.com/1099-4300/27/4/426)
[19](https://arxiv.org/pdf/2305.19123.pdf)
[20](https://openaccess.thecvf.com/content/ICCV2021/papers/Liu_Adversarial_Unsupervised_Domain_Adaptation_With_Conditional_and_Label_Shift_Infer_ICCV_2021_paper.pdf)
[21](https://arxiv.org/abs/2409.18418)
[22](https://arxiv.org/abs/1810.00740)
[23](https://proceedings.mlr.press/v205/niemeijer23a/niemeijer23a.pdf)
[24](https://www.sciencedirect.com/science/article/abs/pii/S0031320323004855)
[25](https://openaccess.thecvf.com/content/WACV2025/papers/Pujol-Perich_SADA_Semantic_Adversarial_Unsupervised_Domain_Adaptation_for_Temporal_Action_Localization_WACV_2025_paper.pdf)
[26](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Improved_Test-Time_Adaptation_for_Domain_Generalization_CVPR_2023_paper.pdf)
[27](https://dl.acm.org/doi/10.1007/978-3-031-78195-7_2)
