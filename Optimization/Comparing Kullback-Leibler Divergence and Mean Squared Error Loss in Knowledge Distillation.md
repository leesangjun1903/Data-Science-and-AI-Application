# Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation

# 논문 요약 및 고찰

## 1. 핵심 주장과 주요 기여  
이 논문은 **지식 증류(Knowledge Distillation, KD)** 에서 널리 사용되는 Kullback–Leibler (KL) 발산 손실과 **로그잇(logit)** 간의 직접 MSE 손실을 비교 분석한다.  
- **핵심 주장**: 온도 τ를 크게 하면 KL 손실이 사실상 로그잇 매칭에 수렴하나, 평균이동 항(δ∞) 때문에 완전한 매칭이 어렵다. 반면, 로그잇 간 MSE 손실은 순수하게 로그잇 매칭을 수행하여 일반화 성능을 더 향상시킨다.  
- **주요 기여**:  
  1. τ 변화에 따른 KL 손실의 이론·실험적 특성 규명  
  2. 로그잇 벡터 간 MSE 손실(LMSE)을 직접 목표로 제안  
  3. 페널티 텀 δ∞가 penultimate 레이어 표현을 과도하게 확장시키는 현상 이론 분석  
  4. 순차적 증류(먼저 KL, 이후 MSE) 전략 제안  
  5. 소음 레이블 상황에서 KL(특히 작은 τ)이 LMSE보다 견고함 관찰  

## 2. 문제 정의 및 제안 방법  
### 2.1 해결하고자 하는 문제  
- 기존 KD는 KL 손실에 온도 τ를 도입해 부드러운 확률 분포를 학습하지만, τ→∞ 시에도 로그잇이 정확히 일치하지 않고 학생 모델의 로그잇 평균이 왜곡된다.  
- τ→0 시에는 오히려 레이블 매칭만 수행되어 정보전달이 제한적이다.

### 2.2 제안 방법  
1. **KL 발산 손실 분석**  
   - KL손실:  

$$
       \mathcal{L}_{KL} = \tau^2\sum_j p_t^j(\tau)\log\frac{p_t^j(\tau)}{p_s^j(\tau)}
     $$  
   
   - τ→∞ 시 그래디언트는

$$
       \frac{\partial \mathcal{L}\_{KL}}{\partial z_{s,k}}
       \xrightarrow{\tau\to\infty}
       \frac{1}{K}(z_{s,k}-z_{t,k})
       -\frac{1}{K^2}\sum_j(z_{s,j}-z_{t,j})
     $$  
     
  (여기서 두 번째 항이 δ∞이며, 학생 로그잇 평균을 교란)  
   - τ→0 시 그래디언트는 argmax 기반 레이블 매칭에 수렴  

2. **직접 로그잇 매칭 손실(LMSE)**  
   - 제안 손실:  

$$
       \mathcal{L}_{MSE} = \|z_s - z_t\|_2^2
     $$  
   
   - δ∞ 없이 학생이 교사 로그잇을 **정확히** 학습  

3. **순차적 증류**  
   - 대용량→중간 용량(KL)→소용량(MSE)로 차례로 증류하여 큰 용량 차이 완화  

### 2.3 모델 구조 및 실험 설정  
- **데이터셋**: CIFAR-100, ImageNet  
- **모델**: Wide‐ResNet(WRN) 계열, ResNet 계열  
- **하이퍼파라미터**: KL 손실만 사용하는 α=1, τ={1,3,5,20,∞}  

## 3. 성능 향상 및 한계  
### 3.1 일반화 성능  
- **CIFAR-100**: LMSE를 적용한 학생 모델이 KL 손실 대비 최대 약 0.3–0.5%p 더 높은 Top-1 정확도 달성  
- **ImageNet**: ResNet-50 학생이 LMSE로 학습 시 KL(τ=20) 대비 근소하게 상회  

### 3.2 penultimate 표현 변화  
- δ∞ 항으로 인해 KL 손실 학습 시 penultimate 레이어 표현이 과도하게 확장되어 클래스 간 군집이 퍼지는 반면, LMSE는 교사의 군집 구조를 더 잘 유지  

### 3.3 소음 레이블에 대한 견고성  
- 레이블 노이즈(예: 40% 대칭 노이즈) 상황에서 작은 τ의 KL 손실이 LMSE보다 우수한 일반화 성능을 보임  
- τ≈0.5 일 때 KL 손실이 레이블 스무딩 효과로 노이즈 완화에 가장 효과적  

### 3.4 한계  
- **계산 비용**: LMSE는 고차원 로그잇 벡터에 대한 L2 손실 계산 부담  
- **대규모 태스크**: ImageNet 등 대규모 실험에서 성능 향상 폭이 미미  
- **최적 τ 탐색**: 노이즈 상황별 최적 τ 설정 필요  

## 4. 향후 연구에 미치는 영향 및 고려사항  
- **로그잇 매칭 정교화**: δ∞ 없이 순수 매칭할 수 있는 다양한 손실 함수 연구 가능  
- **노이즈 견고성**: KL 손실 τ 조정과 레이블 스무딩 통합 전략 탐색  
- **순차 증류 전략**: 대/중/소 모델 간 최적 순차 학습 스케줄 설계  
- **프라이버시**나 **분산 환경**에서 로그잇 직접 교환 시 보안·통신 효율 고려  

이 논문은 KD 손실 함수 설계에 새로운 시각을 제공하며, 향후 **일반화 강화**, **노이즈 견고성**, **효율적 증류 파이프라인** 분야에서 활발한 연구 방향을 제시할 것이다.

[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/22370781/44948821-ba29-4740-8125-c548c7df6256/2105.08919v1.pdf
