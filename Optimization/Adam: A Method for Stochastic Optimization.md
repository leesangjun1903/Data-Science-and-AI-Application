# Adam: A Method for Stochastic Optimization

## 핵심 주장 및 주요 기여

**Adam(Adaptive Moment Estimation)**은 Kingma와 Ba(2015)가 제안한 **1차 그래디언트 기반 최적화 알고리즘**으로, 확률적 목적 함수(stochastic objective functions)를 최소화하기 위해 설계되었습니다. 이 논문의 핵심 기여는 다음과 같습니다:[1]

1. **적응적 학습률 계산**: 각 매개변수에 대해 **그래디언트의 1차 모멘트(평균)와 2차 모멘트(비중심화 분산)의 지수 이동 평균**을 기반으로 개별 학습률을 자동 조정합니다.[1]

2. **두 가지 우수한 알고리즘의 장점 결합**: AdaGrad(희소 그래디언트에 효과적)와 RMSProp(비정상 목적 함수에 적합)의 장점을 통합합니다.[1]

3. **초기화 편향 보정**: 영벡터로 초기화된 모멘트 추정치를 편향 보정 항으로 보정하여 훈련 초기의 불안정성을 제거합니다.[1]

***

## 해결하는 문제와 제안 방법

### 문제 정의

기존 확률적 경사하강법(SGD)은 다음과 같은 문제가 있었습니다:

- **고정된 학습률**: 모든 매개변수에 동일한 학습률 사용
- **희소 그래디언트 처리 미흡**: 드물게 나타나는 매개변수의 학습 어려움
- **비정상 목적 함수에 취약**: 학습 중 목적 함수의 특성이 변할 때 성능 저하
- **고차 최적화 방법의 비효율**: 고차원 문제에서 메모리와 계산량 증가

### 제안 방법: Adam 알고리즘

Adam의 업데이트 규칙은 다음과 같습니다:[1]

**1차 모멘트 추정치 (편향된 추정):**

$$
m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t
$$

**2차 모멘트 추정치 (편향된 추정):**

$$
v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
$$

**편향 보정 (Bias Correction):**

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

**매개변수 업데이트:**

$$
\theta_t = \theta_{t-1} - \alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

여기서:
- $$\alpha$$: 스텝 크기(학습률)
- $$\beta_1, \beta_2$$: 모멘트 추정의 지수 감소율(기본값: 0.9, 0.999)
- $$\epsilon$$: 수치 안정성을 위한 작은 상수(기본값: $$10^{-8}$$)
- $$g_t$$: 시간 t에서의 그래디언트

### 핵심 특성

**효과적인 스텝 크기의 제약:**

$$
|\Delta_t| = \left|\alpha \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}\right| \leq \alpha
$$

이는 매개변수 공간에서의 변화가 $$\alpha$$로 상한이 정해지므로, 신뢰 영역(trust region) 개념을 자동으로 구현합니다.[1]

**신호-대-잡음비(SNR) 해석:**

$$
\text{SNR} = \frac{\hat{m}_t}{\sqrt{\hat{v}_t}}
$$

SNR이 작을수록(최적값 근처) 스텝 크기가 자동으로 작아지는 **자동 어닐링(automatic annealing)** 효과가 발생합니다.[1]

***

## 성능 향상 및 실험 결과

### 1. Logistic Regression (MNIST & IMDB)

- **MNIST**: Adam은 SGD with Nesterov 모멘텀과 유사한 수렴 속도를 보이며, AdaGrad보다 빠릅니다.[1]
- **IMDB BoW Feature (희소 그래디언트)**: Adam은 AdaGrad와 동등한 성능을 보이면서 계산 효율성에서 우월합니다.[1]

### 2. 다층 신경망 (Multi-layer Neural Network)

Adam은 **dropout 정규화가 포함된 경우** 다른 방법들(AdaGrad, RMSProp, SGD)보다 **현저히 빠른 수렴**을 달성합니다. SFO(Sum-of-Functions Optimizer) 같은 준뉴턴 방법 대비 **5-10배 빠른 계산 속도**를 보여줍니다.[1]

### 3. 합성곱 신경망 (Convolutional Neural Networks)

흥미로운 발견:
- **초기 단계**: AdaGrad와 Adam 모두 빠른 진전
- **후기 단계**: Adam이 SGD with Nesterov 모멘텀보다 우월
- **원인**: CNN에서는 2차 모멘트 추정이 비약화(vanishing)되어 1차 모멘트(분산 감소)가 더 중요한 역할을 합니다.[1]

### 4. 편향 보정의 중요성

VAE 실험에서 $$\beta_2$$가 1에 가까울 때(희소 그래디언트 케이스):
- **편향 보정 없음**: 훈련 초기의 큰 스텝으로 인한 불안정성 발생
- **편향 보정 있음**: 안정적이고 빠른 수렴[1]

***

## 일반화 성능 향상 가능성

### 이론적 분석: 수렴 보장

논문은 온라인 볼록 최적화 프레임워크에서 다음의 **Regret 경계**를 증명합니다:[1]

$$
R_T = O\left(\sqrt{T}\right)
$$

평균 Regret은:

$$
\frac{R_T}{T} = O\left(\frac{1}{\sqrt{T}}\right) \to 0 \text{ as } T \to \infty
$$

### 희소 그래디언트에 대한 적응적 성능

**정리 4.1** (논문의 핵심 이론적 결과):

유한한 그래디언트 $$|g_t|_2 \leq G$$와 경계진 매개변수 거리 $$|\theta_n - \theta_m|_2 \leq D$$를 가정할 때, Adam의 Regret 경계는:

$$
R_T \leq D^2 \sqrt{\frac{2(1-\beta_1)}{1-\beta_2}} + \frac{1-\beta_1}{2}G\sum_{i=1}^d \sqrt{T v_{T,i}} + \text{(희소 항)}
$$

**중요한 함의**:
- 희소한 데이터의 경우, $$\sum_{i=1}^d \|g_{1:T,i}\|_2 \ll dG\sqrt{T}$$이므로 **$$O(\log d / \sqrt{T})$$ 수렴 속도** 달성
- 비적응형 방법 대비 **$$O(\sqrt{dT})$$ vs $$O(\sqrt{d}T)$$** 성능 향상[1]

### 실제 성능 향상 메커니즘

1. **자동 학습률 조정**: 그래디언트의 크기에 따라 매개변수별 학습률이 조정되어 **모든 매개변수가 적절한 속도로 수렴**

2. **가변 스텝 크기 어닐링**: 최적값에 가까워질수록 자동으로 스텝이 작아져 **진동 감소 및 안정적 수렴**

3. **희소 그래디언트 처리**: 자주 업데이트되지 않는 매개변수에 대해 **더 큰 효과적 학습률** 적용

4. **비정상 목적 함수 대응**: 지수 이동 평균이 **최근 그래디언트에 높은 가중치** 부여하여 변화하는 환경에 신속하게 적응

***

## 한계 (Limitations)

논문에서 명시적으로 논의된 한계:

### 1. 비볼록 최적화에 대한 이론적 보장 부족

- 이론적 분석은 **온라인 볼록 최적화 프레임워크**에 제한
- **실제 심층 신경망은 비볼록 문제**이므로 수렴 보장 없음
- 따라서 "Adam이 비볼록 문제에도 잘 작동한다"는 경험적 발견만 제공[1]

### 2. 하이퍼파라미터 민감성

- $$\beta_2$$가 1에 가까울수록 초기화 편향 문제 심화
- 편향 보정 항 없이는 발산 가능성
- 일반적으로 $$\alpha$$ 선택이 중요한 역할

### 3. CNN에서의 제한된 성능

- CNN에서는 2차 모멘트가 빠르게 비약화되어 **1차 모멘트만큼 중요하지 않음**
- SGD with Nesterov 모멘텀과 **"한계적 개선(marginal improvement)"**만 보임[1]

### 4. 메모리 오버헤드

- 1차 및 2차 모멘트 벡터 저장 필요
- 매개변수 개수의 2배 메모리 요구

### 5. 벡터화된 2차 모멘트 사용

- 완전한 Hessian(곡률)이 아닌 **대각 근사만 사용**
- 매개변수 간의 상관관계 미포착

***

## 논문이 향후 연구에 미치는 영향

### 광범위한 산업적 채택

Adam은 **심층 학습의 표준 최적화 알고리즘**이 되었습니다:

- **TensorFlow, PyTorch, Keras** 등 모든 주요 프레임워크에 기본 옵션으로 포함
- 이미지 분류, NLP, 강화학습 등 다양한 분야에서 기본 선택지

### 이론적 개선 연구의 촉발

Adam 이후 다음과 같은 변형 및 개선 알고리즘들이 제안됨:

1. **AdaBound**: Adam의 수렴성을 개선하려는 시도
2. **Lookahead**: 여러 스텝의 매개변수 평균 활용
3. **RAdam**: 초기 단계에서의 분산 감소
4. **AdamW**: 가중치 감쇠(weight decay) 개선

---

## 향후 연구 시 고려 사항

### 1. 문제 특성에 따른 알고리즘 선택

- **희소 데이터**: Adam 최적
- **작은 배치 크기**: Adam의 적응성이 더욱 유리
- **대규모 CNN**: SGD with 모멘텀과 비교 필요
- **매우 복잡한 비볼록 문제**: 다양한 최적화 기법 앙상블 고려

### 2. 하이퍼파라미터 튜닝

- 기본값 $$\alpha = 0.001, \beta_1 = 0.9, \beta_2 = 0.999$$는 매우 안정적이나, 특정 문제에서는 조정 필요
- $$\beta_2$$의 값을 높일수록 장기 그래디언트 히스토리 활용 가능 → 희소 그래디언트 케이스에 유리

### 3. 일반화 성능 향상

- **조기 종료(Early Stopping)**: Adam의 빠른 수렴을 이용하되, 과적합 방지
- **정규화와의 조합**: Dropout, L2 정규화, 가중치 감쇠 등과 함께 사용 권장
- **학습률 스케줄링**: 고정 학습률 외에도 선형 감소, 코사인 감소 등 시도

### 4. 신경망 구조별 최적화

- **RNN/LSTM**: Adam이 매우 효과적 (출력이 희소한 경향)
- **Transformer**: Adam 또는 AdamW 기본 선택
- **CNN**: 초기에는 Adam, 후기에는 SGD with 모멘텀으로 전환하는 **2단계 훈련** 고려

### 5. 이론과 실제의 격차 연구

- 현재 이론은 **온라인 볼록 최적화**에 제한되어 있음
- 비볼록 문제에 대한 수렴 분석 필요
- 특히 **신경망의 손실 지형(loss landscape)** 특성 이해 필수

### 6. 메모리 효율성

- 극도로 큰 모델에서는 메모리 제약으로 인해 **1차 모멘트만 사용**하는 변형 고려
- 혼합 정밀도 훈련에서의 Adam 동작 특성 분석

***

## 결론

Adam은 **단순한 구현, 계산 효율성, 강력한 경험적 성능**을 제공함으로써 현대 딥러닝의 표준 최적화 알고리즘이 되었습니다. 특히 **희소 그래디언트, 비정상 목적 함수, 고차원 문제**에서 뛰어난 성능을 보입니다.[1]

향후 연구는 (1) **비볼록 최적화에 대한 이론적 이해 심화**, (2) **문제 특성별 적응형 알고리즘 개발**, (3) **메모리와 계산의 효율성 트레이드오프 최적화**에 집중할 필요가 있습니다. 또한 의료 영상 처리 같은 **특정 도메인에서의 맞춤형 변형**을 개발하면 모델의 일반화 성능을 더욱 향상시킬 수 있을 것입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/bdea24b6-6ace-467a-9320-722145d64b6e/1412.6980v9.pdf)
