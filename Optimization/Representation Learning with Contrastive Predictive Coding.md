# Representation Learning with Contrastive Predictive Coding

### 1. 핵심 주장과 주요 기여

**Representation Learning with Contrastive Predictive Coding**는 DeepMind의 van den Oord, Li, Vinyals가 2018년에 제안한 획기적인 논문으로, **비지도 학습 기반의 보편적 표현 학습 프레임워크**를 제시합니다.[1]

논문의 핵심 주장은 다음과 같습니다:[1]

**첫째, 잠재공간에서의 미래 예측을 통한 표현 학습**: 고차원 데이터를 저차원의 잠재표현으로 압축한 후, 강력한 자동회귀 모델을 활용하여 미래의 잠재표현을 예측함으로써 의미 있는 특징을 자동으로 추출합니다.

**둘째, 상호정보 최대화를 통한 느린 특징 학습**: CPC는 시간적으로 떨어진 데이터 간의 상호정보를 최대화함으로써, 장기적 구조를 포착하는 "느린 특징(slow features)"을 학습합니다. 이는 단순히 국소적 매끄러움을 이용하는 차세대 예측과 달리, 음성의 음소나 억양, 이미지의 객체, 텍스트의 줄거리 같은 고차원 정보를 추출합니다.[1]

**셋째, 모달리티 독립적 설계**: CPC는 음성, 이미지, 자연어, 강화학습 등 4가지 상이한 도메인에서 일관되게 우수한 성능을 보이며, **도메인 특정 설계 없이도 범용적으로 작동**합니다.[1]

주요 기여는 다음과 같습니다:

- **정보 이론적 기반 제공**: 상호정보 추정이라는 명확한 이론적 틀을 통해 표현 학습의 목표를 정의
- **확장 가능한 손실 함수**: Noise-Contrastive Estimation(NCE) 기반의 InfoNCE 손실함수로 비정규화 모델의 훈련을 가능하게 함
- **크로스모달 검증**: 단일 모달리티에 국한되지 않고 여러 도메인에서 효과성을 실증

***

### 2. 해결하는 문제와 제안 방법

#### 2.1 문제 정의

CPC가 해결하고자 하는 근본적인 문제는 **지도 학습의 한계**입니다. 지도 학습은 큰 성공을 거두었지만, 다음과 같은 문제점이 있습니다:[1]

- **데이터 효율성 문제**: 대규모 레이블 데이터의 확보가 어려움
- **일반화 부족**: 특정 작업에 최적화된 특징이 다른 작업으로 전이되지 않음
- **비지도 학습의 미성숙**: 고차원 데이터에서 의미 있는 표현을 학습하는 방법의 부재

예를 들어, 이미지 분류를 위해 사전학습된 특징은 색상이나 개수 세기처럼 분류와 무관한 정보는 제외되어 이미지 캡셔닝 같은 작업에는 부적절합니다.[1]

#### 2.2 제안하는 방법론

CPC는 **예측 코딩(Predictive Coding) + 대조 손실(Contrastive Loss)**을 결합합니다.

**아키텍처 설계**:

1. **인코더($$g_{enc}$$)**: 고차원 관측 시퀀스 $$x_t$$를 저차원 잠재표현 $$z_t = g_{enc}(x_t)$$로 변환
2. **자동회귀 모델($$g_{ar}$$)**: 문맥 $$c_t = g_{ar}(z_{\leq t})$$를 생성
3. **예측**: 문맥 $$c_t$$로부터 미래의 잠재표현 $$z_{t+k}$$를 예측

**핵심 수식**:

예측 문제를 직접적인 생성 모델 $$p(x_{t+k}|c_t)$$ 대신 **밀도 비(density ratio)** 형태로 정의합니다:[1]

$$
f_k(x_{t+k}, c_t) \propto \frac{p(x_{t+k}|c_t)}{p(x_{t+k})}
$$

로그-이중선형 모델(log-bilinear model)로 구현됩니다:

$$
f_k(x_{t+k}, c_t) = \exp(z_{t+k}^T W_k c_t)
$$

여기서 $$W_k$$는 각 스텝마다 다른 선형 변환입니다.[1]

**InfoNCE 손실 함수**:

주어진 N개의 샘플(1개 양성, N-1개 음성)에 대해:

$$
L_N = -\mathbb{E}_X \left[\log \frac{f_k(x_{t+k}, c_t)}{\sum_{x_j \in X} f_k(x_j, c_t)}\right]
$$

이 손실함수는 다항 교차엔트로피(multinomial cross-entropy)의 형태로, 양성 샘플을 정확히 분류하도록 유도합니다.[1]

**상호정보 관점**:

최적의 $$f_k(x_{t+k}, c_t)$$는 정확히 밀도 비 $$\frac{p(x_{t+k}|c_t)}{p(x_{t+k})}$$에 비례하며, 이는 다음 상호정보 하한을 제공합니다:[1]

$$
I(x_{t+k}, c_t) \geq \log(N) - L_N^{opt}
$$

상호정보가 N이 커질수록 더 정확하게 추정되므로, 더 많은 음성 샘플이 일반적으로 더 나은 표현을 생성합니다.[1]

***

### 3. 모델 구조 및 실험 설계

#### 3.1 모달리티별 구현

**음성(LibriSpeech 100시간 데이터)**:[1]

- 인코더: 5개 층 스트라이드 CNN (해상도: 160배 축소, 10ms 단위 특징)
- 자동회귀: 256차원 GRU
- 예측 범위: 12개 스텝(120ms)
- 결과: 음소 분류 64.6% (지도 학습 74.6% vs 무작위 27.6%)

**이미지(ImageNet)**:[1]

- 인코더: ResNet-v2-101 (선택적 배치 정규화 미사용)
- 자동회귀: PixelCNN 스타일의 행-GRU
- 예측: 5행까지 공간적 예측
- 결과: ImageNet 선형 평가 48.7% Top-1 (이전 SOTA 39.6% 대비 23% 절대 향상)

**자연어(BookCorpus)**:[1]

- 인코더: 1D 합성곱 + ReLU + 평균 풀링 (2400차원)
- 자동회귀: 2400개 숨겨진 유닛의 GRU
- 예측: 3문장까지 예측
- 결과: 다양한 NLP 벤치마크(감정 분석, TREC 등)에서 경쟁적 성능

**강화학습(DeepMind Lab)**:[1]

- 보조 손실로 추가: A2C 기준선에 CPC 손실 결합
- 예측 범위: 30스텝
- 결과: 5개 게임 중 4개에서 유의미한 성능 향상

#### 3.2 주요 절제 연구(Ablation Study)

**음성 영역 절제**:[1]

| 변수 | 음소 분류 정확도 |
|------|-----------------|
| 2 스텝 예측 | 28.5% |
| 4 스텝 예측 | 57.6% |
| 8 스텝 예측 | 63.6% |
| 12 스텝 예측 | 64.6% |
| 16 스텝 예측 | 63.8% |

**음성 샘플링 전략**:[1]

| 샘플링 방식 | 정확도 |
|-----------|--------|
| 혼합 화자 음성 샘플 | 64.6% |
| 동일 화자 음성 샘플 | 65.5% |
| 동일 화자 제외 | 57.3% |
| 현재 시퀀스만 | 65.2% |

***

### 4. 성능 향상 및 분석

#### 4.1 주요 성능 지표

**크로스모달 성능 비교**:

| 도메인 | 메트릭 | CPC | 기저선 | 향상도 |
|--------|--------|-----|--------|--------|
| 음성 | 음소 분류 | 64.6% | 39.7% (MFCC) | 62% ↑ |
| 음성 | 화자 분류 | 97.4% | 17.6% (MFCC) | 453% ↑ |
| 이미지 | ImageNet Top-1 | 48.7% | 39.6% (최신 방법) | 23% ↑ |
| 이미지 | ImageNet Top-5 | 73.6% | 62.5% (최신 방법) | 18% ↑ |

#### 4.2 일반화 성능 향상 분석

**1. 표현의 다목적성(Multi-Purpose Representations)**:

CPC로부터 학습된 표현은 **다양한 하위 작업에서 효과**를 보입니다. 예를 들어, 음성 데이터에서:

- 동일한 표현이 음소 분류(64.6%)와 화자 분류(97.4%)에서 모두 높은 성능 달성
- 이는 표현이 특정 작업 특화적이 아니라 **일반적인 음성 구조를 포착**함을 의미

이는 "느린 특징" 학습을 통해 달성되며, 단기적 예측(2-4 스텝)은 음소 같은 고주파 변화도 포착하고, 장기 예측(12-16 스텝)은 화자 정체성 같은 저주파 특성을 포착합니다.[1]

**2. 선형 분리 가능성 향상**:

t-SNE 시각화에서 CPC 표현은 **뛰어난 선형 분리 가능성**을 보여줍니다. 추가 비선형 레이어(1개 숨겨진 층)를 추가하면 음소 분류 정확도가 64.6%에서 72.5%로 증가하여, 표현에 포함된 정보의 질을 나타냅니다.[1]

**3. 데이터 효율성**:

최신 연구(2020)에서 개선된 CPC 구현은 다음을 보여줍니다:

- ImageNet 1% 데이터로 미세조정 시 78% Top-5 정확도 달성 (원본 이미지만 사용한 모델 대비 34% ↑)[2]
- 산업 시계열 분류에서 라벨이 10-수백 개 범위일 때 최대 15배 라벨 효율성 향상[3]

***

### 5. 모델의 한계와 도전 과제

#### 5.1 이론적 한계

**1. 상호정보 추정의 편향**:

InfoNCE는 상호정보의 하한을 제공하지만, 실제 MI가 높을 때 음성 샘플 수 N에 의해 제한됩니다:[1]

$$
I(x_{t+k}, c_t) \geq \log(N) - L_N^{opt}
$$

따라서 계산 제약으로 N이 제한되면, 실제 MI가 이 상한을 초과할 수 있습니다. 이는 특히 고복잡도 데이터에서 표현 품질을 제한할 수 있습니다.[1]

**2. 예측 거리의 민감성**:

절제 연구에서 12 스텝이 최적이지만 16 스텝에서 성능 저하가 관찰됩니다. 이는 **미래 예측이 너무 멀어지면 신호가 희석**됨을 시사합니다. 음성의 경우 최대 컨텍스트 길이(20480 스텝 = 2초 약간 초과)가 제한되어 장기 패턴 학습을 어렵게 합니다.[1]

#### 5.2 실용적 한계

**1. 계산 비용**:

이미지 실험에서 32개 GPU, 배치 크기 16으로 학습하는 대규모 자원이 필요합니다. 이는 많은 연구실에서 재현이 어렵습니다.[1]

**2. 아키텍처 의존성**:

논문에서는 "표준 아키텍처"(ResNet, GRU)를 사용한다고 하지만, 자동회귀 모델의 선택이 중요합니다. 마스크 CNN이나 Transformer 같은 최신 아키텍처가 더 나을 수 있다고 언급되었으나 미검증입니다.[1]

**3. 자연어 성능의 한계**:

NLP 벤치마크에서 CPC는 Skip-thought 벡터와 비슷하거나 약간 낮은 성능을 보입니다. 저자들은 **단순한 문장 인코더가 병목**이라고 주장하며, 이는 자연어의 계층적 구조 활용 부족을 시사합니다.[1]

#### 5.3 강화학습의 불안정성

5개 DeepMind Lab 게임 중 1개(lasertag_three_opponents_small)에서는 CPC 손실이 도움이 되지 않습니다. 저자들은 이것이 순수 반응적 정책이 필요한 작업이기 때문이라고 설명했으나, 이는 **CPC의 적용 범위 제한**을 보여줍니다.[1]

***

### 6. 모델 일반화 성능 향상 가능성 (집중 분석)

#### 6.1 이론적 일반화 보장

최근 2024-2025년 연구에 따르면, **대조 표현 학습의 일반화 특성**에 대한 이해가 깊어졌습니다:[4]

대조 표현 학습(CRL) 프레임워크의 일반화 한계(Generalization Bounds)는 다음과 같이 유도됩니다:

$$
\text{Generalization Error} \leq O\left(\frac{\log N_c}{m}\right)
$$

여기서 $$N_c$$는 클래스별 커버링 수(covering number)이고, m은 샘플 수입니다. 흥미롭게도, 각 클래스 내 필요 샘플 수는 비정규화 데이터 설정에서도 커버링 수의 로그에 비례합니다.[4]

이는 **CPC 기반 표현이 이론적 보장 하에서 다양한 하위 작업으로 일반화**될 수 있음을 시사합니다.

#### 6.2 하위 작업 전이 성능

**음성 인식 영역의 진전**:

CPC 이후 개선된 변형들이 제안되었습니다:

- **정규화된 CPC**: 음성 "느린성(slowness)" 제약을 추가하여 100시간 학습 데이터로 360시간 기저선 성능을 달성[5]
- **다중 라벨 CPC(ML-CPC)**: 여러 양성 쌍을 처리하여 상호정보 추정 정확도 향상[6]

**이미지 인식의 비약적 발전**:

2020년 개선된 CPC 구현은 원본 48.7%에서 **71.5% ImageNet Top-1 선형 평가**로 향상되었으며, 1% 라벨 미세조정에서 78% Top-5 달성 - 이는 **완전 지도 학습 대비 데이터 효율성 34% 향상**을 의미합니다.[2]

#### 6.3 다중 작업 학습의 이점

**Matryoshka 표현 학습(MRL)** 관점에서, CPC 같은 상호정보 최대화 방법은 **다층적 세분화(coarse-to-fine) 표현**을 학습할 수 있습니다:[7]

- 임베딩 크기 14배 축소해도 같은 정확도 유지
- 다양한 계산 제약 조건을 가진 하위 작업에 적응 가능
- 소수의 레이블로 미세조정 시 최대 2% 정확도 향상

이는 CPC가 **"일반적이고 유연한" 표현**을 학습함을 입증합니다.

#### 6.4 도메인 일반화에서의 활용

**프록시 기반 대조 학습(Proxy-based Contrastive Learning)** 최신 연구(2022)는 CPC와 유사한 대조 목표를 **도메인 일반화**에 적용할 때의 주의점을 제시합니다:[8]

- 표본-대-표본(sample-to-sample) 대조: 도메인 간 큰 분포 차이로 인한 "양성 정렬 문제" 야기
- 프록시-대-표본(proxy-to-sample) 방식: 클래스 프록시를 사용하여 도메인 불변 표현 학습

이는 **CPC 프레임워크를 도메인 적응에 활용할 수 있는 가능성**을 보여줍니다.

#### 6.5 데이터 효율성과 일반화의 관계

**Skip-Step CPC를 활용한 시계열 이상 탐지** 연구(2024)는 다음을 보여줍니다:[9]

- 표준 CPC: F1-점수 기준 기존 방법 대비 17% 향상
- Skip-Step 변형: 예측 거리 동적 조정으로 **지역 연관성 학습 강화**

이는 CPC의 일반화 성능이 **작업별 특성(temporal scale)에 맞춘 미세 조정**으로 향상될 수 있음을 시사합니다.

***

### 7. 향후 연구 영향 및 고려 사항 (최신 기반)

#### 7.1 CPC의 후속 연구 영향

**1. 자기지도 학습의 표준 방법론 확립**:

CPC는 자기지도 학습이 단순한 매력적 아이디어를 넘어 **체계적인 이론적 기반(상호정보 최대화)**을 가진 방법론임을 입증했습니다. 이후 모든 대조 학습 방법(SimCLR, MoCo 등)은 상호정보 최대화 원칙을 채용합니다.[10][11]

**2. 다중 모달리티 표현 학습의 선구자 역할**:

CPC가 음성, 이미지, 텍스트, RL에서 일관된 성능을 보임으로써, **모달리티 독립적 표현 학습이 가능**함을 최초로 입증했습니다. 현재의 CLIP(Vision-Language), Multimodal Transformer 등이 이 기초 위에 구축됩니다.[10]

**3. InfoNCE 손실함수의 광범위 채용**:

CPC의 InfoNCE는 현재 대조 학습의 표준 손실 함수가 되었으며, MINE(Mutual Information Neural Estimation)과도 이론적으로 연결됩니다.[1]

#### 7.2 최신 연구 동향 (2024-2025)

**1. 일반화 이론의 발전**:

대조 표현 학습의 일반화에 대한 **엄밀한 수학적 분석**이 이루어지고 있습니다:[4]

- 비i.i.d. 설정(실제 데이터 재활용 상황)에서도 일반화 한계 도출
- 각 클래스 샘플 필요량이 커버링 수의 로그에 비례 (차원의 저주 완화)

이는 CPC 같은 방법의 **데이터 효율성이 이론적으로 보장**됨을 의미합니다.

**2. 모달리티별 CPC 개선**:

| 영역 | 개선 사항 | 성능 향상 |
|-----|---------|---------|
| 음성 | 느린성 정규화 추가 | 360→100 시간 데이터로 동일 성능 |
| 시계열 | Skip-Step CPC | CPD 태스크에서 17% F1 개선 |
| 의료 센서 | 임상 분류 프레임워크 | 자동 진단 정확도 향상 |
| 이미지 | CPC v2 개선 | 71.5% LinearEval(원본 48.7%) |

**3. 하이브리드 접근의 대두**:

최신 연구(2025)는 **CPC + MAML(Model-Agnostic Meta-Learning) + SHAP**을 결합한 적응형 프레임워크를 제시합니다:[12]

- CPC: 비레이블 시계열 데이터에서 고품질 표현 추출
- MAML: 최소 재훈련으로 새 작업에 빠르게 적응
- SHAP: 모델 예측 해석 가능성 제공

이는 **현업 응용에서 CPC의 실무 활용**을 시사합니다.

#### 7.3 향후 연구 시 고려할 점

**1. 아키텍처 선택의 중요성**:

논문에서 "표준 아키텍처"를 사용했지만, 현재 기준으로 개선 여지가 큽니다:

- **Transformer 아키텍처**: 마스크된 CNN 대신 자기주의(self-attention) 메커니즘 활용
- **Vision Transformer(ViT)**: 이미지 도메인에서 격자식 패러다임 탈피
- **더 깊은 모델**: 한계 수정(residual connection)이 있는 더 깊은 자동회귀 모델

최신 연구는 **더 강력한 아키텍처가 CPC 성능을 크게 향상**시킬 수 있음을 시사합니다.[13]

**2. 음성 샘플링 전략의 최적화**:

절제 연구에서 **음성 샘플링 방식(혼합 화자 vs 동일 화자)**의 영향이 미미합니다(64.6% vs 65.5%). 그러나 최신 연구는:

- 클래스 불균형 문제 처리
- 어려운 음성 샘플 마이닝
- 계층적 샘플 전략

같은 고급 기법이 일반화를 향상시킬 수 있음을 보여줍니다.[14]

**3. 상호정보 추정 정확도 개선**:

현재 InfoNCE는 하한만 제공하므로:

- **적응형 음성 샘플 수**: 데이터 복잡도에 따라 동적 조정
- **더 나은 MI 추정기**: MINE, 변분 추정기 활용
- **다중 예측 거리**: 단일 k 대신 다양한 미래 시점 동시 예측

같은 접근이 표현 품질을 높일 수 있습니다.[6]

**4. 도메인 적응 및 일반화**:

CPC 프레임워크는 **도메인 간 불변 표현** 학습에 활용될 수 있습니다:

- 도메인 특화 예측 작업 설계
- 프록시 기반 손실 함수 결합
- 다중 원본 도메인에서의 공동 학습

이는 현실의 **분포 변화 문제 해결**에 기여할 수 있습니다.[8]

**5. 강화학습 영역의 미개척**:

5개 게임 중 1개에서만 성능 저하가 보이지만, 이를 **심층 분석할 가치**가 있습니다:

- 작업별 표현 필요성 분석(예측 가능성 vs 제어 필요성)
- 반응적 vs 메모리 기반 정책에 따른 표현 차이
- 보조 손실 가중치의 동적 조정

최신 오프라인 강화학습 연구는 CPC 같은 표현 학습이 **다중 작업 RL에서 핵심**임을 보여줍니다.[15]

**6. 계산 효율성 개선**:

원본 CPC는 대규모 자원 필요 문제를 가집니다:

- **지식 증류(Knowledge Distillation)**: 큰 사전학습 모델을 작은 모델로 압축 (SimCLR v2에서 증명된 기법)
- **효율적 대조 손실**: 메모리 뱅크 감소 또는 동적 하드 음성 채우기
- **혼합 정밀도 학습**: 16비트 반정밀도 활용

이는 **일반 연구실에서의 CPC 재현성**을 높입니다.[16]

***

### 8. 결론 및 종합 평가

**Representation Learning with Contrastive Predictive Coding**은 비지도 학습 분야의 **패러다임 전환 논문**입니다. 상호정보 최대화라는 명확한 이론적 틀과 InfoNCE 손실함수를 통해, CPC는:

1. **음성, 이미지, 텍스트, 강화학습 4개 도메인**에서 일관되고 우수한 성능 입증
2. **범용적 표현 학습의 가능성** 제시
3. **현대 자기지도 학습의 기초** 마련

CPC의 일반화 성능 향상 메커니즘은:

- **느린 특징 학습**: 장기 구조 포착으로 다목적 표현 생성
- **상호정보 최대화**: 도메인 불변 특성 추출
- **이론적 일반화 보장**: 최근 연구로 수학적 기초 확립

향후 연구는:

- Transformer 등 최신 아키텍처 통합
- 도메인 적응 및 다중 작업 학습 확대
- 계산 효율성 개선을 통한 접근성 향상
- 강화학습 등 미개척 영역의 탐색

이러한 방향들이 **일반화 가능하고 효율적인 표현 학습의 새로운 기준**을 수립할 것으로 예상됩니다. CPC는 단순히 당시의 우수한 방법이 아니라, **앞으로의 자기지도 학습 연구의 철학적·기술적 토대**를 제공한 기념비적 작품입니다.[14][10][2][4][1]

***

### 참고 문헌

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/98e0e27a-3451-4cd3-ae73-f2d9a2ffd061/1807.03748v2.pdf)
[2](http://proceedings.mlr.press/v119/henaff20a/henaff20a.pdf)
[3](https://dl.acm.org/doi/10.1145/3575637.3575654)
[4](https://openreview.net/forum?id=kWSRVtuIuH&noteId=xbsmvA2PF2)
[5](https://ieeexplore.ieee.org/document/10400460/)
[6](https://papers.neurips.cc/paper_files/paper/2020/file/5cd5058bca53951ffa7801bcdf421651-Paper.pdf)
[7](https://arxiv.org/abs/2205.13147)
[8](https://openaccess.thecvf.com/content/CVPR2022/papers/Yao_PCL_Proxy-Based_Contrastive_Learning_for_Domain_Generalization_CVPR_2022_paper.pdf)
[9](https://ieeexplore.ieee.org/document/10447104/)
[10](https://www.mdpi.com/2227-7080/9/1/2/pdf)
[11](https://wikidocs.net/255168)
[12](https://internationalpubls.com/index.php/cana/article/view/3637)
[13](http://arxiv.org/pdf/2104.03602v2.pdf)
[14](https://arxiv.org/abs/2111.00743)
[15](https://neurips.cc/virtual/2024/poster/96488)
[16](https://rauleun.github.io/SimCLR)
[17](https://downloads.hindawi.com/journals/jhe/2022/5649253.pdf)
[18](https://arxiv.org/pdf/1807.03748.pdf)
[19](https://arxiv.org/pdf/1905.09272.pdf)
[20](https://aclanthology.org/2021.emnlp-main.482.pdf)
[21](https://arxiv.org/pdf/2211.06173.pdf)
[22](https://jimmy-ai.tistory.com/129)
[23](https://harim061.tistory.com/13)
[24](https://openaccess.thecvf.com/content/CVPR2024/html/Chen_Neural_Clustering_based_Visual_Representation_Learning_CVPR_2024_paper.html)
[25](https://sanghyu.tistory.com/184)
[26](https://www.semanticscholar.org/paper/47ac48e7ee37e7cf4d3bb183477e42d6c5632b64)
[27](https://www.hindawi.com/journals/jhe/2022/5649253/)
[28](https://www.semanticscholar.org/paper/d8ef762d782ed4b2a51b76cbcb116069faad5fc1)
[29](https://ieeexplore.ieee.org/document/10843808/)
[30](https://dl.acm.org/doi/10.1145/3442381.3449903)
[31](https://ieeexplore.ieee.org/document/9789954/)
[32](http://arxiv.org/pdf/2306.07285.pdf)
[33](https://arxiv.org/pdf/2011.14097.pdf)
[34](https://arxiv.org/html/2307.00823)
[35](https://pmc.ncbi.nlm.nih.gov/articles/PMC8941554/)
[36](http://arxiv.org/pdf/2403.06382.pdf)
[37](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670205.pdf)
[38](https://dl.acm.org/doi/10.5555/3454287.3455679)
[39](https://www.uni-bamberg.de/fileadmin/xai/studies/theses/2025/Bachelor_Thesis_Sascha_Wolf.pdf)
[40](https://openreview.net/pdf?id=7KZP4d_75I0)
[41](https://arxiv.org/html/2405.04969v1)
