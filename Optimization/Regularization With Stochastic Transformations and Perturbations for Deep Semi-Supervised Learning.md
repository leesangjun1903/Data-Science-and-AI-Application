# Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning

### 1. 핵심 주장과 주요 기여

이 논문은 **레이블 데이터가 제한적일 때 unlabeled 데이터를 효과적으로 활용**하는 심층 준지도 학습 방법을 제안한다. 핵심 아이디어는 신경망 학습 과정에서 자연스럽게 발생하는 **확률적 요소들(randomized data augmentation, dropout, randomized max-pooling)**을 활용하여, 동일한 샘플이 네트워크를 여러 번 통과할 때 생성되는 **예측 결과들 간의 차이를 최소화**하는 비지도 손실 함수를 도입한다.[1]

주요 기여는 다음과 같다:

**Transformation/Stability (TS) Loss Function**: 동일 샘플에 대한 여러 예측 간 일관성을 강제하는 비지도 손실 함수로, 지도 학습과 결합하여 준지도 학습 프레임워크를 구성한다.[1]

**Mutual-Exclusivity (ME) Loss와의 결합**: TS 손실이 예측의 안정성을 보장하지만 trivial solution(모든 클래스에 대해 동일한 확률)을 방지하지 못하는 한계를 보완하기 위해, 예측 벡터가 하나의 클래스에만 높은 값을 갖도록 강제하는 ME 손실과 결합한다.[1]

**다양한 벤치마크에서의 state-of-the-art 달성**: CIFAR-10(3.00%), CIFAR-100(21.43%) 등에서 당시 최고 성능을 달성했으며, 특히 레이블 데이터가 1%만 있는 극한 상황에서도 큰 성능 향상을 보였다.[1]

### 2. 해결하고자 하는 문제, 제안 방법, 모델 구조, 성능 및 한계

#### 해결하고자 하는 문제

**Large labeled dataset의 필요성**: 현대 CNN은 높은 정확도를 달성하지만, 대량의 레이블 데이터가 필요하며 이는 비용과 시간이 많이 소요된다. 반면 unlabeled 데이터는 쉽게 수집할 수 있어, 이를 효과적으로 활용하는 것이 핵심 과제다.[1]

**기존 준지도 학습 방법의 한계**: 기존의 사전 학습 기반 방법들은 학습 초기에만 unlabeled 데이터를 활용하며, 학습 과정 전반에 걸쳐 지속적으로 활용하지 못한다.[1]

#### 제안 방법

논문의 핵심 방법론은 두 가지 비지도 손실 함수로 구성된다:

**Transformation/Stability Loss (TS Loss)**:[1]

$$ l^{TS}_U = \sum_{j=1}^{n-1} \sum_{k=j+1}^{n} \|f^j(T^j(x_i)) - f^k(T^k(x_i))\|^2_2 $$

여기서:
- $$x_i$$는 $$i$$번째 학습 샘플
- $$f^j(x_i)$$는 $$j$$번째 통과 시 예측 벡터
- $$T^j(x_i)$$는 $$j$$번째 통과 전 적용되는 랜덤 변환 (데이터 증강, rotation, translation 등)
- $$n$$은 각 샘플이 네트워크를 통과하는 횟수 (실험에서 4~5회 사용)[1]

이 손실은 동일 샘플에 대해 서로 다른 변환과 dropout 패턴을 적용했을 때 생성되는 모든 예측 쌍 간의 L2 거리를 최소화한다.[1]

**Mutual-Exclusivity Loss (ME Loss)**:[1]

$$ l^{ME}_U = \sum_{j=1}^{n} \left[ -\sum_{k=1}^{C} f^j_k(x_i) \prod_{l=1,l \neq k}^{C} (1 - f^j_l(x_i)) \right] $$

여기서:
- $$f^j_k(x_i)$$는 예측 벡터의 $$k$$번째 원소
- $$C$$는 클래스 수[1]

이 손실은 각 예측 벡터가 하나의 클래스에만 높은 값을 갖고 나머지는 0에 가깝도록 강제하여, 유효한 예측을 생성하도록 한다.[1]

**결합 손실 함수**:[1]

$$ l_U = \lambda_1 l^{ME}_U + \lambda_2 l^{TS}_U $$

실험에서는 $$\lambda_1 = 0.1$$, $$\lambda_2 = 1$$을 주로 사용했으며, 이 파라미터들에 대해 모델 성능이 과도하게 민감하지 않았다.[1]

#### 모델 구조

논문은 두 가지 CNN 구현을 사용했다:

**cuda-convnet (AlexNet 기반)**:[1]
- 64개 맵을 가진 2개의 컨볼루션 레이어 (커널 크기 5)
- 각 컨볼루션 레이어 뒤 max-pooling
- 32개 맵을 가진 2개의 locally connected 레이어 (커널 크기 3)
- 256개 노드의 fully connected 레이어
- SVHN, NORB, ImageNet 실험에 사용[1]

**Sparse Convolutional Networks (Fractional Max-Pooling 포함)**:[1]
- 표기: $$(nkC2-FMP_{\sqrt{2}})^{12}-C2-C1$$
  - $$n$$은 컨볼루션 맵 수 (32, 96, 160 등)
  - $$C2$$는 커널 크기 2
  - $$FMP_{\sqrt{2}}$$는 feature map을 $$\sqrt{2}$$ 비율로 축소하는 fractional max-pooling[1]
- Dropout 사용: 첫 레이어부터 마지막 레이어로 갈수록 비율 증가[1]
- MNIST, CIFAR-10, CIFAR-100 실험에 사용[1]

#### 성능 향상

**MNIST (100개 레이블 샘플)**:[1]
- 레이블만 사용: 5.44 ± 1.48%
- 제안 방법(TS+ME): 0.55 ± 0.16%
- **약 10배 오류율 감소**, Ladder Networks(0.89 ± 0.50%)보다 우수[1]

**SVHN (1% 레이블 데이터, sparse convnet)**:[1]
- 레이블만 사용: 12.25 ± 0.80%
- 제안 방법: 6.03 ± 0.62%
- **약 2배 정확도 향상**[1]

**NORB (1% 레이블 데이터, sparse convnet)**:[1]
- 레이블만 사용: 10.01 ± 0.81%
- 제안 방법: 2.15 ± 0.37%
- **약 4.7배 오류율 감소**, 전체 데이터 사용(1.63%)과 유사한 수준 달성[1]

**CIFAR-10 (4000개 레이블 샘플)**:[1]
- 레이블만 사용: 13.60 ± 0.24%
- 제안 방법: 11.29 ± 0.24%
- Ladder Networks(20.40 ± 0.47%)보다 크게 우수[1]

**CIFAR-10 (전체 데이터)**:[1]
- **3.00% 오류율 달성** (당시 state-of-the-art 3.47% 초과)[1]

**CIFAR-100 (전체 데이터)**:[1]
- **21.43 ± 0.16% 오류율 달성** (당시 state-of-the-art 23.82% 초과)[1]

**ImageNet ILSVRC 2012 (10% 레이블 데이터, Top-5 error)**:[1]
- 레이블만 사용: 45.91 ± 0.25%
- 제안 방법: 39.84 ± 0.23%
- Mutual-Exclusivity만 사용(42.90%)보다 우수[1]

#### 한계

**계산 복잡도 증가**: 각 샘플을 네트워크에 $$n$$번(4~5회) 통과시켜야 하므로 학습 시간이 증가한다. 다만, 높은 $$n$$ 값에서 수렴에 필요한 epoch 수가 감소하여 실제 계산량 증가는 $$n$$배보다 작다.[1]

**레이블 데이터가 충분할 때 제한적 이득**: 레이블 데이터가 많을수록 준지도 학습과 지도 학습 간의 성능 차이가 줄어든다. 예를 들어, SVHN에서 100% 레이블 사용 시 2.28%에서 2.22%로 소폭 개선에 그친다.[1]

**하이퍼파라미터 민감도**: $$\lambda_1$$과 $$\lambda_2$$ 값이 성능에 영향을 미치며, $$n$$의 선택도 프레임워크에 따라 조정이 필요하다(cuda-convnet은 배치 크기 제약으로 $$n=4$$, sparse convnet은 $$n=5$$ 사용).[1]

**데이터 증강 의존성**: 일부 실험(SVHN, NORB with cuda-convnet)에서는 데이터 변환이 주요 변동 원천이지만, 다른 실험(MNIST, CIFAR-100)에서는 dropout/randomized pooling만 사용한다. 효과적인 확률적 요소가 없는 경우 방법의 효과가 제한될 수 있다.[1]

### 3. 모델의 일반화 성능 향상 가능성

이 논문의 방법론은 **일관성 정규화(consistency regularization)**의 관점에서 일반화 성능을 향상시킨다:

#### 예측 안정성 강제

TS 손실은 동일 입력에 대해 다양한 변환 및 네트워크 상태(dropout 패턴)에서 일관된 예측을 생성하도록 강제한다. 이는 모델이 **입력의 불필요한 변동에 강건**하도록 만들어, 테스트 시 새로운 샘플에 대한 일반화 능력을 향상시킨다.[1]

수학적으로, TS 손실은 다음을 보장한다:

$$ \mathbb{E}_{T,D}[\|f(T(x)) - f(T'(x))\|^2] \rightarrow 0 $$

여기서 $$T, T'$$는 서로 다른 변환이고, $$D$$는 dropout 분포다. 이는 모델이 **의미를 보존하는 변환에 대해 불변**하도록 학습됨을 의미한다.[1]

#### Unlabeled 데이터를 통한 결정 경계 개선

비지도 손실을 unlabeled 데이터에 적용함으로써, 모델은 **데이터 분포의 저밀도 영역에 결정 경계를 배치**하도록 유도된다. 이는 준지도 학습의 클러스터 가정(cluster assumption)과 일치하며, 같은 클래스의 샘플들이 매니폴드 상에서 가까이 위치하도록 한다.[1]

#### Dropout과 데이터 증강의 시너지

Dropout은 **앙상블 효과**를 통해 일반화를 향상시키고, 데이터 증강은 **효과적인 학습 샘플 수를 증가**시킨다. TS 손실은 이 두 요소를 결합하여, 여러 dropout 마스크와 증강 버전에서 일관된 예측을 생성하도록 학습함으로써 **더 매끄럽고 안정적인 결정 경계**를 형성한다.[1]

#### 극소 레이블 데이터 환경에서의 효과

MNIST 100개 샘플(0.55%), NORB 1% 레이블(2.15%)에서 전체 데이터 사용 시와 유사한 성능을 달성한 것은, **제한적인 지도 신호를 unlabeled 데이터의 구조적 정보로 효과적으로 보완**할 수 있음을 보여준다. 이는 레이블 획득이 어려운 의료 영상, 희귀 질병 진단 등의 도메인에서 특히 유용하다.[1]

#### Fractional Max-Pooling과의 결합

Sparse convolutional networks에서 사용된 fractional max-pooling은 **확률적 pooling 위치 선택**을 통해 추가적인 정규화 효과를 제공한다. TS 손실은 이러한 확률성에 대한 일관성을 강제하여, 모델이 **특정 pooling 패턴에 과적합되는 것을 방지**한다.[1]

### 4. 향후 연구에 미치는 영향 및 고려사항

#### 향후 연구에 미치는 영향

**일관성 정규화의 표준화**: 이 논문은 Mean Teacher, Virtual Adversarial Training(VAT), MixMatch, FixMatch 등 후속 준지도 학습 연구들의 기반이 되었다. 특히 예측 일관성을 강제하는 아이디어는 현대 준지도 학습의 핵심 원리로 자리잡았다.

**Self-supervised Learning과의 연결**: 데이터 증강 불변성 학습은 SimCLR, MoCo 등 대조 학습(contrastive learning) 기반 자기지도 학습 방법들의 핵심 아이디어와 밀접하게 연결된다.

**Domain Adaptation 및 Transfer Learning**: 확률적 변환에 대한 안정성 학습은 도메인 불변 특징 학습에 적용될 수 있으며, 소스 도메인과 타겟 도메인 간의 분포 차이를 완화하는 데 기여할 수 있다.

**의료 영상 등 특수 도메인**: 레이블 획득이 어렵고 비용이 높은 의료 영상, 위성 영상, 산업 검사 등의 분야에서 실용적 가치가 크다. 특히 사용자의 흉부 X-ray bone suppression 연구와 같은 의료 영상 처리에 직접 적용 가능하다.

#### 향후 연구 시 고려사항

**더 강력한 데이터 증강 기법 탐색**: AutoAugment, RandAugment, CutMix 등 최신 증강 기법과 결합하여 TS 손실의 효과를 극대화할 수 있다. 다만, 증강이 의미를 변화시키지 않도록 주의해야 한다(예: 의료 영상에서 과도한 회전은 병변의 위치 정보를 왜곡할 수 있음).

**계산 효율성 개선**: 각 샘플을 $$n$$번 통과시키는 대신, momentum-based teacher model을 사용하는 Mean Teacher 방식이나, 단일 forward pass에서 다양한 예측을 생성하는 방법(예: multi-head 구조)을 고려할 수 있다.

**적응적 가중치 스케줄링**: 학습 초기에는 지도 손실에 집중하고, 점진적으로 비지도 손실의 가중치를 증가시키는 warmup 전략이 안정적 학습에 도움이 될 수 있다. 논문에서는 고정된 $$\lambda_1, \lambda_2$$를 사용했지만, 적응적 조정이 더 나은 결과를 낳을 수 있다.

**클래스 불균형 상황에서의 적용**: ME 손실은 모든 클래스를 동등하게 취급하므로, 클래스 불균형 데이터셋에서는 소수 클래스에 대한 편향이 발생할 수 있다. 클래스별 가중치 조정이나 focal loss와의 결합을 고려해야 한다.

**확률적 요소의 설계**: 단순한 dropout 외에도 DropBlock, Stochastic Depth, Cutout 등 다양한 확률적 정규화 기법을 TS 손실과 결합하여 더 다양한 변동성을 활용할 수 있다.

**대규모 데이터셋 및 고해상도 영상**: ImageNet 실험에서는 10% 레이블만 사용했는데, 전체 데이터에서의 효과나 고해상도 의료 영상(CT, MRI)에서의 적용 가능성을 추가로 검증할 필요가 있다.

**이론적 분석 강화**: 논문은 주로 실험적 결과에 초점을 두었으나, TS 손실이 일반화 오류에 미치는 영향에 대한 이론적 분석(예: PAC-Bayes bounds, Rademacher complexity)을 추가하면 방법론의 신뢰성을 높일 수 있다.

**다른 준지도 학습 기법과의 비교 및 결합**: Pseudo-labeling, 엔트로피 최소화, VAT 등 다른 준지도 학습 기법과 비교 실험을 수행하고, 이들을 결합하여 상호 보완적 효과를 얻을 수 있는지 탐색해야 한다.

이 논문은 **단순하면서도 효과적인 일관성 정규화 프레임워크**를 제시하여, 준지도 학습 분야에 중요한 기여를 했다. 특히 딥러닝 모델에 내재된 확률성을 정규화에 활용하는 아이디어는 다양한 응용 분야에서 실용적 가치가 크며, 레이블 데이터가 제한적인 환경에서 모델의 일반화 성능을 크게 향상시킬 수 있는 강력한 도구를 제공한다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/360e490a-cffc-45fb-bb1a-a9c28e22405e/1606.04586v1.pdf)
