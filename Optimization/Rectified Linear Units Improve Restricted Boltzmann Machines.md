# Rectified Linear Units Improve Restricted Boltzmann Machines

### 1. 핵심 주장과 주요 기여

이 논문의 핵심 주장은 **Rectified Linear Units (ReLU)가 Restricted Boltzmann Machines (RBMs)의 성능을 크게 향상시킬 수 있다**는 것입니다. Nair와 Hinton이 2010년 ICML에서 발표한 이 논문의 주요 기여는 다음과 같습니다:[1]

첫째, **이론적 혁신**: 전통적인 이진 숨겨진 유닛을 무한 개수의 가중치 공유 이진 유닛으로 대체하는 "Stepped Sigmoid Units (SSU)"를 제안했습니다. 이들은 각각 다른 편향 오프셋을 가지지만 동일한 가중치를 공유합니다.[1]

둘째, **실용적 개선**: 이 SSU를 근사하기 위해 **Noisy ReLU (NReLU)**를 제안했는데, 이는 $$y = \max(0, x + N(0, \sigma(x))) $$ 형태입니다. 여기서 $$N(0, V) $$는 0 평균과 분산 V를 가진 가우시안 잡음입니다.[1]

셋째, **성능 증명**: NORB 물체 인식 작업에서 단일 은닉층 모델은 NReLU가 17.8%의 오류율을 기록한 반면, 이진 유닛은 23.0%를 기록했습니다. 이중 은닉층 모델에서는 NReLU가 15.2%로 이진 유닛의 18.8%를 크게 앞질렀습니다.[1]

### 2. 해결하고자 하는 문제와 제안 방법

#### 문제 정의

RBM이 직면한 주요 문제는 **이진 유닛의 제한된 표현력**이었습니다. 기존의 이진 유닛은 단지 on/off 상태만을 표현할 수 있어, 정보 표현 능력이 제한적이었습니다.[1]

#### 수식을 통한 방법 설명

**전통적인 RBM 학습 규칙:**

기존 RBM의 가중치 업데이트는 다음과 같습니다:[1]

$$ \Delta w_{ij} = \epsilon(\langle v_i h_j \rangle_{\text{data}} - \langle v_i h_j \rangle_{\text{recon}}) $$

여기서 $$\epsilon $$는 학습률이고, $$\langle v_i h_j \rangle_{\text{data}} $$는 훈련 이미지로 구동될 때 시각 유닛 i와 숨겨진 유닛 j가 함께 활성화될 빈도입니다.[1]

**이진 숨겨진 유닛의 활성화 확률:**

$$ p(h_j = 1) = \frac{1}{1 + \exp(-b_j - \sum_{i \in \text{vis}} v_i w_{ij})} $$

**Stepped Sigmoid Units의 수학적 근사:**

논문의 핵심 아이디어는 무한 개의 이진 유닛 사본을 만드는 것입니다. 각 사본은 동일한 가중치 $$w $$와 기본 편향 $$b $$를 가지지만, 편향 오프셋이 $$-0.5, -1.5, -2.5, ... $$ 입니다:[1]

$$\sum_{i=1}^{N} \sigma(x - i + 0.5) \approx \log(1 + e^x) $$

여기서 $$x = vw^T + b $$입니다.[1]

**Noisy ReLU의 구현:**

이 복잡한 샘플링을 피하기 위해, 논문은 다음과 같은 빠른 근사를 제안합니다:[1]

$$ y = \max(0, x + N(0, \sigma(x))) $$

이 수식의 의미는 매우 중요합니다. 입력 $$x $$가 양수면 선형적으로 통과하되 가우시안 잡음이 추가되고, 음수면 0으로 절단됩니다. 분산 $$\sigma(x) $$는 입력에 따라 적응적으로 조정되어 강력한 신호는 큰 잡음을 가지지만, 약한 신호는 작은 잡음을 가집니다.[1]

**에너지 함수:**

가우시안 시각 유닛이 있는 RBM의 에너지 함수:[1]

$$ E(v, h) = \sum_{i \in \text{vis}} \frac{(v_i - b_i)^2}{2\sigma_i^2} - \sum_{j \in \text{hid}} b_j h_j - \sum_{i,j} \frac{v_i}{\sigma_i} h_j w_{ij} $$

### 3. 모델 구조

#### 아키텍처 설계

**NORB 작업용 아키텍처:**

1. **입력 층**: 32×32×2 스테레오 쌍 이미지 (정규화됨)
2. **첫 번째 RBM 층**: 4000개의 NReLU 유닛
3. **두 번째 RBM 층**: 2000개의 NReLU 유닛
4. **분류 층**: 다항 회귀 (6개 클래스)

**LFW 얼굴 인증 작업용 Siamese 아키텍처:**

1. **입력**: 두 개의 32×32×3 컬러 얼굴 이미지
2. **공유 특징 추출기**: 4000개 NReLU 유닛의 단일 은닉층 (RBM으로 사전학습)
3. **메트릭**: 코사인 거리 (두 특징 벡터 간)
4. **출력**: 동일 인물 확률

#### 학습 절차

**사전학습 단계:**
- 방법: Contrastive Divergence (CD)
- 에포크: 300
- 배치 크기: 100
- 학습률: 10^-3
- 모멘텀: 적용됨

**미세조정 단계:**
- 역전파를 사용한 차별적 학습
- 목적 함수: 교차 엔트로피 손실 (분류) 또는 로지스틱 손실 (인증)

### 4. 성능 향상 및 강화 메커니즘

#### 성능 개선 증거

**NORB 데이터셋 결과:**[1]

| 설정 | NReLU | 이진 유닛 |
|------|-------|---------|
| 1 층, 사전학습 없음 | 17.8% | 23.0% |
| 1 층, 사전학습 | 16.5% | 18.7% |
| 2 층, 둘 다 사전학습 | 15.2% | 18.8% |

**LFW 얼굴 인증 결과:**[1]

| 설정 | NReLU | 이진 유닛 |
|------|-------|---------|
| 사전학습 없음 | 0.7925 ± 0.0173 | 0.7768 ± 0.0070 |
| 사전학습 | 0.8073 ± 0.0134 | 0.7777 ± 0.0109 |

#### 성능 향상의 근본 원인

**1. 강도 동변성 (Intensity Equivariance):**

NReLU의 가장 중요한 특성 중 하나는 **강도 동변성**입니다. 영상의 모든 강도를 α > 0으로 스케일링하면:[1]

- 편향이 0이고 잡음이 없는 ReLU 유닛에서는 입력이 0 위, 아래를 벗어나지 않습니다
- 모든 "off" 유닛은 off 상태를 유지하고, 나머지 모든 유닛은 활동을 α배 증가시킵니다.[1]

이는 여러 ReLU 층을 통해 유지되며, LFW 작업에서는 이 성질을 이용하여 **코사인 거리 기반 얼굴 인증**을 수행합니다 (코사인은 강도 불변성을 보장).[1]

이진 유닛은 이러한 성질을 가지지 않으므로, 조명 변화에 더 취약합니다.

**2. 정보 보존:**

이진 유닛은 단지 확률적 on/off만 출력하므로, 상대 강도 정보를 잃습니다. 반면 NReLU는 연속적인 활성 값을 유지하여 깊은 층을 통해 상대 강도 정보를 보존합니다.[1]

**3. 표현력 증가:**

Figure 5에 보이는 히스토그램에 따르면, NORB RBM의 4000개 NReLU는 다양한 활성화 패턴을 보입니다:[1]
- x축 0.2 근처: Gabor와 유사한 필터 (~25%)
- x축 0.6 근처: 점 필터
- x축 0.4~0.5: 전역 필터

이러한 다양성은 이진 유닛으로는 달성할 수 없습니다.

### 5. 일반화 성능 향상

#### 일반화 메커니즘

**1. 지수적으로 많은 선형 모델의 혼합:**

논문의 섹션 7에서는 흥미로운 이론적 통찰을 제시합니다. NReLU는 **지수적으로 많은 선형 모델의 혼합**을 구현합니다:[1]

- N개의 0-편향 ReLU 유닛은 초구의 표면에 2^N개의 영역을 생성합니다
- 각 영역 내에서 모델은 선형이지만, 서로 다른 영역에서는 다른 선형 모델입니다
- 이는 제한된 매개변수로 고도의 비선형성을 달성합니다

**2. 스파시티 유도:**

Figure 5의 히스토그램은 많은 NReLU 유닛이 훈련 이미지에서 자주 비활성화됨을 보여줍니다. 이는 자연적인 정규화 효과를 생성하여 과적합을 감소시킵니다.[1]

**3. 사전학습의 시너지:**

표 2에서 보이듯이, NReLU는 사전학습 없이도 이진 유닛의 사전학습 모델(18.7%)을 능가합니다(16.5%). 이는 NReLU 자체가 더 나은 특징을 학습함을 시사합니다.[1]

#### 검증 데이터 분석

NORB 작업에서:[1]
- 검증 세트는 5개 물체 클래스 각각에서 무작위로 선택된 인스턴스 하나의 9,720개 이미지로 구성
- 이는 테스트 시간에 본 적 없는 인스턴스의 인식을 요구하는 진정한 일반화 문제

LFW 작업에서:[1]
- 사전정의된 10개의 분할로 10-fold 교차 검증 수행
- 훈련과 테스트 세트의 정체성은 항상 분리 (진정한 일반화)

### 6. 한계와 제약

#### 이론적 한계

**1. 근사의 정확도:**

Figure 1에서 보이듯이, Noisy ReLU 근사는 SSU의 정확한 표현이 아닙니다. 특히 음수 x 값에서 근사 오류가 존재합니다. 논문에서는 이를 실제로는 문제가 되지 않음을 경험적으로 보였지만, 이론적 보장은 없습니다.[1]

**2. 하이퍼파라미터 선택:**

- 분산 매개변수 σ(x)의 정확한 선택에 대한 이론적 지침이 부족합니다
- NORB와 LFW 작업 모두에서 4000 유닛이 최고 성능을 보였지만, 더 많은 유닛으로 더 나은 성능을 얻을 가능성이 제시됩니다: "더 많은 유닛으로 더 나은 분류 결과를 항상 얻었으므로, 최고 결과를 가진 아키텍처는 더 많은 숨겨진 유닛을 가질 수 있습니다."[1]

**3. 대규모 실험의 부재:**

논문의 가장 큰 한계는 매우 제한된 스케일의 실험입니다:[1]
- NORB: 32×32×2 저해상도 이미지로 축소
- LFW: 32×32×3로 축소
- 컨볼루션 네트워크와의 비교에서 더 낮은 성능: 컨볼루션 네트 7.2%, NReLU RBM 15.2%[1]

#### 실용적 한계

**1. 계산 복잡도:**

NReLU 샘플링은 정확한 SSU 샘플링보다 빠르지만, 실제 구현에서 가우시안 잡음 생성 오버헤드가 고려되어야 합니다.

**2. 차별적 미세조정 중 Dying ReLU 문제:**

역전파 중 $$y = \max(0, x) $$의 그래디언트를 0(x ≤ 0) 또는 1(x > 0)로 취급할 때, x = 0에서의 불연속성이 문제가 될 수 있습니다. 논문은 이를 무시하지만, 이는 장기 훈련에서 뉴런 죽음 문제로 이어질 수 있습니다.[1]

### 7. 현재 연구(2024-2025)에 미치는 영향과 고려 사항

#### 이 논문의 지속적 영향

**1. ReLU의 표준화:**

이 논문은 ReLU를 현대 딥러닝의 표준 활성화 함수로 만드는 데 결정적인 역할을 했습니다. 오늘날 80% 이상의 10층 이상의 신경망이 ReLU 기반 접근을 사용합니다.[2][3]

ReLU의 주요 장점들은 여전히 유효합니다:[3]
- 소실 기울기 문제 회피: 이진 수렴 구조(음수에서 0, 양수에서 1)는 깊은 층에서 그래디언트 붕괴를 방지합니다
- 계산 효율성: $$\max(0, x) $$ 연산은 기하급수적 함수보다 훨씬 빠릅니다

**2. 후속 활성화 함수 개발:**

이 논문의 성공에 영감을 받아 많은 ReLU 변형이 개발되었습니다:[4][5][6]

| 활성화 함수 | 주요 개선사항 | 2024-2025 상태 |
|----------|-----------|------------|
| Leaky ReLU | 음수 기울기 추가로 dying ReLU 완화[4] | 일반적으로 사용, 특히 노이즈가 많은 데이터 |
| ELU (Exponential Linear Unit) | 음수 값에 대한 부드러운 곡선으로 더 빠른 수렴[5] | 특정 도메인(음성)에서 우수성 증명 |
| PReLU (Parametric ReLU) | 학습 가능한 기울기로 적응적 활성화[4] | 노이즈가 많은 환경에서 ReLU 능가 |
| Swish/GELU | 시그모이드 게이팅으로 더 부드러운 전환 | 변압기 모델의 주류 선택 |
| 훈련 가능한 활성화 함수 (APALU, CosLU, ReLUN) | 훈련 중 동적 적응[7][8] | CIFAR 벤치마크에서 ReLU보다 2.53% 더 높은 정확도 달성 |

**3. 의료 영상에서의 적용:**

NReLU의 강도 동변성 특성은 현대 의료 영상에서 여전히 중요합니다. 최근 연구는 공간-강도 변환(SIT)을 통해 해부학적 및 질감 변화를 분리하려고 시도하고 있습니다. 이는 NReLU의 강도 보존 특성을 확장한 것입니다.[9]

#### 앞으로의 연구 시 고려할 점

**1. RBM과 현대 생성 모델의 관계:**

RBM은 현재 연구에서 덜 직접적으로 사용되지만, 해석 가능성 측면에서 여전히 가치가 있습니다.[10]

- **Variational Autoencoder (VAE)와의 연계**: RBM의 에너지 기반 확률 해석은 VAE의 변분 추론과 이론적으로 관련됨
- **현대 일반화 모델의 선구자**: RBM의 계층적 구조는 현재의 스택된 생성 모델에 영감을 제공함[10]

최근 연구(2021-2025)는 RBM이 생물학적 데이터(신경망 활동 분석, 단백질 패밀리 분석)에서 해석 가능성과 효율성의 좋은 균형을 제공함을 보였습니다.[10]

**2. 강도 동변성의 현대적 응용:**

강도 동변성 개념은 회전 동변성 합성곱(2025)과 같은 더 일반적인 등변성(equivariance) 개념으로 확장되었습니다.[11]

의료 영상에서 이는 다음에 중요합니다:
- 다중 스캐너 조화화(multi-site harmonization)[9]
- 자기공명영상(MRI)과 CT의 강도 정규화
- 조명 변화에 견고한 물체 인식

**3. 깊은 아키텍처에서의 그래디언트 안정성:**

이 논문의 ReLU는 vanishing gradient 문제를 해결했지만, 현대의 매우 깊은 네트워크에서는 새로운 과제가 등장했습니다:[12]

- **배치 정규화와의 상호작용**: 정규화 레이어와 ReLU의 상호작용이 훈련 역학을 복잡하게 함
- **동적 활성화 경계 불안정성**: 최근 연구는 많은 일반적인 파라미터화(표준, 가중치 정규화, 배치 정규화)에서 ReLU 활성화 경계의 진화 불안정성을 식별했습니다[12]

**4. 사전학습 패러다임의 변화:**

2010년의 그리디 RBM 사전학습은 현재 거의 사용되지 않습니다. 대신:[13][14]

- **자기감독 학습**: Contrastive Learning (CLIP), Masked Autoencoder (MAE) 등이 사전학습 표준이 됨
- **블록별 학습**: ResNet-50의 4개 블록을 독립적으로 사전학습하는 방법이 엔드-투-엔드 역전파만큼 효과적임을 보임[13]

**5. 규제와 일반화:**

NReLU의 자연적 스파시티 유도는 명시적 정규화의 필요성을 감소시키지만, 현대 연구는 더 정교한 정규화 기법을 제안합니다:

- **동적 기울기 정규화 (DyT)**: 정규화 레이어를 완전히 제거하면서 훈련 안정성 유지[14]
- **경쟁 기반 적응 ReLU (CAReLU)**: 양수 값이 "경쟁"하도록 강제하여 과활성화 방지[15]

### 결론

"Rectified Linear Units Improve Restricted Boltzmann Machines"은 딥러닝 역사에서 획기적인 논문입니다. ReLU를 도입함으로써 단순하면서도 매우 효과적인 활성화 함수를 제공하여, 심층 신경망의 훈련을 혁신했습니다.[3][1]

특히 이 논문의 강도 동변성 분석은 단순한 활성화 함수 선택이 얼마나 깊은 이론적 함의를 가질 수 있는지 보여줍니다. 비록 RBM이 현대에는 덜 직접적으로 사용되지만, ReLU의 원리와 일반화 특성은 계속해서 현대 딥러닝 아키텍처의 기초를 이루고 있습니다.[2][3]

앞으로의 연구에서는:

1. **더 적응적인 활성화 함수** 개발: 고정 형태가 아닌 훈련 중 학습되는 활성화 함수의 탐색[7][8]

2. **등변성 개념의 확장**: 강도 동변성을 넘어 회전, 스케일, 기타 변환에 대한 등변성 설계[11]

3. **해석 가능성과 효율성의 균형**: RBM의 해석 가능성 장점을 현대 생성 모델에 통합하는 방법[10]

4. **매우 깊은 아키텍처의 안정성**: 새로운 활성화 함수와 정규화 기법의 조합이 극도로 깊은 네트워크에서 안정적인 훈련을 가능하게 하는지 연구[14][12]

5. **멀티모달과 자기감독 학습 시대의 활성화 함수**: 현재의 대규모 자기감독 사전학습 패러다임에 최적화된 새로운 활성화 함수 개발

이 논문은 단순한 아이디어가 어떻게 과학 분야를 변혁할 수 있는지 보여주는 좋은 사례입니다주는 좋은 사례입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/22f03898-7d4e-4e7a-a370-e3b853b24adf/reluICML.pdf)
[2](https://link.springer.com/10.1007/s40808-025-02630-6)
[3](https://aihint.co.uk/relu-in-deep-learning-why-its-the-default-activation-function/)
[4](https://ieeexplore.ieee.org/document/10718061/)
[5](https://arxiv.org/pdf/1511.07289v2.pdf)
[6](https://arxiv.org/pdf/2104.02523.pdf)
[7](http://www.emerald.com/ijwis/article/20/4/452-469/1216720)
[8](https://www.mdpi.com/2078-2489/12/12/513/pdf)
[9](https://pmc.ncbi.nlm.nih.gov/articles/PMC10651358/)
[10](https://arxiv.org/html/2501.04387v1)
[11](https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_Rotation-Equivariant_Self-Supervised_Method_in_Image_Denoising_CVPR_2025_paper.pdf)
[12](https://arxiv.org/html/2305.15912v4)
[13](http://arxiv.org/pdf/2302.01647.pdf)
[14](https://arxiv.org/html/2503.10622v1)
[15](https://arxiv.org/html/2407.19441v1)
[16](https://ieeexplore.ieee.org/document/10754939/)
[17](https://ieeexplore.ieee.org/document/10723936/)
[18](https://ieeexplore.ieee.org/document/11076877/)
[19](https://ieeexplore.ieee.org/document/11064020/)
[20](https://ejournal.nusamandiri.ac.id/index.php/jitk/article/view/5258)
[21](https://internationalpubls.com/index.php/pmj/article/view/2934)
[22](https://ieeexplore.ieee.org/document/10906880/)
[23](https://arxiv.org/html/2408.09156)
[24](https://arxiv.org/pdf/2402.08244.pdf)
[25](https://arxiv.org/pdf/2104.03693.pdf)
[26](http://arxiv.org/pdf/2406.02529.pdf)
[27](https://www.cs.toronto.edu/~fritz/absps/reluICML.pdf)
[28](https://www.isca-archive.org/interspeech_2015/sivadas15_interspeech.pdf)
[29](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)
[30](https://www.superannotate.com/blog/activation-functions-in-neural-networks)
[31](https://www.nature.com/articles/s41467-022-33126-x)
[32](https://inass.org/wp-content/uploads/2024/12/2025043012-2.pdf)
[33](https://arxiv.org/pdf/1909.06134.pdf)
[34](http://arxiv.org/pdf/2402.11137.pdf)
[35](https://www.mdpi.com/1099-4300/18/7/251/pdf?version=1467969203)
[36](https://arxiv.org/pdf/1803.01164.pdf)
[37](https://pmc.ncbi.nlm.nih.gov/articles/PMC11509008/)
[38](https://arxiv.org/pdf/2303.04143.pdf)
[39](https://arxiv.org/pdf/1804.09812.pdf)
[40](https://en.wikipedia.org/wiki/Unsupervised_learning)
[41](https://www.nature.com/articles/s41598-024-68183-3)
[42](https://www.reddit.com/r/MachineLearning/comments/1giovxi/r_what_is_your_recipe_for_training_neural/)
[43](https://www.cs.cmu.edu/~rsalakhu/papers/neco_DBM.pdf)
[44](https://pubs.acs.org/doi/abs/10.1021/acs.iecr.1c02768)
[45](https://openreview.net/forum?id=nmRY3BAll4)
