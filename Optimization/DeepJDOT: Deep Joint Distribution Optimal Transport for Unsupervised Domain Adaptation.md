# DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation

### 1. 핵심 주장과 주요 기여[1]

**DeepJDOT**(Deep Joint Distribution Optimal Transport)는 컴퓨터 비전의 **도메인 시프트 문제**를 해결하기 위한 혁신적인 방법입니다. 이 논문의 핵심 주장은 다음과 같습니다:[1]

**주요 주장**: 최적 운송 이론에 기반하여 소스 도메인(레이블이 있는)과 타겟 도메인(레이블이 없는) 간의 **결합 표현-레이블 분포를 정렬**함으로써, 단순히 표현만 맞추는 것이 아니라 분류 성능까지 동시에 보존할 수 있습니다.

**주요 기여**:

- **확장성 해결**: 기존 JDOT 방법은 $$n_1 \times n_2$$ 크기의 결합 행렬을 풀어야 하므로 대규모 데이터셋에 적용하기 어렵습니다. DeepJDOT는 **미니배치 기반 확률적 최적화**를 도입하여 이를 해결합니다.[1]

- **의미적 표현 학습**: 기존 방법은 입력 공간(픽셀 수준)에서 거리를 계산하지만, DeepJDOT는 CNN의 **깊은 층의 의미적 표현** 공간에서 최적 운송 결합을 학습합니다.[1]

- **통합 프레임워크**: 특징 추출자 $$g$$, 분류기 $$f$$, 최적 운송 결합 $$\Gamma$$를 **단일 CNN 프레임워크 내에서 동시에 학습**합니다.

***

### 2. 문제 정의, 제안 방법, 모델 구조, 성능 및 한계[1]

#### 2.1 문제 정의[1]

컴퓨터 비전에서 **도메인 시프트**는 소스 도메인에서 잘 학습한 분류기를 타겟 도메인에 적용할 때 심각한 성능 저하를 야기합니다. 예를 들어:

- MNIST 데이터셋에 학습된 모델을 MNIST-M(컬러 배경 추가)에 적용하면 정확도가 60.8%로 떨어집니다.
- 조명, 배경, 색상 공간 등의 변화로 인해 발생합니다.

**목표**: 타겟 도메인의 레이블 없이 소스 도메인의 정보를 활용하여 타겟 도메인에 적합한 분류기를 학습하는 것(비지도 도메인 적응)

#### 2.2 제안 방법 및 수식[1]

**기초 개념 - 최적 운송**:

두 확률 분포 $$\rho_1$$과 $$\rho_2$$ 간의 최적 운송은 다음과 같이 정의됩니다:

$$OT_c(\rho_1, \rho_2) = \min_{\Gamma} \left\langle \Gamma, C \right\rangle_F$$

여기서 $$\Gamma$$는 결합 행렬(coupling matrix), $$C$$는 비용 행렬입니다. 이산 설정에서:

$$\min_{\Gamma \in \Pi} \left\langle \Gamma, C \right\rangle_F$$

**JDOT와의 차이점**:

기존 JDOT의 비용 함수는:

$$d(x_i^s, y_i^s; x_j^t, y_j^t) = \alpha c(x_i^s, x_j^t) + \alpha_t L(y_i^s, y_j^t)$$

여기서 $$c(\cdot, \cdot)$$는 특징 공간의 $$\ell_2$$ 거리, $$L(\cdot, \cdot)$$는 분류 손실입니다.

**DeepJDOT의 목적 함수**:

DeepJDOT는 다음의 최적화 문제를 풀어냅니다:[1]

$$\min_{\Gamma, f, g} \frac{1}{n_s} \sum_i L_s(y_i^s, f(g(x_i^s))) + \sum_{i,j} \Gamma_{ij} \left[\|g(x_i^s) - g(x_j^t)\|^2 + \alpha_t L_t(y_i^s, f(g(x_j^t)))\right]$$

핵심 차이점:
- $$g(x^s)$$와 $$g(x^t)$$ - **CNN의 깊은 층에서의 의미적 표현**
- $$\Gamma_{ij}$$ - 결합 행렬이 자동으로 상관성 있는 샘플들을 연결

**확률적 최적화**:

계산 복잡도를 해결하기 위해, 미니배치 크기 $$m$$으로 근사합니다:

$$\min_{\Gamma, f, g} E\left[\frac{1}{m}\sum_{i=1}^m L_s(y_i^s, f(g(x_i^s))) + \min_\Gamma \sum_{i,j} \Gamma_{ij}\left[\|g(x_i^s) - g(x_j^t)\|^2 + \alpha_t L_t(y_i^s, f(g(x_j^t)))\right]\right]$$

#### 2.3 모델 구조[1]

**DeepJDOT 아키텍처**:

1. **특징 추출자** $$g: X \rightarrow Z$$: CNN의 깊은 층으로, 입력을 의미적 표현으로 변환
2. **분류기** $$f: Z \rightarrow Y$$: 완전연결층 + softmax로 클래스 확률 계산
3. **최적 운송 결합** $$\Gamma$$: 소스-타겟 샘플 간의 의미적 유사성 기반 매칭

**학습 알고리즘** (알고리즘 1):[1]

```
입력: xs (소스 도메인 샘플), xt (타겟 도메인 샘플), ys (소스 도메인 레이블)

각 미니배치마다:
  1. g와 f를 고정, 식(8)에 따라 Γ를 계산
  2. Γ를 고정, 식(9)에 따라 g와 f를 경사하강으로 업데이트
```

**네트워크 구성** (숫자 분류 실험):

- 6개의 3×3 합성곱 층 (필터: 32, 32, 64, 64, 128, 128)
- 1개의 완전연결층 (128개 숨은 유닛)
- Adam 최적화기 (학습률: 2×10⁻⁴)
- 미니배치 크기: 500 (소스 도메인: 50개 샘플/클래스)
- 하이퍼파라미터: $$\alpha = 0.001$$, $$\alpha_t = 0.0001$$

#### 2.4 성능 향상[1]

**숫자 분류 작업**:

| 적응 과제 | MNIST↔USPS | SVHN→MNIST | MNIST→MNIST-M |
|---------|-----------|-----------|--------------|
| DeepJDOT | 96.4% | 96.7% | 92.4% |
| 목표만 학습 | 98.7% | 98.7% | 96.8% |
| 성능 향상 폭 | +36.8p | +36.0p | +31.6p |

(소스만 학습의 정확도: MNIST→USPS 59.6%, SVHN→MNIST 60.7%, MNIST→MNIST-M 60.8%)

**Office-Home 데이터셋**:

- DeepJDOT 평균 정확도: **50.67%**
- DANN: 44.94%, CORAL: 37.91%
- 평균 **5.73% 향상**

**VisDA-2017 도전과제**:

- DeepJDOT 평균 정확도: **66.9%** (소스만 28.0%)
- 평균 **38.9% 향상**
- 순위: 평균 정확도 기준 6위, 적응 성능 차이 기준 3위

#### 2.5 한계[1]

1. **Catastrophic Forgetting**: 초기 목적함수(식 5)는 타겟 도메인 분류기에만 집중하여 소스 도메인 성능이 저하될 수 있습니다. 이를 보정하기 위해 소스 손실항을 추가했습니다.

2. **확률적 근사의 부정확성**: 미니배치 기반 최적 운송은 전체 샘플 간의 진정한 최적 결합과 다를 수 있으며, 실제로는 존재하지 않았을 샘플 간 연결이 생길 수 있습니다.

3. **입력 수준에서의 정보 손실**: 결합을 깊은 층에서만 계산하므로, 얕은 층의 저수준 특징 정보는 활용되지 않습니다.

4. **VisDA-2017 제한된 성능**: 일부 클래스(예: 자동차, 칼)에서 음의 전이(negative transfer) 현상 발생

5. **확장성 제약**: 여전히 미니배치 크기에 따라 성능이 영향을 받으며, 극도로 대규모 데이터셋에서는 한계가 있을 수 있습니다.

6. **하이퍼파라미터 민감성**: $$\alpha$$와 $$\alpha_t$$ 파라미터를 수동으로 조정해야 합니다.

***

### 3. 일반화 성능 향상 가능성[1]

#### 3.1 이론적 근거[1]

논문의 핵심 강점은 **Wasserstein 거리 기반의 이론적 보장**입니다. 기존 JDOT에서 증명된 바에 따르면, 결합 분포를 정렬하는 것은 도메인 적응 학습 한계(learning bound)를 최소화하는 것과 동치입니다.

#### 3.2 t-SNE 시각화 분석[1]

MNIST→MNIST-M 적응 작업에서의 임베딩 품질:

- **Source Only**: 소스 샘플은 잘 군집화되지만 타겟 샘플은 분산됨
- **StochJDOT** (이미지 공간의 최적 운송): 분포 정렬 실패
- **DANN** (적대적 방법): 부분적 정렬만 달성
- **DeepJDOT**: **완벽한 분포 정렬** + 클래스 간 최대 여백 달성

이는 DeepJDOT가 표현 정렬뿐만 아니라 **판별 정보 보존**에 우수함을 보여줍니다.

#### 3.3 절제 연구(Ablation Study)[1]

각 손실 항의 기여도:

| 설정 | USPS↔MNIST | MNIST→MNIST-M |
|-----|-----------|--------------|
| 전체 모델 | 96.4% | 92.4% |
| $$L_s + d$$ 만 | 95.53% | 82.3% |
| $$d + L_t$$ 만 | 86.41% | 73.6% |

- 소스 손실 $$L_s$$: catastrophic forgetting 방지
- 특징 정렬 $$d$$: 도메인 분포 정렬
- 타겟 손실 $$L_t$$: 레이블 전파 및 판별 학습

세 항 모두가 **상호보완적**이며, 특징 공간이 깊은 CNN 층일 때 최적 성능을 나타냅니다.

#### 3.4 일반화 성능 향상 메커니즘[1]

1. **의미적 표현 정렬**: 깊은 CNN 층에서 최적 운송을 학습하므로, 저수준의 픽셀 차이보다는 **고수준의 의미적 유사성**에 기반한 정렬
2. **판별 정보 보존**: 단순 분포 정렬이 아니라, 분류 손실을 결합함으로써 **클래스 간 구분성** 유지
3. **확률적 정규화 효과**: 미니배치 기반 근사가 이웃 샘플 간의 질량 공유를 촉진하여 **정규화** 역할

***

### 4. 최신 연구 기반 영향 및 고려사항

#### 4.1 논문의 영향과 기여[2][3][4]

**도메인 적응 분야의 혁신**:

2018년 발표 이후, DeepJDOT는 **최적 운송 기반 도메인 적응의 선두주자**로 평가됩니다. 특히:[4]

- 확장성 있는 스토캐스틱 최적 운송 방법론 제시
- 깊은 표현 학습과 최적 운송의 통합이라는 새로운 관점 제공
- 이후 연구에서 광범위하게 인용되며 최적 운송 기반 적응 방법들의 모범 사례 제시

#### 4.2 최신 연구 동향(2023-2025)[5][3][6][2]

**도메인 일반화(Domain Generalization)로의 확장**:

최근 연구는 단순한 소스-타겟 이분 설정에서 벗어나 **다중 도메인 일반화**로 확대되었습니다:[7][8][2]

- **Source-Free UDA**: 소스 데이터에 접근 불가능한 실제 상황에서의 적응[3]
- **Meta-Learning 통합**: 여러 소스 도메인에서 학습한 메타-파라미터로 신규 타겟에 빠르게 적응[8]
- **정보 이론 통합**: 상대 엔트로피 정규화와 결합된 새로운 UDA 프레임워크[2]

**최적 운송 이론의 발전**:

최근 연구들은 다음과 같은 개선을 보고합니다:[4]

- **Gromov-Wasserstein 거리**: 메트릭 공간 간 구조 보존 정렬
- **Unbalanced OT**: 클래스 불균형 문제 해결
- **Neural OT**: 신경망 기반 최적 운송 계산으로 초고차원 문제 해결
- **Partial OT**: 부분적 정렬이 필요한 경우 대응

**비전-언어 모델의 활용**:

2024년 연구에 따르면, CLIP, DINO 등 사전학습된 비전-언어 모델을 특징 추출자로 사용할 때:[6]

- 기존 UDA 방법의 성능이 **최대 10% mIoU 향상**
- 미보유 도메인에 대한 일반화가 **13.7% 이상 개선**

#### 4.3 앞으로의 연구 시 고려사항

**1. 다중 도메인 시나리오**[1]

논문에서 명시한 미래 방향: DeepJDOT를 여러 소스 도메인에서 동시에 학습하는 설정으로 확장

**최신 접근**:
- **메타-학습 기반**: 여러 도메인 간의 공통 부분공간 학습
- **대조 학습**: 도메인 간 차이를 최소화하면서 클래스 내 유사성 극대화

**2. 계층 간 유사성 활용**[1]

논문에서 제안한: "임베딩 층들 간의 표현 유사성" 및 "여러 분류기 간의 레이블 유사성을 고려한 비용 함수"

**현대적 확장**:
- **멀티스케일 특징 정렬**: 다양한 해상도의 CNN 층에서 동시 정렬
- **계층 간 시간적 일관성**: 적응 과정 중 깊은 층과 얕은 층 간의 정합성 유지

**3. 약한 감독 신호 활용**

**Source-Free 설정**: 소스 데이터 없이 사전학습 모델만으로 적응
- 개인정보보호 관점에서 중요
- 실제 산업 응용에서 빈번히 필요

**4. 동적 도메인 시프트**

시간에 따라 변하는 도메인 분포에 대응:
- 온라인 학습과의 결합
- Continual Domain Adaptation

**5. 비전 외 모달리티로의 확장**

- **그래프 도메인 적응**: 그래프 신경망과 최적 운송의 결합[9]
- **시계열 데이터**: 센서 데이터나 금융 데이터의 도메인 시프트
- **자연언어처리**: 다국어 간 지식 전이

**6. 계산 효율성**

**현재 도전**:
- 여전히 미니배치 크기에 종속적
- 극도로 고차원인 데이터에서 최적 운송 계산의 복잡도

**최근 진전**:
- Sliced Wasserstein 거리로 계산량 감소
- GPU/TPU 병렬화를 통한 확장성 개선

**7. 이론적 검증 강화**

- **일반화 한계(Generalization Bounds)**: 더욱 엄밀한 이론적 분석
- **수렴성 보장**: 확률적 최적화의 수렴 속도 분석
- **최적성(Optimality) 조건**: 비볼록 최적화 문제의 해의 특성 규명

***

## 결론

**DeepJDOT**는 비지도 도메인 적응 분야에서 최적 운송 이론과 심층 학습의 **획기적인 결합**을 제시합니다. 그 주요 공헌은 확장 가능한 확률적 최적화를 통해 의미적 표현 공간에서 결합 분포를 정렬함으로써, **표현 정렬과 판별 학습을 동시에** 달성한 점입니다.[1]

2024-2025년 최신 연구 동향을 고려하면, DeepJDOT의 프레임워크는 다음과 같이 진화하고 있습니다:[3][2][4]

1. **소스-프리 설정**으로의 확장으로 실무 적용성 증대
2. **메타-학습**과의 결합으로 다중 도메인 일반화 성능 향상
3. **사전학습 모델**(CLIP, DINO 등) 활용으로 기초 성능 대폭 상향
4. **그래프 신경망**을 포함한 비전 외 모달리티로의 확장

향후 연구에서는 이러한 확장과 함께, **이론적 보장의 강화**, **계산 효율성 개선**, **실시간 동적 도메인 시프트 대응**이 핵심 과제가 될 것입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/b7eae9cd-e403-48ca-a969-8a9c6ebd0888/1803.10081v3.pdf)
[2](https://www.mdpi.com/1099-4300/27/4/426)
[3](https://pubmed.ncbi.nlm.nih.gov/38490115/)
[4](https://arxiv.org/abs/2306.16156)
[5](https://arxiv.org/html/2502.06272v1)
[6](https://arxiv.org/abs/2411.16407)
[7](http://scis.scichina.com/en/2025/112103.pdf)
[8](https://arxiv.org/pdf/2411.12913.pdf)
[9](https://www.nature.com/articles/s41598-024-59890-y)
[10](https://arxiv.org/pdf/2201.01806.pdf)
[11](http://arxiv.org/pdf/1607.03516.pdf)
[12](https://arxiv.org/pdf/2308.00287.pdf)
[13](http://arxiv.org/pdf/2303.03770.pdf)
[14](https://arxiv.org/pdf/2402.16090.pdf)
[15](https://arxiv.org/pdf/2110.12024.pdf)
[16](https://otml2021.github.io)
[17](https://github.com/changwxx/Awesome-Optimal-Transport-in-Deep-Learning)
[18](https://pure.korea.ac.kr/en/publications/a-broad-study-of-pre-training-for-domain-generalization-and-adapt/)
[19](https://drpress.org/ojs/index.php/HSET/article/view/29425)
