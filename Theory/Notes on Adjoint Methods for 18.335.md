# Notes on Adjoint Methods for 18.335

### 1. 핵심 주장 및 기여도 요약

**Steven G. Johnson의 "Notes on Adjoint Methods for 18.335"**는 대규모 최적화 문제와 역설계 문제에서 구배(gradient)를 효율적으로 계산하는 수학적 기초를 제공하는 중요한 교육 자료입니다.[1]

논문의 **핵심 주장**은 다음과 같습니다. 복잡한 계산의 결과 $$x \in \mathbb{R}^M$$이 P개의 설계 변수 $$p$$에 의해 매개변수화될 때, 함수 $$g(x,p)$$의 구배 $$\frac{dg}{dp}$$를 계산하는 것이 어렵습니다. 직접적인 방법은 $$O(M^2P)$$의 계산 복잡도를 요구하여 P가 클 때 비실용적입니다. **수반(adjoint) 방법은 이 문제를 해결하여 구배 계산 비용이 P와 무관하고, 원래 문제를 한 번 푸는 것과 비슷한 수준의 비용으로 감소**시킵니다.[1]

**주요 기여**는 다음과 같습니다:

1. **선형 방정식의 수반 방법**: $$Ax = b$$ 형태의 선형 문제에서 $$\lambda^T = g_x A^{-1}$$를 만족하는 수반 변수를 정의하여 구배를 효율적으로 계산[1]

2. **비선형 방정식으로의 확장**: $$f(x,p) = 0$$ 형태의 비선형 문제에서 $$f_x^T \lambda = g_x^T$$를 풀어 수반 방법 적용[1]

3. **고유값 문제에 대한 응용**: 고유값 최적화 문제에서 특이 연산자를 처리하는 방법을 제시[1]

4. **시간 의존 문제로의 확장**: ODE 적분 문제에서 역시간 적분을 통한 수반 방법 설명[1]

5. **역설계 구체 사례**: Schrödinger 고유방정식 예제를 통한 실무적 구현 제시[1]

---

### 2. 문제 정의, 제안 방법 및 모델 구조

#### 2.1 해결하고자 하는 문제

**근본적 문제**: 매개변수화된 복잡한 계산 시스템에서 목적 함수의 구배를 계산하는 것[1]

수학적으로, 주어진:
- 복잡한 계산으로부터 얻은 결과 $$x \in \mathbb{R}^M$$ 
- P개의 설계 변수 $$p \in \mathbb{R}^P$$
- 목적 함수 $$g(x,p)$$

**목표**: $$\frac{dg}{dp}$$를 효율적으로 계산하면서 P가 클 때도 실행 가능성 보장[1]

**기존 접근법의 한계**:
직접 미분을 통해 $$\frac{dg}{dp} = g_p + g_x x_p$$를 계산하려면, $$x_p = A^{-1}(b_p - A_p x)$$를 P번 계산해야 하므로 $$O(M^2P)$$ 복잡도 발생[1]

#### 2.2 제안 방법 및 수식

**기본 전략**: 행렬 곱의 결합성(associativity)을 이용하여 순서를 재배치

**선형 방정식의 경우** ($$Ax = b$$):

직접 계산:

$$g_x x_p = g_x \underbrace{[A^{-1}(b_p - A_p x)]}_{M \times P}$$

수반 방법을 통한 재배치:

$$g_x x_p = [g_x A^{-1}] \underbrace{(b_p - A_p x)}_{M \times P} = [\lambda^T] (b_p - A_p x)$$

여기서 $$\lambda$$는 **수반 방정식**을 풀어 얻습니다:

$$A^T \lambda = g_x^T$$

최종 구배:

$$\frac{dg}{dp}\bigg|_{f=0} = g_p - \lambda^T (A_p x - b_p)$$

**이 방법의 이점**:
- 수반 문제는 크기가 $$M \times M$$으로 원래 문제와 동일
- 한 번의 선형 방정식 풀이로 완료 ($$O(M^3)$$이 상수 인수로만 증가)
- 전체 복잡도: $$O(M^3 + M \cdot P)$$ → P에 무관[1]

**비선형 방정식의 경우** ($$f(x,p) = 0$$):

수반 방정식:
$$f_x^T \lambda = g_x^T \quad \text{(Eq. 2)}$$

구배:
$$\frac{dg}{dp}\bigg|_{f=0} = g_p - \lambda^T f_p \quad \text{(Eq. 3)}$$

#### 2.3 모델 구조: 고유값 문제의 예

논문은 고유값 문제 $$Ax = \alpha x$$에서의 복잡성을 상세히 다룹니다.[1]

확장된 미지수 벡터:

$$\tilde{x} = \begin{pmatrix} x \\ \alpha \end{pmatrix}$$

정규화 조건($$x^T x = 1$$)을 포함한 확대 방정식 체계:

$$\tilde{f} = \begin{pmatrix} Ax - \alpha x \\ x^T x - 1 \end{pmatrix} = 0$$

이는 $$M+1$$개의 미지수와 $$M+1$$개의 방정식을 제공합니다.

수반 변수 $$\tilde{\lambda} = (\lambda, \beta)^T$$에 대한 수반 방정식:

$$(A-\alpha I)\lambda = g_x^T - 2\beta x$$

$$-x^T\lambda = g_\alpha$$

**핵심 도전**: $$(A-\alpha I)$$는 특이 행렬(null space가 $$x$$)[1]

**해결책**: 
- 우변이 $$x$$에 직교하도록 $$\beta = x^T g_x^T / 2$$ 설정
- 투영 연산자 $$P = 1 - xx^T$$ 사용
- 수정된 수반 방정식: $$(A-\alpha I)\lambda_0 = P g_x^T$$[1]

최종 구배:
$$\frac{dg}{dp} = g_p - \lambda_0^T A_p x + g_\alpha x^T A_p x$$

#### 2.4 시간 의존 문제의 구조

**초기값 문제** ($$\dot{x} = Bx$$, $$x(0) = b$$):

정상해: $$x(t) = e^{Bt}b$$

수반 ODE:
$$\dot{\lambda} = B^T \lambda, \quad \lambda(0) = g_x^T$$

**중요한 성질**: $$B$$와 $$B^T$$의 고유값이 동일하므로 안정성이 보존[1]

**계산 문제**: $$A_p = -\int_0^t e^{-Bt'} B_p e^{-B(t-t')} dt'$$

이로 인해 **모든 시간 $$0 \leq t' \leq t$$에서 $$x(t')$$을 저장**해야 함 → 메모리 집약적[1]

***

### 3. 모델의 성능 향상 및 한계

#### 3.1 성능 향상

**계산 효율성**:

| 항목 | 직접 방법 | 수반 방법 |
|------|---------|---------|
| 복잡도 | $$O(M^2P)$$ | $$O(M^3 + MP)$$ |
| P의 영향 | 선형 증가 | 무영향 |
| 실용성(P ≫ 1) | 불가능 | 가능[1] |

**적용 영역**:
- **역설계(Inverse Design)**: 형상/위상 최적화에서 수백~수천 개의 설계 변수 처리[1]
- **심층 학습**: 신경망의 백프로퍼게이션(backpropagation)으로도 알려짐[1]
- **대규모 최적화**: P가 수백만에 달하는 경우도 가능

**Schrödinger 역설계 예제의 성능**:
논문은 구체적인 예제를 통해 성능을 입증합니다. 100차원 공간에서:
- 500번의 비선형 켤레 구배(nonlinear CG) 반복 후 빠른 수렴[1]
- 불연속 함수($$\psi_0(x) = 1-|x|$$ for $$|x| < 0.5$$)를 목표로 하는 경우 5000번 반복으로 수렴[1]

#### 3.2 한계 및 제약

**1. 메모리 문제 (시간 의존 문제)**
- $$x(t')$$을 모든 시간 지점에서 저장해야 함
- 장시간 통합 또는 세밀한 시간 이산화에서 심각한 병목[1]

**2. 비선형 방정식의 특이성**
- 고유값 문제에서 $$(A-\alpha I)$$의 특이성 처리 필요
- 퇴화된(degenerate) 고유값의 경우 더욱 복잡한 처리 필요[1]

**3. 자동 미분(AD)의 한계**
- 외부 라이브러리(Fortran 등)를 호출하는 코드는 AD 불가능
- 반복적/근사 알고리즘에 대한 AD의 비효율성[1]

**4. 수렴 속도**
- Schrödinger 예제에서 불연속 특성은 느린 수렴 (5000회 vs 500회)
- 전조건화(preconditioning) 또는 다른 최적화 방법이 필요할 수 있음[1]

**5. 고차 미분의 복잡성**
- 2차 이상의 도함수 계산은 별도 수반 문제 필요
- 계산 복잡도가 증가[1]

***

### 4. 모델 일반화 성능 향상

#### 4.1 일반화 성능 향상의 메커니즘

논문의 수반 방법이 모델 일반화 성능을 향상시키는 방식:

**1. 매개변수 공간에서의 최적화 효율성**
- 구배 기반 최적화에서 P 무관 복잡도는 **큰 매개변수 공간에서도 안정적인 최적화** 가능
- 이는 더 정교한 모델(더 많은 매개변수)을 사용할 수 있음을 의미[1]

**2. 역설계를 통한 모델 구조 개선**
- Schrödinger 예제: 목표 함수 $$\psi_0$$에 맞추는 퍼텐셜 $$V(x)$$를 찾음[1]
- 이는 **주어진 행동을 구현하는 구조를 설계**하는 능력 제공
- 신경망 아키텍처 최적화에 유사하게 적용 가능

**3. 자동 미분과의 결합**
최신 연구에서 수반 방법과 자동 미분의 결합:

- **Vector-Jacobian product (vJp)** 효율화: 임의의 벡터 $$v$$에 대해 $$v^T x_p$$를 싸게 계산[1]
- 역모드 자동 미분(reverse-mode AD)으로 구현되어 신경망 학습에 직접 적용[1]

#### 4.2 최신 연구 기반 일반화 성능 향상 (2023-2025)

**1. 신경 상미분방정식(Neural ODE)의 적응형 체크포인트 수반 방법**[2]

기존 수반 방법의 문제:
- 전진 모드와 후진 모드 궤적이 불일치하여 수치 오차 발생
- 순진한 방법은 깊은 계산 그래프로 인해 기울기 오류

해결책:
- **적응형 체크포인트 수반(ACA) 방법**: 전진 모드 궤적을 후진 모드로도 사용하여 정확성 보장
- 성과: 이미지 분류에서 기존 수반 방법 대비 **오류율 절반, 속도 2배**
- NODE가 ResNet을 능가하는 성능 달성[2]

**2. 음함수 미분의 효율화**[3]

음함수 함수 $$g(y(x), x) = 0$$에 대한 미분:
- 암묵적 함수 정리 vs 일반화된 수반 방법
- **일반화된 수반 방법이 자동 미분에 더 효과적**[3]

**3. 역모드 미분의 새로운 방향성**[4]

**Moonwalk**: 역함수 네트워크에 대한 새로운 전진 모드 미분

메커니즘:
- Vector-inverse-Jacobian product 활용
- 계산 시간: 후진 모드처럼 선형 ($$O(N)$$)
- **메모리: 백프로퍼게이션보다 훨씬 적음**

성과:
- 가역 및 우측 가역 네트워크에서 **메모리 사용 대폭 감소**
- 백프로퍼게이션과 유사한 속도 유지[4]

**4. 암묵적 수치 스킴의 미분화**[5]

대규모 수문(hydrological) 모델링에 적용:

기존 한계:
- 암묵적 스킴은 AD로 기울기 소실/메모리 문제
- 이전에는 명시적 스킴만 미분화 가능

혁신:
- **"Discretize-then-Optimize" 수반 방법**으로 암묵적 스킴 미분화 가능
- 성과: Kling-Gupta 효율 지표에서 기존 명시적 모델 초과[5]

**5. 자동 미분 라이브러리의 발전**[6]

스펙트럼 PDE 솔버에 역모드 AD 적용:

특징:
- 경계 조건, 제약 조건, 게이지 선택 자동 처리
- 기존 방식: 연속 수반 → 분석적 도출 후 이산화
- 새로운 방식: **이산 수반을 AD로 자동 생성**

성능:
- 사용자 개입 최소화 (수 줄의 코드만 추가)
- 고차 시간 간격 스킴에 자동 적용 가능[6]

#### 4.3 일반화 성능 측정

**수렴성 지표**:
- Schrödinger 예제에서 매끄러운 함수($$\psi_0(x) = 1+\sin[\pi x + \cos(3\pi x)]$$): 500회 반복 후 우수한 수렴[1]
- 불연속 함수: 5000회 반복 필요, 느린 수렴 示唆[1]

**최신 연구 성과**:[2][4][5]
- ACA를 이용한 NODE: ResNet 능가
- Moonwalk: 메모리 사용량 대폭 감소
- 암묵적 스킴 미분화: LSTM 초과 성능

***

### 5. 결론 및 미래 연구 방향

#### 5.1 논문의 역사적 중요성

Steven G. Johnson의 노트는 **2006년 작성 후 2021년 업데이트**되었습니다. 이는 기초 이론이 얼마나 안정적이었는지를 보여줍니다. 백프로퍼게이션, 역 모드 자동 미분, 신경망 최적화의 이론적 기반을 제공합니다.[1]

#### 5.2 미래 연구의 고려 사항

**1. 메모리-계산 트레이드오프 최적화**
- **그래디언트 체크포인트(Gradient Checkpointing)**: 중간 결과 선택적 저장으로 메모리-계산 균형[2]
- **재계산 전략**: 필요시만 중간값 재계산

**2. 혼합 정밀도 수치 계산**
- 부동 소수점 오차 축적 문제, 특히 장시간 적분
- 수치 안정성 분석 강화 필요

**3. 확률적 최적화와의 융합**
- 미니배치 처리와 수반 방법의 결합
- 대규모 데이터 시나리오에서의 확장성

**4. 하드웨어 가속 최적화**
- GPU/TPU 활용 시 메모리 병목 제거
- 분산 학습 환경에서의 수반 방법 적용[7]

**5. 음함수 신경망과의 통합**
- Neural ODE, Implicit Deep Learning의 미분화
- 끝단 학습(End-to-end learning)의 신뢰성 향상[5]

**6. 다중 물리 시뮬레이션**
- 복합 시스템(coupling multiple PDE)의 수반 방법
- 역설계의 복잡도 증가에 따른 알고리즘 개선

#### 5.3 최신 기술 트렌드 (2024-2025)

**Moonwalk **: 가역 네트워크를 위한 혁신적 접근[4]
- 기존: 백프로퍼게이션이 유일
- 신규: 전진 모드로도 백프로퍼게이션 수준의 효율

**스펙트럼 PDE 자동 미분 **: 고차 방법의 자동화[6]
- 분석적 도출 불필요
- 프로그래밍 복잡도 대폭 감소

**드래드(DrMAD) **: 하이퍼파라미터 최적화의 메모리 효율화[8]
- 기존: 전체 훈련 궤적 저장
- 신규: 지름길을 통한 근사 역추적로 **메모리 100배 감소, 속도 45배 향상**

***

### 최종 요약

Johnson의 "Notes on Adjoint Methods"는 현대 기계학습과 과학 컴퓨팅의 근간입니다. 수반 방법의 $$O(P)$$ 무관 복잡도는 대규모 매개변수 공간에서의 최적화를 가능하게 했습니다.[1]

최근 연구(2023-2025)는 이 기초 이론을 다음 방향으로 확장하고 있습니다:
- **정확성**: 수치 오차 감소 (ACA)
- **메모리 효율**: 새로운 미분 모드 탐색 (Moonwalk)
- **자동화**: 복잡한 문제의 자동 미분 생성
- **확장성**: 암묵적 스킴, 다중 물리, 분산 시스템

이러한 발전들은 수반 방법이 **앞으로도 대규모 역설계, 신경망 학습, 과학 계산 최적화의 핵심 도구**로 계속할 것을 시사합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/f631e894-70fc-40ae-bc74-6c9877e99f2b/adjoint.pdf)
[2](http://proceedings.mlr.press/v119/zhuang20a/zhuang20a.pdf)
[3](https://arxiv.org/pdf/2112.14217.pdf)
[4](https://openreview.net/forum?id=97dJ3Jp5P4)
[5](https://hess.copernicus.org/articles/28/3051/2024/)
[6](https://arxiv.org/html/2506.14792v1)
[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC4772124/)
[8](https://www.ijcai.org/Proceedings/16/Papers/211.pdf)
