# Representational Power of Restricted Boltzmann Machines and Deep Belief Networks

### 1. 핵심 주장 및 주요 기여

이 논문의 핵심 기여는 **제한된 볼츠만 머신(RBM)과 심층 신념 네트워크(DBN)의 표현 능력에 대한 이론적 증명**에 있습니다. 저자들이 증명한 세 가지 주요 정리는 다음과 같습니다:[1]

**정리 1: 숨겨진 유닛 증가의 순효과(Theorem 2.3)**
RBM에 숨겨진 유닛을 추가하면 데이터를 완벽하게 모델링하지 않는 한 항상 모델링 성능이 엄격하게 개선됩니다. 이는 추가된 유닛에 대해 가중치 벡터 $$w$$와 편향 $$c$$을 적절히 설정할 수 있으며, KL 발산(KL divergence)을 감소시킬 수 있음을 의미합니다.[1]

**정리 2: 범용 근사 성질(Theorem 2.4)**
RBM은 이산 분포(discrete distribution)의 범용 근사기(universal approximator)입니다. 보다 구체적으로, 모든 이산 분포를 $$k + 1$$개의 숨겨진 유닛으로 임의의 정확도로 근사할 수 있습니다. 여기서 $$k$$는 확률이 0이 아닌 입력 벡터의 개수입니다.[1]

**정리 3: DBN의 표현 한계(Proposition 3.1)**
흥미롭게도, 2층 DBN의 경우, 최적의 상층 RBM을 사용하더라도 달성 가능한 최선의 성능은 $$\text{KL}(p_0 \| p_1)$$로 제한됩니다.[1] 여기서 $$p_0$$는 경험적 데이터 분포, $$p_1$$은 첫 번째 층 RBM을 한 번 통과한 후의 분포입니다.

***

### 2. 문제 정의 및 해결 방법

**해결하고자 하는 문제:**

논문은 두 가지 중요한 질문에 답하고자 합니다:[1]

1. RBM이 충분한 숨겨진 유닛을 가질 때 임의의 분포를 모델링할 수 있는가?
2. DBN에 더 많은 층을 추가하면 표현 능력이 엄격하게 향상되는가?

배경으로는, 두 층 신경망(예: 하나의 숨겨진 층을 가진 신경망)이 모든 함수를 표현할 수 있다는 **범용 근사 정리**가 존재했으나, 이러한 얕은 네트워크는 $$\text{O}(2^d)$$개의 요소가 필요할 수 있습니다. 반면 깊은 아키텍처는 계산 이론상 훨씬 효율적입니다.[1]

**제안하는 방법:**

**(1) 숨겨진 유닛 추가의 순효과 증명 (Theorem 2.3)**

경험적 분포 $$p_0$$와 현재 RBM의 분포 $$p$$에 대해, $$\text{KL}(p_0 \| p) > 0$$이면 다음과 같은 $$w, c$$가 존재합니다:

$$
\text{KL}(p_0 \| p_{w,c}) < \text{KL}(p_0 \| p)
$$

여기서 $$p_{w,c}$$는 새로운 유닛 추가 후 분포입니다. 증명에서는 보조정리(Lemma 2.1)를 통해 추가된 유닛이 동등 클래스(equivalence class)를 보존함을 보입니다.[1]

**(2) 범용 근사 성질 증명 (Theorem 2.4)**

구성적 증명 방식을 사용합니다:[1]

- 확률이 0이 아닌 입력 벡터 개수를 $$k$$라 하면, $$k+1$$개 숨겨진 유닛으로 충분합니다.
- 각 숨겨진 유닛을 하나의 입력 벡터에 할당합니다.
- 적절한 가중치를 설정하여 $$v^{(i)}$$일 때 해당 유닛의 활성 확률이 $$\text{sigm}(\lambda_i)$$가 되도록 조정합니다.

**RBM의 에너지 함수와 확률 표현:**

바이너리 RBM은 다음 에너지 함수로 정의됩니다:[1]

$$
E(v, h) = -h^T W v - b^T v - c^T h
$$

여기서:
- $$v \in \{0,1\}^d$$: 가시 유닛(visible units)
- $$h \in \{0,1\}^n$$: 숨겨진 유닛
- $$W$$: 가중치 행렬, $$b, c$$: 편향

결합 분포는:

$$
p(v,h) \propto \exp(-E(v,h)) = \exp(h^T W v + b^T v + c^T h)
$$

조건부 분포는 인수분해 형태:[1]

$$
P(v|h) = \prod_j P(v_j|h), \quad P(v_j=1|h) = \text{sigm}(b_j + \sum_i W_{ij}h_i)
$$

**(3) Contrastive Divergence 학습 방법**

정확한 최대 우도 추정은 계산 불가능하므로, 근사적 기울기를 사용합니다:[1]

$$
\frac{\partial \log p(v,h)}{\partial \theta} = -\left\langle \frac{\partial E(v,h)}{\partial \theta} \right\rangle_0 + \left\langle \frac{\partial E(v,h)}{\partial \theta} \right\rangle_\infty
$$

여기서 $$\langle \cdot \rangle_0$$은 데이터 분포에 대한 평균, $$\langle \cdot \rangle_\infty$$는 모델 분포에 대한 평균입니다. Contrastive divergence는 $$\langle \cdot \rangle_\infty$$를 $$\langle \cdot \rangle_k$$ (보통 k=1)로 근사합니다.

**가중치에 대한 Contrastive Divergence 기울기 추정기:**[1]

$$
\nabla W_{ij} \approx P(h_i=1|v)v_j - P(h_i=1|v')v'_j
$$

여기서 $$v'$$는 한 번의 Gibbs 샘플링 후 복원된 입력입니다.

***

### 3. 모델 구조 및 DBN의 구조적 특성

**RBM 구조:**

RBM은 이분 그래프(bipartite graph)로, 가시 층과 숨겨진 층 내에서는 연결이 없습니다.[1] 이는 $$P(h|v)$$와 $$P(v|h)$$를 효율적으로 계산할 수 있게 합니다.

**Deep Belief Network 구조:**

DBN은 다수의 RBM을 계층적으로 쌓은 구조입니다:[1]

$$
p(v, h^{(1)}, h^{(2)}, \ldots, h^{(\ell)}) = P(v|h^{(1)})P(h^{(1)}|h^{(2)}) \cdots P(h^{(\ell-2)}|h^{(\ell-1)})p(h^{(\ell-1)}, h^{(\ell)})
$$

각 층에 대해:

$$
P(h^{(k)}|h^{(k+1)}) = \prod_i P(h_i^{(k)}|h^{(k+1)}), \quad P(h_i^{(k)}=1|h^{(k+1)}) = \text{sigm}\left(b_i^{(k)} + \sum_j W_{ij}^{(k)} h_j^{(k+1)}\right)
$$

상층 RBM만 무방향 연결을 가지고, 나머지는 방향성 연결입니다.

**탐욕 층별 학습(Greedy Layer-wise Learning):**

각 층을 순차적으로 학습합니다. $$k$$번째 층 학습 시, 하층은 고정되고 상층만 RBM으로 학습됩니다. 이 때 최적화되는 것은 변분 경계(variational bound)입니다:[1]

$$
\log p(v) \geq \sum_{h^{(1)}} Q(h^{(1)}|v) \left[\log p(h^{(1)}) + \log P(v|h^{(1)})\right] - \sum_{h^{(1)}} Q(h^{(1)}|v) \log Q(h^{(1)}|v)
$$

***

### 4. 일반화 성능 향상 가능성 (핵심 주제)

**4.1 변분 경계의 한계**

논문의 가장 흥미로운 발견은 DBN의 계층 추가의 효과에 대한 것입니다. 2층 DBN에서, 상층 RBM이 최적의 분포 $$p^*(h^{(1)})$$를 달성하더라도:[1]

$$
p^*(h^{(1)}) = \sum_v p_0(v)Q(h^{(1)}|v)
$$

이 경우 달성 가능한 최선의 KL 발산은 다음과 같이 제한됩니다:[1]

$$
\text{KL}(p_0 \| p_{DBN}) \geq \text{KL}(p_0 \| p_1)
$$

여기서 $$p_1$$은 첫 번째 RBM에서 한 번의 "상향-하향(up-down)" Gibbs 샘플링 후의 분포입니다.

**4.2 실험적 검증**

논문은 10개 원소 이진 벡터 데이터셋에서 실험을 수행합니다:[1]
- 60개 가능한 예제 중 40개를 학습셋으로 사용
- 첫 번째 RBM: 5개 바이너리 숨겨진 유닛
- 두 번째 RBM: 10개 숨겨진 유닛

결과는 **KL(p₀||p₁) 기준으로 학습한 경우가 Contrastive Divergence 기준보다 우수함**을 보여줍니다:[1] 테스트 세트에서 KL 발산이 약 0.4로 제한되는 반면, CD 기준은 약 1.1을 달성합니다.

**4.3 일반화 성능 개선 메커니즘**

계층 추가가 여전히 유용한 이유:[1]

1. **정규화 효과**: 더 깊은 네트워크는 더 압축된 표현을 학습하여 과적합을 줄일 수 있습니다.

2. **복합 표현 학습**: 상층의 분포가 제한되지 않으므로, 첫 번째 층이 매우 고용량이어도 상층에서 더 추상적인 특징을 추출할 수 있습니다.

3. **비탐욕 학습 기준**: 저자들은 KL(p₀||p₁)을 근사하는 트랙터블한 방법을 제안합니다:

$$
\text{KL}(p_0 \| p_1) = \sum_{i=1}^N \frac{1}{N} \log \sum_{j=1}^N \frac{1}{N} \hat{P}(V^1 = v_i | V^0 = v_j)
$$

여기서 $$\hat{P}(V^1 = v_i | V^0 = v_j)$$는 $$v_j$$에서 시작하여 한 번의 Gibbs 샘플링 후 $$v_i$$를 관찰할 확률의 추정치입니다.

***

### 5. 모델의 한계

논문이 명시적으로 지적하는 한계:[1]

**이론적 한계:**

1. **구성적 증명의 비실용성**: 범용 근사성은 구성적으로 증명되지만, 실제 구현은 $$2^d$$ 개 정도의 숨겨진 유닛이 필요할 수 있습니다 (여기서 $$d$$는 입력 차원).

2. **연속 변수 확장 부재**: 논문의 모든 증명은 이산 입력에 제한됩니다.

3. **개방된 DBN 표현 능력 질문**: DBN 계층 수 증가에 따른 표현 능력의 엄격한 증가 여부 ($$D_\ell^n \subset D_{\ell+1}^n$$)는 여전히 미해결입니다.

**실무적 한계:**

1. **Contrastive Divergence의 근사 오차**: CD-1 (k=1)은 $$\langle \cdot \rangle_\infty$$를 $$\langle \cdot \rangle_1$$로만 근사하므로 바이어스가 존재합니다.

2. **계층 수 증가의 불명확한 이점**: 논문의 결과는 "왜 계층을 추가해야 하는가"에 대한 명확한 답을 제공하지 않습니다.

3. **계산 복잡도**: KL(p₀||p₁) 정확 계산은 $$\mathcal{O}(N^2)$$ 복잡도를 가지므로 대규모 데이터셋에 부적합합니다.

***

### 6. 앞으로의 연구 영향 및 고려사항

**6.1 이 논문의 이론적 영향**

이 논문은 심층 학습의 기초적 이론 구축에 중요한 역할을 했습니다:[1]

1. **깊은 아키텍처의 효율성 강화**: 비록 RBM이 범용 근사기임을 보였지만, 실질적으로는 깊은 구조가 샘플 효율성 측면에서 훨씬 우수함을 암시합니다.

2. **DBN 학습 알고리즘 개선**: KL(p₀||p₁) 기준의 도입은 더 나은 학습 기준을 제시합니다.

3. **표현 능력 분석의 원형**: 이후 다양한 신경망 구조의 표현 능력 분석에 영향을 미쳤습니다.

**6.2 최신 연구 기반의 발전 방향**

최근 연구들은 다음과 같은 방향으로 진행되고 있습니다:[2][3][4][5]

**일반화 성능 측정:**
최신 연구는 심층 네트워크의 일반화 성능을 보다 정량적으로 평가하는 메트릭을 개발하고 있습니다. 2024년 연구에 따르면, 심층 네트워크의 일반화 용량은 **분류 정확도뿐만 아니라 미숙 데이터의 다양성**에도 의존합니다.[3]

**표현 학습과 깊이의 관계:**
2025년 통계 물리학 기반 연구는 심층 신경망의 일반화 경계가 **전체 파라미터 개수가 아닌 마지막 층의 크기**에만 의존함을 보여줍니다. 이는 깊은 아키텍처의 본질적 효율성을 강화합니다.[6]

**RBM과 DBN의 현재 응용:**

1. **생물 정보학 응용**: RBM은 단백질 패밀리, 신경 데이터, 면역학적 데이터 분석에 활용되고 있습니다. 특히 T세포 수용체 분석과 항원 제시 예측에서 높은 정확도를 달성합니다.[7]

2. **환경 모니터링**: 앙상블 DBN 방법은 오염물질 배출량 예측에서 표준 방법(BP, SVM)보다 우수한 일반화 성능을 보여줍니다.[8]

3. **신경영상 분석**: RBM은 fMRI 데이터에서 내재적 신경망을 식별하는 데 ICA와 동등하거나 우수한 성능을 보입니다.[9]

**6.3 향후 연구 시 고려할 점**

**이론적 측면:**

1. **깊이에 따른 표현 능력의 엄격한 증가 증명**: Dₙₗ ⊂ Dₙₗ₊₁ 여부와 Dₙ∞의 특성 규명이 필요합니다.[1]

2. **연속 변수 확장**: 실제 응용에서는 연속 값을 다루므로, 가우시안 또는 지수 RBM에 대한 표현 능력 분석이 필수적입니다.

3. **최적 학습 기준 개발**: CD 기준 대신 KL(p₀||p₁)을 효율적으로 근사하는 트랙터블한 방법 개발이 필요합니다.

**실무적 측면:**

1. **현대 심층 학습과의 통합**: Transformer, CNN 등 현대 아키텍처와 결합하여 RBM/DBN의 가치를 재평가할 필요가 있습니다.

2. **일반화 성능의 실증적 검증**: 이론과 실무 간의 갭을 좁히기 위해 대규모 데이터셋에서 DBN의 일반화 성능을 체계적으로 평가해야 합니다.

3. **정규화 메커니즘 규명**: 깊은 네트워크가 어떻게 정규화 효과를 달성하는지에 대한 명확한 이해가 필요합니다.

4. **하이퍼파라미터 최적화**: 계층 수, 유닛 수, 학습률 등의 상호작용을 보다 깊이 있게 분석해야 합니다.

***

## 결론

Le Roux와 Bengio의 논문은 **RBM의 범용 근사 성질과 DBN 학습의 이론적 한계를 명확히 제시**함으로써, 심층 신경망 이론의 기초를 마련했습니다. 특히 KL(p₀||p₁) 기준의 도입은 단순한 탐욕 학습을 넘어 더 정교한 학습 알고리즘 개발을 촉발했습니다. 

최근 연구들은 이 이론적 토대 위에서 **일반화 성능의 정량적 측정, 표현 학습의 깊이별 분석, 실제 응용 분야에서의 DBN 확장** 등으로 진화하고 있습니다. 앞으로의 연구는 이론과 실무 간의 갭을 좁히면서도, 현대 심층 학습 아키텍처 패러다임에 이 기초 이론을 어떻게 통합할 것인가에 초점을 맞춰야 할 것입 것입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/2ed6e816-83ac-4e8b-a11e-2dd0e8d9d2e3/representational_power.pdf)
[2](http://arxiv.org/pdf/2403.16768.pdf)
[3](https://arxiv.org/html/2409.01498)
[4](https://pmc.ncbi.nlm.nih.gov/articles/PMC11928604/)
[5](https://arxiv.org/pdf/1710.05468.pdf)
[6](http://arxiv.org/pdf/2501.19281.pdf)
[7](https://arxiv.org/html/2501.04387v1)
[8](https://pubs.acs.org/doi/10.1021/acsomega.0c06317)
[9](https://pmc.ncbi.nlm.nih.gov/articles/PMC4348021/)
[10](https://arxiv.org/pdf/1912.02178.pdf)
[11](https://arxiv.org/pdf/1903.04659.pdf)
[12](https://arxiv.org/pdf/1807.04587.pdf)
[13](https://proceedings.mlr.press/v202/marwah23a.html)
[14](https://www.semanticscholar.org/paper/Deep-belief-networks-Hinton/79dc4da8f131ff26046d5564e5dedb1c5ce72c6a)
[15](https://iclr.cc/virtual/2023/session/13348)
[16](https://www.sciencedirect.com/science/article/abs/pii/S0950705120303154)
[17](https://www.sciencedirect.com/science/article/abs/pii/S0925231217315849)
[18](https://pubmed.ncbi.nlm.nih.gov/40660356/)
[19](https://www.nature.com/articles/s41598-025-93005-5)
