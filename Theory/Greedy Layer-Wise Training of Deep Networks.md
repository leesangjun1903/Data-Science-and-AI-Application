# Greedy Layer-Wise Training of Deep Networks

### 1. 핵심 주장과 주요 기여

본 논문(NIPS 2006)의 핵심 주장은 **깊은 신경망 학습의 어려움을 그리디 레이어별 비지도학습 사전학습(greedy layer-wise unsupervised pre-training)으로 해결할 수 있다**는 것이다. 저자들은 Deep Belief Networks(DBN)를 기반으로 심층 신경망 최적화 문제를 체계적으로 연구하였다.[1]

주요 기여는 세 가지이다:[1]

1. **연속값 입력 처리의 확장**: RBM과 DBN을 수정하여 연속값 입력(continuous-valued inputs)을 자연스럽게 처리하는 방법 제시

2. **최적화 메커니즘의 규명**: 그리디 레이어별 비지도학습 전략이 심층 네트워크의 최적화를 돕는다는 가설 검증

3. **"협력적이지 않은" 입력분포 처리**: 입력분포의 구조가 목표변수를 충분히 나타내지 못할 때 부분지도학습(partial supervision)을 통한 해결책 제시

### 2. 해결하고자 하는 문제와 제안 방법

**핵심 문제**[1]

기존 심층 신경망은 경사하강법(gradient-based optimization)을 통해 학습할 때 부실한 국소최솟값(poor local minimum)에 빠져 성능이 저조했다. 이는 복잡한 함수의 학습, 특히 "고차 변동 함수(highly-varying functions)"의 표현 어려움과 관련이 있다.[1]

**제안 방법: 그리디 레이어별 사전학습**[1]

논문에서 제시하는 알고리즘은 다음과 같이 구성된다:

**Restricted Boltzmann Machine (RBM) 기반**

RBM의 에너지 함수는 다음과 같다:[1]

$$
\text{energy}(v, h) = -h'Wv - b'v - c'h
$$

여기서 $$v$$는 가시 유닛(visible units), $$h$$는 은닉 유닛(hidden units), $$W$$는 가중치 행렬, $$b$$, $$c$$는 편향 벡터다.

**Contrastive Divergence를 이용한 업데이트**

RBM 파라미터의 로그우도(log-likelihood) 그래디언트는 다음과 같다:[1]

$$
\frac{\partial \log P(v_0)}{\partial \theta} = -\sum_{h_0} Q(h_0|v_0)\frac{\partial \text{energy}(v_0, h_0)}{\partial \theta} + \sum_{v_k,h_k} P(v_k, h_k)\frac{\partial \text{energy}(v_k, h_k)}{\partial \theta}
$$

Contrastive Divergence 알고리즘은 $$k$$를 작은 값(보통 1)으로 제한하여 계산을 효율화한다.

**Deep Belief Networks의 레이어별 학습**

DBN은 다음과 같은 결합분포를 가진다:[1]

$$
P(x, g_1, g_2, \ldots, g_\ell) = P(x|g_1)P(g_1|g_2) \cdots P(g_{\ell-2}|g_{\ell-1})P(g_{\ell-1}, g_\ell)
$$

각 $$P(g_i|g_{i+1})$$는 인수분해된 조건부 분포이며, 각 유닛 $$j$$에 대해:[1]

$$
P(g_j^i = 1|g^{i+1}) = \text{sigm}\left(b_j^i + \sum_{k=1}^{n_{i+1}} W_{kj}^i g_k^{i+1}\right)
$$

여기서 $$\text{sigm}(t) = 1/(1 + e^{-t})$$는 시그모이드 함수다.

**연속값 입력 처리: Gaussian 유닛**

논문은 연속값 입력을 더 적절히 처리하기 위해 Gaussian 유닛을 도입했다. 에너지 함수에 이차항을 추가하면:[1]

$$
\text{energy}(v, h) = -h'Wv - b'v - c'h + \sum_i d_i^2 v_i^2
$$

이 경우 $$v_i$$의 조건부 기댓값은:[1]

$$
E[v_i|h] = \frac{a(h)}{2d_i^2}
$$

**모델 구조**

전체 학습 절차는 다음 단계로 진행된다:[1]

1. 첫 번째 RBM을 입력 데이터에 대해 학습
2. 학습된 RBM의 포스테리어 $$Q(g_1|x)$$를 통해 데이터 표현 변환
3. 변환된 표현에서 다음 레이어의 RBM 학습 반복
4. 전체 네트워크를 지도학습 미세조정(supervised fine-tuning)으로 최적화

### 3. 성능 향상 및 실험 결과

**Experiment 1: 연속값 입력 처리**[1]

Abalone 데이터셋과 Cotton 선물 데이터에서 Gaussian 입력 단위를 사용한 DBN이 이진 입력을 사용한 경우보다 현저히 향상되었다:

- Abalone (MSE): 4.23 → 4.23 (부분지도 Gaussian)
- Cotton (분류오류): 45.0% → 31.4% (부분지도 Gaussian)

**Experiment 2: MNIST 분류**[1]

네트워크 구조: 784 입력 → 3개 은닉층(500-1000 유닛) → 10 출력

| 방법 | 테스트 오류 |
|------|-----------|
| DBN 비지도 사전학습 | 1.2% |
| 자동인코더 사전학습 | 1.4% |
| 지도 그리디 사전학습 | 2.0% |
| 사전학습 없음 (깊은망) | 2.4% |
| 사전학습 없음 (얕은망) | 1.9% |

**Experiment 3: 상위 은닉층 제약**[1]

상위 은닉층을 20개 유닛으로 제한했을 때:

- 사전학습 있음: 1.5% 테스트 오류
- 사전학습 없음: 5.0% 테스트 오류

이 결과는 사전학습이 표현 학습의 질에 주요 영향을 미친다는 가설을 강력히 지지한다.

### 4. 일반화 성능 향상 메커니즘

**핵심 가설**[1]

논문이 제시하는 일반화 성능 향상의 주요 메커니즘은 다음과 같다:

1. **더 나은 초기화**: 비지도 사전학습이 가중치를 좋은 국소최솟값 근처로 초기화
2. **고수준 추상화 표현**: 상위 레이어가 입력의 의미 있는 고차 추상화(high-level abstractions)를 학습
3. **정보 보존**: 각 레이어를 입력 정보 모델링에 최적화함으로써 중요한 정보 손실 최소화

**실험적 증거**[1]

Experiment 3의 핵심 발견: 상위 층이 작을 때(20 유닛), 사전학습이 없는 깊은 네트워크는 훈련 오류가 현저히 증가했다. 이는 다음을 시사한다:

> "사전학습 없이 하위 레이어들은 훈련 집합을 맞출 수 있을 정도로 충분한 정보를 상위 두 계층(완전히 연결된 얕은 네트워크)에 전달하지만, 일반화를 돕는 의미 있는 표현은 학습하지 못한다."[1]

### 5. 한계 및 도전과제

**협력적이지 않은 입력분포 문제**[1]

비지도 사전학습이 효과적이려면 입력분포 $$p(x)$$의 구조가 목표변수 $$y$$를 충분히 나타내야 한다. 예를 들어, $$y = \sin(x) + \text{noise}$$ 형태의 회귀 문제에서 입력분포가 $$p(x)$$와 관계없으면 순수 비지도 학습은 도움이 되지 않는다.[1]

**해결책: 부분지도학습**[1]

첫 번째 레이어를 다음과 같이 혼합 기준으로 학습:

```math
\mathcal{L} = \lambda_{\text{unsup}} \cdot \mathcal{L}_{\text{unsup}} + \lambda_{\text{sup}} \cdot \mathcal{L}_{\text{sup}}
```

여기서 $$\mathcal{L}\_{\text{unsup}}$$는 입력 재구성 오류, $$\mathcal{L}_{\text{sup}}$$는 예측 오류다.[1]

### 6. 이후 연구에 미친 영향과 고려사항

**획기적 영향**

이 논문은 **깊은 학습(deep learning)의 기초**를 마련했다:[1]

- 심층 신경망 학습의 실질적 해결책 제시
- 비지도 사전학습이라는 새로운 패러다임 개척
- 2012년 AlexNet, 이후 현대 딥러닝 혁명의 이론적 토대 제공

**향후 연구 시 고려사항**

1. **하이퍼파라미터 최적화의 중요성**: 각 레이어의 훈련 반복 횟수, 학습률, 레이어 크기 결정이 성능에 큰 영향[1]

2. **아키텍처 설계**: 레이어 수와 각 레이어의 유닛 수를 신중히 선택 필요[1]

3. **학습 속도 조정**: 지도학습 미세조정 시 비지도 학습 시 대비 20배 큰 학습률 사용[1]

4. **연속 레이어 학습 전략**: 각 레이어를 순차적으로 추가하지 않고 모든 레이어를 동시에 학습할 수 있는 변형 알고리즘 가능성[1]

5. **작업 특성에 맞는 적응**: 입력분포와 목표의 관계를 사전에 분석하여 순수 비지도 vs. 부분지도 학습 선택[1]

6. **계산 효율성**: 실무 적용 시 GPU 기반 병렬 처리와 메모리 최적화 필수 고려[1]

이 논문은 단순한 알고리즘 제시를 넘어, **왜 깊은 네트워크가 중요한가**라는 이론적 토대를 제공하며, 현대 딥러닝의 중심 개념들(표현 학습, 계층적 추상화, 사전학습)의 원점이 되었다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/300eee82-b4ec-46f3-a1e7-6ca3a1bd64f7/NIPS-2006-greedy-layer-wise-training-of-deep-networks-Paper.pdf)
