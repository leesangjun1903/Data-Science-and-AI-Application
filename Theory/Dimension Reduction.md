# Dimension Reduction

## 핵심 주장과 주요 기여[1]

Padraig Cunningham의 이 기술 보고서는 **고차원 데이터 분석의 효율성과 정확도를 동시에 개선할 수 있는 차원 축소 기법들의 체계적 분류 및 설명**을 핵심 주장으로 제시합니다. 주요 기여는 다음과 같습니다:

**첫째, 차원 축소 기법의 이원적 분류**: 논문은 차원 축소 기법을 **지도 학습(Supervised)과 비지도 학습(Unsupervised), 그리고 특성 선택(Feature Selection)과 특성 변환(Feature Transformation)** 두 차원으로 분류하여 명확한 프레임워크를 제시합니다.[1]

**둘째, 실무 중심의 포괄적 개요**: PCA, LDA, LSA 등 고전 기법부터 최신 비지도 특성 선택 기법(Laplacian Score, Category Utility, Q-α 알고리즘)까지 대표적인 기법들을 상세하게 설명합니다.[1]

**셋째, 실제 문제 해결 관점**: "Big p Small n 문제"와 "차원의 저주(Curse of Dimensionality)"라는 실제 데이터 분석의 핵심 문제를 명확하게 정의하고 이를 해결하는 방법론을 제시합니다.[1]

---

## 해결하고자 하는 문제

### 근본 문제[1]

논문이 다루는 핵심 문제는 다음과 같습니다:

- **Big p Small n 문제**: 데이터의 특성(feature) 개수 p가 샘플 개수 n을 크게 초과하는 상황
- **차원의 저주**: 특성 개수가 증가함에 따라 기계학습 알고리즘의 성능이 저하되는 현상
- **고차원 데이터의 희소성(Sparsity)**: 많은 특성 중 실제로 예측에 유용한 특성은 극히 제한적
- **계산 효율성**: 고차원 데이터 처리 시 훈련 및 분류 시간의 증가
- **노이즈 및 불필요한 특성**: 무관한 특성이 예측 특성과 동일한 영향을 미치는 문제

***

## 제안 방법과 수식

### 1. 특성 변환(Feature Transformation)

#### 주성분 분석(PCA)[1]

**기본 원리**: 데이터를 새로운 차원 공간으로 변환하되, 첫 번째 변환 특성이 최대 분산을 포함하도록 합니다.

선형 변환:
$$x'_i = Wx_i$$

공분산 행렬의 대각화를 통해:
$$C_{PCA} = \frac{1}{n-1}YY^T$$

여기서 $$Y = PX$$이고, P의 행들은 $$XX^T$$의 고유벡터입니다. k개의 주성분만 유지하여 차원 축소를 달성합니다.[1]

#### 잠재 의미 분석(LSA)[1]

특히 텍스트 분석에 사용되며, 특이값 분해(SVD)를 기반으로 합니다:

$$X = TSV^T$$

여기서:
- $$T_{p×m}$$: $$XX^T$$의 고유벡터 행렬
- $$S_{m×m}$$: 고유값의 제곱근을 포함하는 대각 행렬  
- $$V_{n×m}$$: $$X^TX$$의 고유벡터 행렬

k개의 특이값만 유지:
$$\hat{X} = T'S'V'^T$$

새로운 쿼리의 변환:
$$q' = S'^{-1}T'^Tq$$

***

#### 선형 판별 분석(LDA)[1]

PCA와 달리, LDA는 **클래스 간 분리를 최대화**하며, 클래스 레이블을 활용합니다.

**클래스 간 분산 행렬**:
$$S_B = \sum_{c \in C} n_c(\mu_c - \mu)(\mu_c - \mu)^T$$

**클래스 내 분산 행렬**:
$$S_W = \sum_{c \in C} \sum_{j:y_j=c} (x_j - \mu_c)(x_j - \mu_c)^T$$

**Fisher 판별식 최적화**:
$$W_{LDA} = \arg \max_W \frac{|W^TS_BW|}{|W^TS_WW|}$$

또는 동시 대각화를 통한 일반화 고유값 문제:
$$S_Bv_i = \lambda_i S_W v_i$$

***

### 2. 특성 선택(Feature Selection)

#### 지도 학습에서의 특성 선택

**필터 기법(Filter Methods)**

카이제곱 검정(Chi-Square):
$$\chi^2(D, c, f) = \sum_{i=1}^{r} \left[\frac{(n_{i+} - \mu_{i+})^2}{\mu_{i+}} + \frac{(n_{i-} - \mu_{i-})^2}{\mu_{i-}}\right]$$

정보 이득(Information Gain):
$$IG(D, c, f) = \text{Entropy}(D, c) - \sum_{v \in \text{values}(f)} \frac{|D_v|}{|D|} \text{Entropy}(D_v, c)$$

엔트로피:
$$\text{Entropy}(D, c) = -\sum_{i=1}^{r} \left[\frac{n_{i+}}{n_i}\log_2\frac{n_{i+}}{n_i} + \frac{n_{i-}}{n_i}\log_2\frac{n_{i-}}{n_i}\right]$$

오즈비(Odds Ratio):
$$OR(D, c^+, f) = \frac{n_{1+}n_{2-}}{n_{2+}n_{1-}}$$

**래퍼 기법(Wrapper Methods)**

- 전진 선택(Forward Selection): 개별 특성에서 시작하여 가장 개선된 특성을 추가
- 후진 제거(Backward Elimination): 모든 특성에서 시작하여 가장 덜 유용한 특성을 제거
- 유전 알고리즘(GA): 특성 부분집합을 개별체로 표현하여 교차와 돌연변이 적용
- 시뮬레이션 어닐링(SA): 확률적 탐색으로 국소 최적값 회피[1]

#### 비지도 학습에서의 특성 선택[1]

**라플라시안 점수(Laplacian Score)**

근처 이웃 관계를 기반으로 한 그래프 G에서:

$$S_{ij} = e^{-\frac{||x_i - x_j||^2}{t}}$$

라플라시안 $$L = D - S$$ (D는 차수 대각행렬):

정규화된 특성:
$$\tilde{m}_i = m_i - \frac{m_i^T D \mathbf{1}}{\mathbf{1}^T D \mathbf{1}}$$

특성 i의 라플라시안 점수:
$$LS_i = \frac{\tilde{m}_i^T L \tilde{m}_i}{\tilde{m}_i^T D \tilde{m}_i}$$

**카테고리 유틸리티(Category Utility)**

클러스터링 품질 평가:
$$CU(C, F) = \frac{1}{k} \sum_{C_l \in C} \left[\sum_{f_i \in F} \sum_{j=1}^{r_i} p(f_{ij}|C_l)^2 - \sum_{f_i \in F} \sum_{j=1}^{r_i} p(f_{ij})^2\right]$$

**Q-α 알고리즘**

특성 가중치 벡터 α를 통한 관련성 정량화:
$$\text{Rel}(\alpha) = \text{trace}(Q^T A_\alpha^T A_\alpha Q)$$

최적화:
$$\max_{Q,\alpha} \text{trace}(Q^T A_\alpha^T A_\alpha Q) \quad \text{subject to} \quad \alpha^T\alpha = 1, Q^TQ = I$$

여기서 $$A_\alpha = \sum_{i=1}^{p} \alpha_i m_i m_i^T$$[1]

***

## 모델 구조 및 성능 향상

### 기법별 특성 비교[1]

| 기법 | 특성 | 장점 | 단점 |
|------|------|------|------|
| PCA | 비지도 변환 | 계산 효율, 해석 용이 | 선형 변환만 가능, 클래스 정보 미반영 |
| LDA | 지도 변환 | 클래스 분리 최적화 | 작은 n 문제 시 S_W 특이성, 오버피팅 위험 |
| LSA | 비지도 변환 | 의미론적 정보 발굴 | 다른 매체 적용 어려움 |
| 필터 기법 | 특성 선택 | 빠른 계산 | 특성 간 의존성 무시 |
| 래퍼 기법 | 특성 선택 | 분류 알고리즘과 통합 | 지수 복잡도, 오버피팅 위험 |
| 라플라시안 점수 | 비지도 선택 | 위치 보존 원칙 | 무관 특성에 부적절 |

***

## 일반화 성능 향상 가능성[1]

### 1. 오버피팅 문제 및 해결책

**문제점**:
- LDA에서 S_W 행렬의 특이성으로 인한 수치 불안정성
- 래퍼 기법에서 훈련 데이터에 과적합되는 경향
- 더 집약적인 탐색 전략이 오버피팅을 심화시킬 수 있음[1]

**해결 방안**:
- **교차 검증 활용**: 래퍼 기법에서 k-fold 교차 검증으로 일반화 성능 추정
- **조기 중단**: 스토케스틱 탐색에서 조기 중단으로 오버피팅 회피
- **정규화**: 동시 대각화를 통한 수치적 안정성 개선

### 2. 특성 간 종속성 처리

**문제점**: 기본 필터 기법은 특성을 독립적으로 평가하여 중복성 무시[1]

**개선 방안**:
- **상호정보(Mutual Information)를 활용한 고급 필터**: 특성 그룹을 함께 평가
- **래퍼 기법**: 알고리즘이 컨텍스트 내에서 특성 상호작용을 고려
- **임베디드 기법**: 랜덤 포레스트 등 자체적으로 특성 중요도 평가

### 3. 매니폴드 구조 활용

**위치 보존 프로젝션**: 실제 고차원 데이터는 입력 공간의 매니폴드에만 분포하므로, 이 구조를 유지하는 차원 축소가 더 나은 일반화 성능 제공[1]

- **라플라시안 점수**: 인접한 점들이 축소 공간에서도 가깝도록 유지
- **지역적 특성 선택**: 서로 다른 클러스터가 다른 특성 부분집합으로 정의되는 경우 효과적

### 4. 비선형 확장

**커널 PCA**: 커널 함수를 통해 특성 공간에서 비선형 변환 수행[1]

**국소 Fisher 판별 분석(Local FDA)**: 지역적 구조를 고려한 비선형 LDA

이들은 선형 기법의 제약을 극복하여 복잡한 데이터 구조에서 더 나은 일반화 가능

***

## 한계 및 과제

### 1. 비지도 특성 선택의 문제점[1]

**"닭과 달걀" 문제**: 특성 선택과 클러스터링 사이의 순환 종속성
- 서로 다른 특성 부분집합이 서로 다른 타당한 클러스터링 결과 도출
- 최적의 특성 부분집합 결정이 선험적(a priori) 클러스터 구조에 종속

### 2. 클러스터 타당성 평가의 편향[1]

클러스터 검증 지표들의 내재적 편향:
- 작은 클러스터 수 선호
- 균형 잡힌 클러스터 크기 편호
- 고차원 데이터에 대한 부적절한 가정

### 3. 계산 복잡도[1]

- 래퍼 기법의 지수 복잡도: p개 특성에 대해 2^p개 부분집합 탐색
- 큰 규모 데이터에서 실용적 적용 어려움

### 4. 선형성 제약[1]

PCA와 LDA는 선형 변환만 가능하여, 복잡한 비선형 관계를 포착 불가

***

## 앞으로의 연구 방향 및 고려사항

### 1. 고차원 특성 공간에서의 이론적 발전

**필요성**: LDA의 작은 n 문제를 해결하기 위한 이론적 기초 강화 필요
- Kernel LDA, Local FDA 등 비선형 확장의 이론화
- 고차원-소규모 표본(High-dimensional-small-sample) 상황에서의 최적성 보장

### 2. 실제 응용 분야 확대

**멀티미디어 데이터**: 이미지, 비디오, 오디오 등 다양한 매체로의 LSA 적용 확대

**생의학 데이터**: 유전자 발현 데이터, 단백질 수열 분석 등에서의 효율적 차원 축소

### 3. 하이브리드 접근법[1]

- 필터 기법의 효율성과 래퍼 기법의 정확도를 결합한 **임베디드 기법** 개발
- 지도 학습과 비지도 학습의 통합 프레임워크: 반지도 학습 관점에서 부분 레이블 활용

### 4. 특성 간 종속성 모델링

**고급 필터 기법**: 상호정보 기반 특성 상호작용 분석으로 중복성 감소

**그래프 기반 방법**: 특성 간 관계를 그래프로 표현하여 최적 부분집합 결정

### 5. 비지도 특성 선택의 객관적 기준 개발

- 클러스터 타당성 편향 완화
- 다중 시각(multiple viewpoints)에서의 특성 선택 평가
- 불확실성 정량화

### 6. 계산 효율성 개선

**근사 알고리즘**: 정확도와 속도의 트레이드오프 최적화

**병렬 계산**: GPU 기반 대규모 특성 공간 탐색

***

## 결론

이 논문은 차원 축소 분야의 **포괄적 로드맵**을 제시합니다. 연구자들이 새로운 문제에 접근할 때, 지도 vs. 비지도, 특성 변환 vs. 특성 선택이라는 이원적 선택지를 체계적으로 고려할 수 있는 프레임워크를 제공합니다.

**연구 시 고려사항**:

1. **문제 특성 파악**: 데이터 레이블 가용성, 해석가능성 요구, 계산 자원 평가
2. **오버피팅 방지**: 교차 검증, 정규화, 조기 중단 등의 기법 적용
3. **데이터 구조 활용**: 매니폴드 구조, 그래프 기반 근처 이웃 정보 활용
4. **다중 기법 비교**: 단일 기법이 최적이 아닐 수 있으므로, 여러 접근법의 조합 고려
5. **실시간 성능 모니터링**: 훈련과 검증 성능의 갭 모니터링으로 일반화 능력 평가

이러한 고려사항들은 의료 영상 처리나 단백질 예측 등 복잡한 고차원 문제에서 더욱 중요하며, 앞서 언급한 반지도 학습이나 비선형 확장 기법들은 향후 연구의 핵심이 될 것입니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/ed260576-7962-403f-a5ad-e41b892e0bba/Dimension_Reduction.pdf)
