
# Multilayer feedforward networks are universal approximator

## 1. 핵심 주장 및 주요 기여 요약

**Multilayer Feedforward Networks are Universal Approximators** 논문(Hornik, Stinchcombe, White, 1989)의 핵심 주장은 **단 하나의 은닉층을 가진 다층 피드포워드 네트워크가 임의의 Borel 측도 함수를 원하는 정도의 정확도로 근사할 수 있다**는 것입니다.[1]

**주요 기여:**

- 단일 은닉층 네트워크가 범용 근사기(universal approximator)임을 엄밀하게 수학적으로 증명했습니다. 이는 1969년 Minsky-Papert의 2층 퍼셉트론 한계 이후 가능성에 대한 궁극적 해결입니다.[1]

- **임의의 연속 비상수 활성함수**를 사용하는 네트워크가 범용 근사 성질을 가진다는 것을 입증했습니다. 기존의 Kolmogorov 정리와 달리, 특정 활성함수(시그모이드, 쌍곡탄젠트 등)에만 국한되지 않습니다.[1]

- **Stone-Weierstrass 정리**를 신경망 이론에 처음으로 적용하여, 이론적 엄밀성을 제공했습니다.[1]

***

## 2. 문제 정의, 제안 방법, 모델 구조 및 성능

### 2.1 해결하고자 한 문제

논문은 다음의 근본적인 질문을 제기합니다:[1]

> "다층 피드포워드 네트워크의 명백한 성공이 근본적인 근사 능력을 반영하는가, 아니면 선택적 보고의 결과인가?"

1969년 Minsky-Papert의 2층 퍼셉트론 불가능성 증명 이후, 다층 네트워크가 일반적 함수를 근사할 수 있는지에 대한 이론적 근거가 부족했습니다. 응용에서의 경험적 성공과 이론 사이의 간극을 메우는 것이 목표입니다.[1]

### 2.2 제안된 방법 및 수학적 정식화

#### 정의 및 기호 체계

**정의 2.1 (아핀 함수 집합):**

$$
A' = \{A(x) = w \cdot x + b : w, x \in \mathbb{R}', b \in \mathbb{R}\}
$$

여기서 $$w$$는 입력-은닉층 가중치, $$b$$는 편향입니다.[1]

**정의 2.2 (시그마 함수 클래스):**

```math
\Sigma'(G) = \left\{f: \mathbb{R}' \to \mathbb{R} : f(x) = \sum_{q=1}^{Q} \beta_q G(A_q(x)), \beta_q \in \mathbb{R}, A_q \in A'\right\}
```

여기서 $$G(\cdot)$$는 측정 가능 활성함수, $$\beta_q$$는 은닉-출력층 가중치입니다.[1]

**정의 2.3 (스쿼싱 함수):**

함수 $$\Psi: \mathbb{R} \to $$이 스쿼싱 함수이려면:[1]
- 비감소(non-decreasing)
- $$\lim_{\lambda \to +\infty} \Psi(\lambda) = 1$$
- $$\lim_{\lambda \to -\infty} \Psi(\lambda) = 0$$

예시: 로지스틱 함수, 쌍곡탄젠트, Gallant-White의 코사인 스쿼셔[1]

#### 수렴성 정의

**정의 2.6 (조밀성):**

메트릭 공간 $$(X, \rho)$$에서 부분집합 $$S$$가 $$T$$에 $$\rho$$-조밀이려면, 모든 $$\varepsilon > 0$$과 모든 $$t \in T$$에 대해, $$\rho(s, t) < \varepsilon$$인 $$s \in S$$가 존재해야 합니다.[1]

**정의 2.7 (컴팩타에서의 균일 조밀성):**

$$
\rho_K(f, g) = \sup_{x \in K} |f(x) - g(x)|
$$

#### 핵심 정리들

**정리 2.1 (ΣΠ 네트워크 범용 근사):**

$$G$$를 임의의 연속 비상수 함수라 하면, $$\Sigma\Pi'(G)$$는 $$C'$$에서 $$\rho_K$$-조밀입니다.[1]

**증명 개요:** Stone-Weierstrass 정리를 적용합니다. ΣΠ'(G)가 다음을 만족함을 보입니다:
1. $$K$$ 위의 대수(algebra)를 형성
2. 점 분리(point separation) 조건을 만족
3. 어떤 점에서도 소멸하지 않음(vanish at no point)

따라서 Stone-Weierstrass 정리에 의해 균일 조밀성이 성립합니다.[1]

**정리 2.4 (표준 시그마 네트워크의 범용 성질):**

모든 스쿼싱 함수 $$\Psi$$, 모든 차원 $$r$$, 모든 확률측도 $$\mu$$에 대해:
- $$\Sigma'(\Psi)$$는 $$C'$$에서 균일 조밀
- $$\Sigma'(\Psi)$$는 $$M'$$에서 $$\rho_\mu$$-조밀[1]

여기서 $$\rho_\mu$$ 메트릭은:

$$
\rho_\mu(f, g) = \inf\{\varepsilon > 0 : \mu\{x : |f(x) - g(x)| > \varepsilon\} < \varepsilon\}
$$

이는 두 함수가 확률적으로 거의 모든 곳에서 가까움을 의미합니다.[1]

### 2.3 모델 구조

단일 은닉층 네트워크의 계산 구조:

```
입력층 (r개 뉴런)
    ↓
아핀 변환: A_q(x) = w_q · x + b_q
    ↓
활성함수: G(A_q(x))
    ↓
가중 합: β_q G(A_q(x))
    ↓
출력: f(x) = Σ β_q G(A_q(x))
    ↓
출력층 (s개 뉴런)
```

**다중 은닉층 확장:**

**따름정리 2.7**에서 $$l \geq 2$$인 다중 은닉층을 가진 네트워크도 범용 근사기임이 증명됩니다.[1]

활성 규칙:

$$
a_k^i = G_k(A_k(a_{k-1})), \quad i=1,\ldots,q_k; \quad k=1,\ldots,l
$$

여기서:
- $$a_0 = x$$ (입력)
- $$G_{l-1} = \Psi$$ (은닉층 활성함수)
- $$G_l = \text{identity}$$ (출력층)
- $$q_0 = r$$, $$q_l = s$$[1]

### 2.4 성능 향상 관련 주요 결과

**따름정리 2.1 (측도 가능 함수 근사):**

모든 측도 가능 함수 $$g \in M'$$에 대해, 임의의 $$\varepsilon > 0$$에 대해 다음을 만족하는 $$f \in \Sigma'(\Psi)$$와 컴팩트 집합 $$K$$가 존재합니다:[1]
- $$\mu(K) > 1 - \varepsilon$$
- 모든 $$x \in K$$에 대해 $$|f(x) - g(x)| < \varepsilon$$

**따름정리 2.2 (L_p 공간에서의 근사):**

컴팩트 집합 $$K$$에서 $$\mu(K) = 1$$이면, 모든 $$p \in [1, \infty)$$에 대해 $$\Sigma'(\Psi)$$는 $$L_p(\mathbb{R}', \mu)$$에서 $$\rho_p$$-조밀입니다.[1]

**따름정리 2.5 (Boolean 함수 근사):**

모든 Boolean 함수 $$g$$와 모든 $$\varepsilon > 0$$에 대해:

$$
\max_{x \in \{0,1\}'} |g(x) - f(x)| < \varepsilon
$$

를 만족하는 $$f \in \Sigma'(\Psi)$$가 존재합니다.[1]

### 2.5 한계 및 실제 제약사항

**이론적 한계:**[1]

1. **존재성 결과:** 정리는 근사 함수의 존재를 보장하나, 학습 과정을 통해 이를 찾을 수 있는지는 보장하지 않습니다.

2. **필요한 은닉 유닛 수:** 정리는 "충분히 많은 은닉 유닛이 필요하다"고만 명시하며, 구체적인 개수를 제시하지 않습니다.

3. **컴팩트 영역 요구:** 기본 결과는 컴팩트 입력 공간에서만 성립합니다.

4. **측도론적 등가성:** 측도 $$\mu$$ 관점에서의 근사는 확률 1인 집합을 제외한 곳에서의 차이를 허용합니다.

***

## 3. 일반화 성능 향상 가능성

### 3.1 논문에서의 일반화 논의

**학습 이론과의 연결:**[1]

논문의 Section 3에서 저자들은 다음을 언급합니다:

> "결과는 다층 피드포워드 네트워크가 연결 강도를 학습할 수 있음을 확립하는 기초를 제공한다. 'sieve의 방법'(method of sieves)이 이에 특히 적합하다."

이는 White (1988b)의 통계적 일관성(consistency) 결과와 함께, 다음을 보장합니다:
- 훈련 데이터에서의 오류 감소
- 일반화 성능의 개선

**Metric Entropy 활용:**

은닉 유닛 수가 고정된 $$\Sigma'$$ 부분집합의 metric entropy를 통해, 훈련 샘플 수 증가에 따른 은닉 유닛 수의 성장률을 결정할 수 있습니다. 이는 **과적합 회피**를 보장합니다.[1]

### 3.2 현대 연구에서의 일반화 성능 향상 전략

최근 연구들은 Hornik et al.의 범용 근사 정리를 기반으로, 다음과 같은 방식으로 실제 일반화 성능을 개선하고 있습니다:[2][3][4]

**1. 정규화 기법의 통합**

논문 발표 후 35년간 다양한 정규화 방법이 개발되었습니다:[5]

- **드롭아웃(Dropout):** 훈련 중 뉴런을 무작위로 비활성화하여 과적합 감소
- **L1/L2 정규화:** 가중치 크기 제약으로 모델 복잡도 제어
- **배치 정규화:** 계층별 활성화 정규화를 통한 안정화

**2. 깊이와 너비의 최적 균형**

근본적 문제: Hornik et al.은 "충분한 은닉 유닛"의 필요성을 보였으나, 실제 필요 개수는 불명확했습니다.[6]

최근 진전:[6]
- 2018년: Guliyev & Ismailov는 특정 smooth sigmoidal 함수에서 더 적은 유닛으로도 범용 근사 가능함을 증명
- 2022년: Shen et al.은 깊고 넓은 ReLU 네트워크에서 필요한 깊이와 너비의 정량적 정보 제공

**3. 아키텍처 혁신**

**정규화 계층의 통합:**[7]

최근 연구는 배치 정규화/레이어 정규화와 선형층 조합만으로도 범용 근사 성질을 유지함을 증명했습니다. 이는 다음 이점을 제공합니다:
- 더 안정적인 훈련
- 더 나은 일반화
- 더 빠른 수렴

**제약 조건 만족 네트워크(Hard-Constrained Networks):**[8]

물리적 법칙이나 안전 제약을 만족하면서도 범용 근사성을 유지하는 구조가 개발됨:

$$
\mathcal{P}(f_\theta): \mathcal{X} \to \mathbb{R}^{n_{\text{out}}}
$$

여기서 $$\mathcal{P}$$는 미분 가능한 사영(projection) 계층입니다.

**4. 비컴팩트 영역으로의 확장**

원래 정리의 한계: 컴팩트 입력 공간만 다룸[1]

최근 성과:[9]
- 무한대에서 소멸하는 연속 함수($$f(x) \to 0$$ as $$\|x\| \to \infty$$)는 무한 $$\mathbb{R}^n$$에서도 균일 근사 가능
- 이는 물리 문제, 신호 처리 등 실제 응용에서 더 광범위한 적용을 가능하게 함

### 3.3 일반화 실패의 실제 원인과 해결책

논문은 다음을 명시합니다:[1]

> "응용 실패는 부적절한 학습, 불충분한 은닉 유닛, 또는 입력과 목표 간의 확률적(비결정론적) 관계로 인한 것"

**현대적 해석과 해결책:**[10][5]

| 실패 원인 | 이론적 배경 | 현대적 해결 전략 |
|---------|-----------|----------------|
| 과적합(Overfitting) | 범용 근사 정리는 훈련 집합 근사만 보장 | 드롭아웃, 조기 종료, 교차 검증 |
| 일반화 불충분 | 테스트 집합에 대한 이론적 보장 없음 | 정규화, 데이터 증강, Sieve 방법 |
| 최적화 어려움 | 근사 함수 찾기의 NP-어려움 | Adam, momentum, 학습률 스케줄 |
| 데이터 부족 | 학습 이론: 샘플 복잡도 | 전이 학습, 메타 학습, 생성 모델 |

### 3.4 깊이 vs 너비 트레이드오프

중요한 발견: 같은 표현 능력을 위해 깊고 좁은 네트워크가 얕고 넓은 네트워크보다 훨씬 효율적입니다.[11]

**정량적 이해:**

너비 $$n$$인 깊이 1 네트워크: 
- 대략 $$O(2^n)$$ 개의 "절곡"(bends) 표현 가능

너비 $$n$$인 깊이 $$d$$ 네트워크:
- 대략 $$O(2^{n \cdot d})$$ 개의 절곡 표현 가능

따라서 **깊이-너비 트레이드오프**:
- 깊은 네트워크: 지수적 표현 효율, 더 나은 일반화 경향
- 넓은 네트워크: 더 많은 파라미터, 과적합 위험 증가

***

## 4. 향후 연구에 미치는 영향 및 고려사항

### 4.1 기초 이론 측면에서의 기여와 확장

**대수 구조 확장:**[3][12]

Hornik et al.의 원래 정리는 실수 함수 공간에 제한되었습니다. 최근 연구는 다음으로 확장:

- **벡터 및 초복소수(Hypercomplex)-값 신경망:** 복소수, 사원수(quaternion) 입력 처리 가능[12]
- **위상벡터공간(Topological Vector Space) 입력:** 수열, 행렬, 함수 등 일반 입력 공간[13]

**동적 시스템으로의 확장:**[14]

재귀 신경망(RNN)에 대한 범용 근사 정리가 증명되었습니다:
- 무한 너비 RNN은 컴팩트 영역의 모든 열림 동적 시스템 근사 가능
- 깊고 좁은 RNN의 최소 너비: $$n+1$$ (입력 차원 기반)

**연산자 근사:**[15]

함수를 근사하는 것이 아니라 **비선형 연산자**를 근사하는 DeepONet 개발:
- 미분 방정식 해석기의 학습
- 함수-함수 매핑

### 4.2 실제 응용과의 간극

**논문 발표 이후 남은 근본 문제들:**[15][1]

1. **실현 불가능 성(Realizability Gap)**
   - 정리: 근사 함수가 존재한다
   - 현실: 경사 하강법으로 이를 찾기 어려움
   - 최신 해결책: Neural Tangent Kernel 이론, 특성 학습(feature learning)

2. **샘플 복잡도(Sample Complexity)**
   - 정리: 함수 복잡도에 대한 정보 없음
   - 필요: $$N$$ 샘플로 $$\varepsilon$$ 정확도 달성에 필요한 은닉 유닛 수
   - 최신 이해: Sieve 방법, Metric entropy 기반 분석

3. **스케일링 문제(Curse of Dimensionality)**
   - 정리: 입력 차원 $$r$$에 따른 필요 유닛 수의 성장률 불명확
   - 경험적 관찰: 지수적 성장 경향 (매우 높은 차원에서 비효율적)
   - 대응 방안: 구조화된 네트워크(CNN, Transformer), 명백한 대칭성 활용

### 4.3 현대 심층 학습에서의 의미

**1. 신경망 이론의 기초 석조**

Hornik et al.의 정리는 다음의 이론적 발전을 가능하게 했습니다:[4]

- **신경접선커널(Neural Tangent Kernel, NTK):** 무한 너비 극한에서 신경망을 선형 모델로 분석
- **특성 학습(Feature Learning):** 최대 업데이트 파라미터화(Maximal Update Parameterization)를 통한 비선형 영역
- **Kolmogorov-Arnold 네트워크:** 범용 근사의 새로운 형태

**2. 정규화와 일반화의 통합 이해**

원래 문제: 범용 근사 정리는 훈련 오류만 다룸 → 일반화 격차 불명확

현대 접근:[16][5]
- Vapnik-Chervonenkis (VC) 차원, Rademacher complexity 등을 통한 일반화 경계
- 정규화 기법들의 이론적 정당화

**3. 의료 영상 처리 등 특화 영역으로의 적용**

당신의 연구 분야(뼈 억제/의료 영상 처리)에서:[17]
- 복잡한 비선형 관계를 학습하는 능력 이론적 보장
- 가중치 초기화, 활성함수 선택, 정규화 전략의 지표 제공

***

## 5. 향후 연구 시 핵심 고려사항

### 5.1 이론적 고려사항

**1. 깊이-너비의 효율성 분석**

단순히 "충분한 은닉 유닛"이 아닌:
- 구체적인 문제에 대해 필요한 최소 너비 추정
- 깊이 증가에 따른 근사율의 개선 정량화

**2. 비컴팩트 영역 고려**

의료 영상에서:
- 정규화된 입력(픽셀 값:  등)은 사실상 컴팩트
- 하지만 활성화 함수 계층을 거치며 비컴팩트 중간 표현 발생

**권장사항:** 무한 영역 근사 정리 고려 및 활성함수 선택 신중히

### 5.2 실제 구현 고려사항

**1. 활성함수의 선택**

Hornik et al.: 임의의 연속 비상수 함수 가능

현대 관점:
- **ReLU:** 계산 효율, 그래디언트 소실 문제 완화, 그러나 범용 근사에는 추가 조건
- **GELU, Swish:** 더 부드러운 경계, 더 나은 기울기 흐름
- **의료 영상:** 회색 음영 보존의 필요성 → 구체적 활성함수 선택 중요

**2. 정규화 전략의 통합**

Hornik et al. 이후 35년간의 개발 결과:
- 배치 정규화: 훈련 안정성, 일반화 개선
- 스펙트럼 정규화(Spectoral Normalization): Lipschitz 제약으로 더 나은 일반화

의료 영상 컨텍스트:
- Dropout: 불확실성 정량화 (Bayesian Neural Networks)
- 가중치 감소: 과적합 방지

### 5.3 의료 영상/뼈 억제 연구에 특화된 조언

**1. 표현 용량과 일반화 트레이드오프**

이론: 범용 근사 가능
실제: 뼈 억제는 매우 구체적인 특성(edges, frequencies)을 활용

**권장 사항:**
- 구조화된 아키텍처 (CNN, U-Net) 활용으로 암시적 귀납 편향(inductive bias) 제공
- Hornik et al. 정리는 이론적 보장이지만, 효율성을 위해 도메인 지식 통합 필수

**2. 로컬 특성 학습**

뼈와 폐 조직의 구별:
- 국소 패턴 인식 필요
- Convolutional 구조가 Fully Connected보다 훨씬 효율적 (필요 파라미터 수 대폭 감소)

**3. 데이터 효율성**

범용 근사 정리는 이론적 가능성만 보장:
- 제한된 의료 영상 데이터로는 더 구체화된 모델 필요
- 사전학습(Pre-training) 또는 전이학습(Transfer Learning) 활용

***

## 결론

Hornik et al. (1989)의 "Multilayer Feedforward Networks are Universal Approximators"는 신경망 이론의 기초석입니다. 이 정리는 다층 피드포워드 네트워크가 **원칙적으로** 임의의 함수를 근사할 수 있음을 엄밀하게 입증했습니다.[1]

그러나 이론적 존재성과 실제 학습 가능성 사이의 간극은 35년 동안 많은 후속 연구의 주제가 되었습니다. 현대 심층 학습은 이 정리 위에 구축되면서도, 정규화, 아키텍처 설계, 최적화 알고리즘 개선을 통해 **실제 일반화 성능**을 극적으로 향상시켰습니다.[2][3][4][7][5]

당신의 의료 영상 연구에서는 이 정리의 이론적 보증이 중요한 근거가 되지만, 동시에 구조화된 아키텍처, 정규화 기법, 도메인 특화 설계를 통해 표현 능력과 일반화의 균형을 맞추을 맞추어야 합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/601d605d-574c-42bf-8d3c-03bd8095c47b/Kornick_et_al.pdf)
[2](https://arxiv.org/pdf/1909.13563.pdf)
[3](https://arxiv.org/html/2405.13682v1)
[4](http://arxiv.org/pdf/2408.00082.pdf)
[5](https://www.geeksforgeeks.org/deep-learning/universal-approximation-theorem-for-neural-networks/)
[6](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
[7](https://openreview.net/forum?id=fGflKXfEP1)
[8](https://arxiv.org/html/2410.10807v1)
[9](http://arxiv.org/pdf/2308.03812.pdf)
[10](https://wikidocs.net/178818)
[11](https://arxiv.org/html/2407.12895v1)
[12](https://arxiv.org/html/2401.02277v2)
[13](https://arxiv.org/abs/2409.12913)
[14](https://arxiv.org/pdf/2211.13866.pdf)
[15](https://arxiv.org/pdf/1910.03193.pdf)
[16](https://www.datacamp.com/tutorial/multilayer-perceptrons-in-machine-learning)
[17](https://www.nature.com/articles/s41598-025-89798-0)
[18](http://arxiv.org/pdf/2410.14759.pdf)
[19](https://www.koreascience.kr/article/JAKO201027042828882.pdf)
