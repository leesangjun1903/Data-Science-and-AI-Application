# Polynomial Regression as an Alternative to Neural Nets

## 핵심 주장 및 주요 기여

**"Polynomial Regression as an Alternative to Neural Nets"** 논문의 가장 중요한 핵심 주장은 **신경망(NN)이 본질적으로 다항 회귀(PR) 모델의 변형**이라는 **NN ↔ PR 원칙**입니다. 논문 저자들은 신경망의 "블랙박스" 특성에 대한 의문을 제기하며, 단순한 활성화 함수 분석을 통해 신경망이 사실상 고차 다항식을 수행하고 있음을 수학적으로 보여줍니다.[1]

주요 기여는 다음과 같습니다:[1]

1. **NN ↔ PR 대응 관계**: 신경망의 각 계층에서 특정 다항 회귀 모델과의 대응 관계 존재
2. **다항식 차수의 증가**: 숨겨진 계층을 거칠수록 다항식의 차수가 증가하는 메커니즘 발견
3. **새로운 다중공선성 성질 예측**: 다항 회귀의 성질을 통해 신경망의 숨겨진 다중공선성 문제를 예측하고 실증적으로 확인
4. **실무적 대안 제시**: 신경망 대신 다항 회귀를 사용하여 하이퍼파라미터 선택과 수렴 문제 회피
5. **polyreg 소프트웨어 패키지 개발**: R 기반의 오픈소스 패키지 제공 및 Python 버전 계획

***

## 논문이 해결하는 문제와 제안 방법

### 1. 핵심 문제

신경망의 성공에도 불구하고, 왜 신경망이 작동하는지에 대한 근본적인 이해 부족이 문제입니다. 보편근사 정리(Universal Approximation Theorem)는 통계적 일관성만 보장할 뿐, 신경망의 예외적인 성능을 설명하기에 불충분합니다.[1]

### 2. 제안 방법: NN ↔ PR 원칙

#### 기본 논증 (p=2인 단순 예시)

특징을 $$u$$와 $$v$$로 표기하면, 첫 번째 숨겨진 계층의 입력은 다음 형태입니다:[1]

$$a_{00} + a_{01}u + a_{02}v$$
$$a_{03} + a_{04}u + a_{05}v$$

활성화 함수가 $$a(t) = t^2$$인 경우, 첫 번째 계층의 출력은 $$u$$와 $$v$$의 이차 함수가 되고, 두 번째 계층은 4차, 세 번째는 8차 다항식을 생성합니다.[1]

#### 일반 활성화 함수의 경우

초월함수(예: tanh)는 Taylor 급수로 구현되므로 다항식으로 근사 가능합니다. Stone-Weierstrass 정리에 의해, 콤팩트 집합 위의 모든 연속함수는 다항식으로 균등하게 근사될 수 있습니다.[1]

#### ReLU의 특수성

ReLU $$a(t) = \max(0, t)$$는 Taylor 급수가 없지만, **구간별 다항식(Piecewise Polynomial)**으로 표현됩니다. 따라서 NN ↔ PPR (구간별 다항 회귀)이 성립합니다.[1]

### 3. 모델 구조

신경망은 다음과 같은 계층적 다항식 구조를 가집니다:[1]

| 계층 | 다항식 차수 | 설명 |
|------|-----------|------|
| 입력층 | 1 | 원본 선형 결합 |
| 첫 번째 숨겨진 계층 | ~2 (활성화 함수가 이차인 경우) | 2차 다항식 생성 |
| 두 번째 숨겨진 계층 | ~4 | 4차 다항식 생성 |
| n번째 숨겨진 계층 | ~2^n | 지수적 증가 |

---

## 성능 향상 및 한계

### 1. 성능 향상 측면

논문의 실증 실험 결과는 다항 회귀가 신경망과 동등하거나 우수한 성능을 보임을 시연합니다:[1]

| 데이터셋 | PR 성능 | NN 성능 | 결과 |
|---------|--------|--------|------|
| Concrete Strength | 0.869 | 0.546-0.608 | PR 우수 |
| Letter Recognition | 0.9030 (도수 2) | 0.4484-0.7825 | PR 우수 |
| MOOCs | 0.9871 | 0.9712-0.9747 | PR 우수 |
| Kidney Cancer | 0.8288 | 0.5387-0.7170 | PR 우수 |
| MNIST (50 PCs) | 0.985 | 이와 유사 | 동등 수준 |

성능 향상의 이유는 신경망이 과도하게 매개변수화되는 경향이 있기 때문입니다. 정규화(Regularization)가 많은 가중치를 0으로 설정하여 실제로는 더 낮은 차수의 다항식을 학습하게 됩니다.[1]

### 2. 다중공선성 문제와 일반화 성능

#### 발견사항

신경망의 다중공선성은 계층을 거치며 증가합니다. MNIST 데이터로 실험한 결과:[1]

**표 1: 10개 단위 모델의 VIF 결과**[1]

| 계층 | VIF > 10인 계수 비율(%) | 평균 VIF |
|------|----------------------|----------|
| dense_1 | 0 | 3.43 |
| dense_2 | 0.7 | 14.96 |
| dense_3 | 1 | 1.578 × 10^13 |

**표 2: 128 단위 모델의 VIF 결과**[1]

| 계층 | VIF > 10인 계수 비율(%) | 평균 VIF |
|------|----------------------|----------|
| dense_1 | 0.0078 | 4.354 |
| dense_2 | 0.9922 | 46.84 |
| dense_3 | 1 | 5.196 × 10^13 |

#### 일반화 성능 개선을 위한 시사점

1. **계층별 단위 축소**: 후속 계층이 이전 계층보다 적은 단위를 가져야 함[1]
2. **정규화의 근거**: L1/L2 정규화와 드롭아웃은 다중공선성 완화에 효과적[1]
3. **수렴 문제 해결**: 계층별 다중공선성 진단으로 문제 계층 사전 파악 가능[1]

### 3. 한계

#### 다항 회귀의 근본적 한계

1. **메모리와 계산량**: 다항식 항의 수 $$l_d = O(p^d)$$로 증가[1]
   - 선형 및 일반화 선형 모델 계산: $$O(n, l_d^2)$$ 시간 소요
   - 분류의 경우: $$O(n, q l_d^2)$$ (q는 클래스 수)

2. **통계적 제약**: $$l_d < \sqrt{n}$$ 권장[1]
   - 높은 차수의 다항식은 큰 샘플 크기 필요

3. **외삽(Extrapolation) 문제**: 데이터 가장자리에서 매우 큰 예측값[1]
   - 하지만 이는 신경망도 동일하게 겪는 문제[1]

#### 현재 polyreg의 한계

- 특화된 신경망(CNN, RNN)에 아직 미적용[1]
- 고차원 데이터 처리 시 메모리 제한[1]
- 이미지 전처리에서 PCA만 사용 (고급 기법 미적용)[1]

---

## 모델의 일반화 성능 향상 가능성

### 1. 다중공선성 제어를 통한 일반화 개선

논문은 신경망의 다중공선성이 **수렴 문제와 과적합의 근본 원인**임을 제시합니다. 따라서 일반화 성능 향상은 다음을 통해 가능합니다:[1]

- **계층별 VIF 모니터링**: VIF > 10인 계수 비율 추적
- **단위 수 동적 조정**: 다중공선성 높은 계층의 단위 감소
- **드롭아웃 적응적 적용**: 문제 계층에 대한 선택적 드롭아웃

### 2. Crossfit 데이터의 사례 연구

표본 크기에 따른 성능 비교에서, **다항 회귀는 표본이 1,000개에서도 전체 표본 성능과 유사**합니다. 신경망은 표본 증가에 따라 개선되지만, 메모리 효율성에서 다항 회귀가 우수합니다.[1]

### 3. PCA를 이용한 차원 축소

주성분 분석(PCA)을 활용하면 메모리, 계산시간, 다중공선성 문제를 모두 완화할 수 있습니다. Million Song 데이터셋에서 PCA 적용 시:[1]
- PR 성능: 7.57-7.77 (90% 분산)
- KF (Keras) 성능: 7.96-8.43

### 4. 일반화 성능 저하 원인

신경망이 과적합되는 주요 이유:[1]

1. **과도한 하이퍼파라미터**: 선택할 매개변수 증가로 이상값 포착 위험 증가
2. **정규화 설정 오류**: 드롭아웃 비율도 하이퍼파라미터로 작용
3. **다항식 차수 과다 설정**: 다항 회귀의 관점에서 보면 불필요하게 고차 다항식 학습

---

## 앞으로의 연구에 미치는 영향 및 고려사항

### 1. 이론적 영향

#### NN ↔ PR 원칙의 의미

신경망 연구의 패러다임 전환을 의미합니다. 기존의 "블랙박스" 관점에서 **"고차 다항 회귀"라는 명확한 수학적 구조**로 이해할 수 있게 됩니다. 이는:[1]

- 신경망의 수렴 문제를 다항 회귀의 다중공선성으로 설명
- 정규화의 효과를 다중공선성 완화로 해석
- 신경망 설계의 원칙적 접근 가능

#### 다른 머신러닝 방법과의 연결

저자들은 **임의의 머신러닝 방법이 다항식과의 관계**를 가질 가능성을 시사합니다. 예를 들어:[1]

- 서포트 벡터 머신(SVM): 커널 함수가 다항식 또는 그 근사
- 랜덤 포레스트: 계층적 의사결정 트리의 다항식적 해석 가능성

### 2. 실무적 영향

#### polyreg 패키지의 확산

R과 Python 기반의 오픈소스 패키지로 실무에서:

- 신경망 대비 **해석 가능성** 대폭 향상
- 하이퍼파라미터 선택 부담 감소
- 계산 비용 절감 (특히 CPU 환경)

#### 산업 응용의 변화

의료(암 유전체), 금융(신용평가), 센서 데이터 등 **해석 가능성이 중요한 분야**에서 다항 회귀의 채용 확대 예상

### 3. 향후 연구 시 고려사항

#### 필수 개선 영역[1]

1. **특화 신경망 통합**
   - CNN의 합성곱 전처리 단계와 다항 회귀 결합
   - RNN의 시계열 구조 모델링
   - 해결 방법: 구조적 방정식 모델(SEM)과 다항식 형태 결합

2. **메모리/계산량 최적화**
   - 다항식 특성 행렬에 PCA 직접 적용
   - 비선형 PCA, 비음(Nonnegative) 행렬 분해 활용
   - 병렬 계산 구현
   - 빅메모리 패키지 백업 저장소 활용
   - Software Alchemy 기법 도입

3. **다중공선성 대응**
   - 능선 회귀(Ridge Regression) 옵션 추가
   - LASSO를 통한 차원 축소
   - 계층별 VIF 기반 자동 최적화 알고리즘

4. **이미지 분류 고도화**
   - 단순 PCA 대신 고급 전처리 기법 (블록 이미지 평활화, 다중 스케일 특성 추출)
   - Fashion MNIST 등 복잡한 데이터셋 실험
   - CNN-다항식 하이브리드 모델 개발

5. **실증적 검증 확대**
   - 고차원(large-p) 데이터셋 실험
   - 다른 머신러닝 방법(랜덤 포레스트, SVM) 비교
   - 산업별 데이터셋 검증 (금융, 의료, 센서)

#### 설계 원칙[1]

1. **다항식 차수 선택**: 
   - 대부분의 경우 차수 2가 충분
   - 필요시 통계적 검정으로 결정
   - $$l_d < \sqrt{n}$$ 준수

2. **차원 축소 선택**:
   - 작은 데이터: 전체 특성 사용
   - 중간 데이터: PCA + 다항식
   - 큰 데이터: 랜덤 특성 선택 또는 Advanced PCA

3. **하이퍼파라미터 최소화**:
   - 신경망 대비 설정 변수 극소화
   - 교차 검증으로 자동 튜닝
   - Forward Stepwise Regression 활용

### 4. 학문적 기여 방향

#### 새로운 이론 개발

- **NN ↔ PR을 넘어**: 신경망을 다항식이 아닌 다른 함수 클래스로 이해할 가능성
- **비볼록 최적화**: 왜 신경망 학습이 국소 최소값에 수렴하는가?
- **정규화의 일반 이론**: 다중공선성 완화 메커니즘의 수학적 정립

#### 새로운 응용 분야

- **의료 진단**: 다항 회귀의 해석 가능성이 임상 의사결정에 직접 기여
- **금융 규제**: 신용평가 모델의 설명 가능성 확보
- **제조업 QC**: 공정 매개변수와 제품 특성의 다항식 관계 발견

***

## 결론

이 논문은 신경망을 "블랙박스"에서 해방시켜 **수학적으로 명확한 다항 회귀 모델**로 재해석하는 근본적인 관점 변화를 제시합니다.[1]

**핵심 기여:**
- 신경망의 본질을 다항 회귀로 설명 가능
- 일반화 성능 향상의 원칙적 접근 가능
- 실무적으로 신경망의 대안 제시

**연구자가 고려할 점:**
- 특화 신경망으로의 확장이 필수
- 메모리 문제 해결을 위한 적극적 차원 축소
- 다중공선성 기반의 신경망 설계 원칙 개발
- 산업 응용을 통한 실증 검증

이 연구는 딥러닝의 "블랙박스" 비판에 과학적 기초를 제공하고, 해석 가능한 머신러닝으로의 패러다임 전환을 촉발할 수 있는 중요한 작업입니다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/05855715-6b75-4527-b61c-5c735bd9e3a0/1806.06850v3.pdf)
