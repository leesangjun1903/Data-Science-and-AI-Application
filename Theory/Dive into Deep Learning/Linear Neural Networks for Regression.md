# Linear Neural Networks for Regression
## 3.1 Linear Regression
실행 예제로, 면적(제곱 피트)과 연령(년)을 기반으로 주택 가격(달러)을 추정하고자 한다고 가정해 보겠습니다.  
주택 가격을 예측하는 모델을 개발하려면 각 주택의 매매 가격, 면적, 연령을 포함한 데이터를 확보해야 합니다.  
머신 러닝 용어로 데이터 세트를 학습 데이터 세트 또는 학습 세트 라고 하며 , 각 행(한 매매에 해당하는 데이터 포함)을 example (or data point, instance, sample)라고 합니다.  
예측하려는 것(가격)을 label (or target)이라고 합니다. 예측의 기반이 되는 변수(연령 및 면적)를 features (or covariates)이라고 합니다.

### 3.1.1 Basics
먼저, 우리는 특징 x와 목표 𝑦 간의 관계가 대략 선형적이라고 가정합니다. 즉, 조건부 평균 𝐸[𝑌 | 𝑋= x]가 특징 x의 가중 합으로 표현될 수 있다고 가정합니다.  
이 설정은 관찰 노이즈로 인해 목표 값이 여전히 예상 값에서 벗어날 수 있게 합니다.  
다음으로, 이러한 노이즈가 가우시안 분포를 따르며 잘 작동한다고 가정할 수 있습니다.  
일반적으로, 우리는 데이터셋의 예제 수를 나타내기 위해 𝑛를 사용할 것입니다. 우리는 샘플과 목표를 열거하기 위해 위첨자를 사용하고, 인덱스 좌표에 대한 첨자를 사용합니다.  
보다 구체적으로, x(𝑖)는 𝑖 번째 샘플을 나타내고, 𝑥(𝑖) 𝑗는 그 𝑗 번째 좌표를 나타냅니다.

#### Model
$$\textrm{price} = w_{\textrm{area}} \cdot \textrm{area} + w_{\textrm{age}} \cdot \textrm{age} + b.$$

여기서 𝑤 면적과 𝑤 연령을 가중치라고 하고, 𝑏을 편향(또는 오프셋 또는 절편)이라고 합니다.  
가중치는 각 특징이 예측에 미치는 영향을 결정합니다. 편향은 모든 특징이 0일 때 추정치의 값을 결정합니다.  
면적이 정확히 0인 새로 지어진 집은 볼 수 없지만, 원점을 통과하는 선에 국한하지 않고 특징의 모든 선형 함수를 표현할 수 있기 때문에 편향이 여전히 필요합니다.

입력이 𝑑 특징으로 구성된 경우 각 입력에 인덱스(1에서 𝑑 사이)를 할당하고 예측 ˆ 𝑦를 다음과 같이 표현할 수 있습니다(일반적으로 "모자" 기호는 추정치를 나타냅니다)  

$$\hat{y} = w_1  x_1 + \cdots + w_d  x_d + b.$$

여기서 X는 모든 예제에 대해 하나의 행과 모든 특징에 대해 하나의 열을 포함합니다. 특징 X의 집합에 대해 예측 ˆ y ∈R 𝑛은 행렬-벡터 곱을 통해 표현할 수 있습니다:

$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,$$

훈련 데이터셋 X와 해당 (알려진) 레이블 Y의 특징이 주어졌을 때, 선형 회귀의 목표는 가중치 벡터 w와 바이어스 항 𝑏를 찾는 것입니다.  
이를 통해 X와 동일한 분포에서 샘플링된 새로운 데이터 예제의 특징이 주어졌을 때, 새로운 예제의 레이블이 (예상대로) 가장 작은 오류로 예측될 것입니다.

#### Loss Function
손실은 일반적으로 작은 값이 더 좋고 완벽한 예측이 0의 손실을 초래하는 음수가 아닌 숫자가 됩니다. 회귀 문제의 경우 가장 일반적인 손실 함수는 제곱 오차입니다. 예제 𝑖에 대한 예측이 ˆ 𝑦(𝑖)이고 해당하는 참 레이블이 𝑦(𝑖)인 경우 제곱 오차는 다음과 같이 주어집니다:

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$

전체 𝑛 예제 데이터셋에서 모델의 품질을 측정하기 위해 학습 세트의 손실을 평균(또는 동등하게 합산)하기만 하면 됩니다:

$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

```math
\mathbf{w}^*, b^* = argmin_{\mathbf{w}, b}\  L(\mathbf{w}, b).
```

#### Analytic Solution
특히, 다음과 같은 간단한 공식을 적용하여 학습 데이터에서 평가된 최적의 매개변수를 분석적으로 찾을 수 있습니다. 먼저, 모든 1로 구성된 설계 행렬에 열을 추가하여 편향 𝑏를 매개변수 w에 포함시킬 수 있습니다.  
그런 다음 우리의 예측 문제는 ∥y-Xw ∥2를 최소화하는 것입니다. 

$$\begin{aligned}
    \partial_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 =
    2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) = 0
    \textrm{ and hence }
    \mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}.
\end{aligned}$$

$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}$$

#### Minibatch Stochastic Gradient Descent
가장 기본적인 형태로, 각 반복 𝑡에서 먼저 훈련 예제의 고정된 수 |B|로 구성된 미니배치 B 𝑡를 무작위로 샘플링합니다. 그런 다음 모델 매개변수에 대한 미니배치의 평균 손실의 도함수(gradient)를 계산합니다.  
마지막으로, 기울기에 미리 정해진 작은 양의 값 𝜂를 곱한 다음 현재 매개변수 값에서 결과 항을 뺍니다. 업데이트를 다음과 같이 표현할 수 있습니다:

$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$

요약하자면, 미니배치 SGD는 다음과 같이 진행됩니다. (i) 일반적으로 무작위로 모델 매개변수 값을 초기화합니다.  
(ii) 데이터에서 무작위 미니배치를 반복적으로 샘플링하여 음의 기울기 방향으로 매개변수를 업데이트합니다. 이차 손실과 아핀 변환의 경우, 이는 닫힌 형태의 전개를 따릅니다.

<img width="642" alt="스크린샷 2025-04-24 오후 2 31 27" src="https://github.com/user-attachments/assets/f91683f8-d62f-4e2c-84bf-271f22f7831e" />

미니배치 B를 선택하기 때문에 크기 |B|로 정규화해야 합니다. 미니배치 크기와 학습률은 종종 사용자가 정의합니다. 학습 루프에서 업데이트되지 않는 이러한 조정 가능한 매개변수를 하이퍼파라미터라고 합니다.

#### Predictions
추론은 대체로 매개변수 값과 보지 못한 사례에 대한 가능성 있는 레이블을 포함한 증거에 근거하여 도달한 모든 결론을 의미합니다.

### 3.1.2 Vectorization for Speed
모델을 훈련할 때 일반적으로 전체 미니배치 예제를 동시에 처리하고자 합니다. 이를 효율적으로 수행하려면 비용이 많이 드는 Python에 for-loop을 작성하는 대신 계산을 벡터화하고 빠른 선형 대수 라이브러리를 활용해야 합니다. 

### 3.1.3 The Normal Distribution and Squared Loss
우선, 평균 𝜇과 분산 𝜎2를 갖는 정규 분포(표준 편차 𝜎)가 다음과 같이 주어짐을 기억하세요

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right).$$

손실 제곱이 있는 선형 회귀를 유도하는 한 가지 방법은 노이즈 𝜖이 정규 분포 N(0, 𝜎2)를 따르는 노이즈 측정에서 관측값이 발생한다고 가정하는 것입니다:

<img width="339" alt="스크린샷 2025-04-24 오후 3 53 41" src="https://github.com/user-attachments/assets/08abff4b-10e1-4a2a-81f5-c67522891745" />

따라서 이제 주어진 x에 대해 특정 𝑦를 볼 가능성을 다음과 같이 쓸 수 있습니다

<img width="432" alt="스크린샷 2025-04-24 오후 3 53 59" src="https://github.com/user-attachments/assets/5862cd80-7635-4644-967e-615e06a34dc0" />

따라서 우도는 인수분해됩니다. 최대 우도의 원리에 따르면 매개변수 w와 𝑏의 최적 값은 전체 데이터 세트의 우도를 최대화하는 값입니다:

$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)}).$$

많은 지수 함수의 곱을 최대화하는 것은 어려워 보일 수 있지만, 목표를 변경하지 않고 우도의 로그를 최대화함으로써 상황을 크게 단순화할 수 있습니다.  
역사적인 이유로 최적화는 최대화가 아닌 최소화로 표현되는 경우가 더 많습니다. 따라서 아무것도 변경하지 않고 음수 로그 우도를 최소화할 수 있으며, 이를 다음과 같이 표현할 수 있습니다:

$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2.$$

𝜎가 고정되어 있다고 가정하면 첫 번째 항은 w나 𝑏에 의존하지 않기 때문에 무시할 수 있습니다.  
두 번째 항은 앞서 소개한 제곱 오차 손실과 동일하지만 곱셈 상수 $\frac{1}{𝜎^2}$를 제외하고는 마찬가지입니다. 다행히도 해는 𝜎에 의존하지 않습니다. 따라서 평균 제곱 오차를 최소화하는 것은 가우시안 노이즈를 가정한 선형 모델의 최대 우도 추정과 동일합니다.

### 3.1.4 Linear Regression as a Neural Network

<img width="703" alt="스크린샷 2025-04-24 오후 3 59 32" src="https://github.com/user-attachments/assets/e382b1cd-2bbc-4ba1-a3b5-46ae4fb9da61" />

입력값은 𝑥1,...,𝑥𝑑입니다. 우리는 𝑑를 입력 레이어의 입력 수 또는 특징 차원으로 부릅니다.  
네트워크의 출력은 𝑜1입니다. 단일 수치 값을 예측하려고 하기 때문에 출력 뉴런은 하나뿐입니다.  
입력 값은 모두 주어져 있습니다. 계산된 뉴런은 하나뿐입니다.  
요약하자면, 선형 회귀는 단일 레이어로 완전히 연결된 신경망(fully connected neural network)이라고 생각할 수 있습니다.

## 3.2 Object-Oriented Design for Implementation
### 3.2.1 Utilities

## 3.3 Synthetic Regression Data
### 3.3.1 Generating the Dataset
이 예제에서는 간결성을 위해 저차원에서 작업할 것입니다. 다음 코드 스니펫은 표준 정규 분포에서 추출한 2차원 특징을 포함한 1000개의 예제를 생성합니다.  
결과적으로 생성된 설계 행렬 X는 R1000×2에 속합니다. 우리는 각 예제에 대해 독립적이고 동일하게 그려진 가산 잡음 𝝐을 통해 이를 손상시키는 지상 진리 선형 함수를 적용하여 각 레이블을 생성합니다:

편의상, 우리는 𝝐가 평균 𝜇= 0이고 표준 편차 𝜎 = 0.01인 정규 분포에서 도출된다고 가정합니다. 

### 3.3.2 Reading the Dataset



## 3.4 Linear Regression Implementation from Scratch


## 3.6. Generalization
실제로 우리는 유한한 데이터 집합을 사용하여 모델을 맞춰야 합니다. 해당 데이터의 일반적인 규모는 도메인마다 크게 다릅니다. 
### 3.6.1 Training Error and Generalization Error
우선, 훈련 데이터셋에서 계산된 통계인 훈련 오류 𝑅emp와 기본 분포에 대해 취한 기대값인 일반화 오류 𝑅를 구분해야 합니다.  
일반화 오류는 동일한 기본 데이터 분포에서 추출한 무한한 추가 데이터 예제 스트림에 모델을 적용하면 어떤 결과가 나올지 생각할 수 있습니다.  
공식적으로 훈련 오류는 3.1절과 동일한 표기법으로 합계로 표현됩니다:

$$R_\textrm{emp}[\mathbf{X}, \mathbf{y}, f] = \frac{1}{n} \sum_{i=1}^n l(\mathbf{x}^{(i)}, y^{(i)}, f(\mathbf{x}^{(i)})),$$

$$R[p, f] = E_{(\mathbf{x}, y) \sim P} [l(\mathbf{x}, y, f(\mathbf{x}))] =
\int \int l(\mathbf{x}, y, f(\mathbf{x})) p(\mathbf{x}, y) \;d\mathbf{x} dy.$$

#### Model Complexity
고전 이론에서는 단순한 모델과 풍부한 데이터가 있을 때 학습 오류와 일반화 오류가 밀접한 경향이 있습니다.  
그러나 더 복잡한 모델이나 더 적은 수의 예제를 다룰 때는 학습 오류는 감소하지만 일반화 격차는 커질 것으로 예상합니다.  
이는 놀라운 일이 아닙니다. 𝑛 예제 데이터셋에 대해 임의의 레이블에 무작위로 할당되더라도 완벽하게 맞출 수 있는 매개변수 집합을 찾을 수 있을 정도로 표현력이 뛰어난 모델 클래스를 상상해 보세요.  
이 경우 학습 데이터를 완벽하게 맞추더라도 일반화 오류에 대해 어떻게 결론을 내릴 수 있을까요? 우리가 아는 한 일반화 오류는 무작위 추측보다 나을 수 없습니다.

일반적으로 모델 클래스에 대한 제한이 없다면 학습 데이터만 맞추는 것만으로는 모델이 일반화할 수 있는 패턴을 발견했다고 결론지을 수 없습니다(Vapnik et al., 1994). 

이제 모델 복잡성의 적절한 개념을 구성하는 것은 복잡한 문제입니다.  
종종 더 많은 매개변수를 가진 모델은 더 많은 수의 임의로 할당된 레이블을 맞출 수 있습니다.  
그러나 반드시 그런 것은 아닙니다. 예를 들어, 커널 방법은 무한한 수의 매개변수가 있는 공간에서 작동하지만 복잡성은 다른 수단에 의해 제어됩니다(Schölkopf와 Smola, 2002). 

종종 유용하다고 증명되는 복잡성의 한 가지 개념은 매개변수가 취할 수 있는 값의 범위입니다.  
여기서 매개변수가 임의의 값을 취할 수 있는 모델은 더 복잡할 것입니다. 

임의의 레이블을 맞출 수 있는 모델이라면, 낮은 훈련 오류가 반드시 낮은 일반화 오류를 의미하는 것은 아닙니다.  
하지만 반드시 높은 일반화 오류를 의미하는 것은 아닙니다! 우리가 자신 있게 말할 수 있는 것은 낮은 훈련 오류만으로는 낮은 일반화 오류를 증명하기에 충분하지 않다는 것입니다.  
딥 뉴럴 네트워크는 실제로는 잘 일반화되지만, 훈련 오류만으로는 많은 것을 결론지을 수 없을 정도로 강력합니다. 이러한 경우에는 보류 데이터에 더 많이 의존하여 사후 일반화를 인증해야 합니다.  
보류 데이터, 즉 검증 세트에 대한 오류를 검증 오류라고 합니다.

