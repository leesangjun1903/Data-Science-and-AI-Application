# Linear Neural Networks for Regression
## 3.1 Linear Regression
실행 예제로, 면적(제곱 피트)과 연령(년)을 기반으로 주택 가격(달러)을 추정하고자 한다고 가정해 보겠습니다.  
주택 가격을 예측하는 모델을 개발하려면 각 주택의 매매 가격, 면적, 연령을 포함한 데이터를 확보해야 합니다.  
머신 러닝 용어로 데이터 세트를 학습 데이터 세트 또는 학습 세트 라고 하며 , 각 행(한 매매에 해당하는 데이터 포함)을 example (or data point, instance, sample)라고 합니다.  
예측하려는 것(가격)을 label (or target)이라고 합니다. 예측의 기반이 되는 변수(연령 및 면적)를 features (or covariates)이라고 합니다.

### 3.1.1 Basics
먼저, 우리는 특징 x와 목표 𝑦 간의 관계가 대략 선형적이라고 가정합니다. 즉, 조건부 평균 𝐸[𝑌 | 𝑋= x]가 특징 x의 가중 합으로 표현될 수 있다고 가정합니다.  
이 설정은 관찰 노이즈로 인해 목표 값이 여전히 예상 값에서 벗어날 수 있게 합니다.  
다음으로, 이러한 노이즈가 가우시안 분포를 따르며 잘 작동한다고 가정할 수 있습니다.  
일반적으로, 우리는 데이터셋의 예제 수를 나타내기 위해 𝑛를 사용할 것입니다. 우리는 샘플과 목표를 열거하기 위해 위첨자를 사용하고, 인덱스 좌표에 대한 첨자를 사용합니다.  
보다 구체적으로, x(𝑖)는 𝑖 번째 샘플을 나타내고, 𝑥(𝑖) 𝑗는 그 𝑗 번째 좌표를 나타냅니다.

#### Model
$$\textrm{price} = w_{\textrm{area}} \cdot \textrm{area} + w_{\textrm{age}} \cdot \textrm{age} + b.$$

여기서 𝑤 면적과 𝑤 연령을 가중치라고 하고, 𝑏을 편향(또는 오프셋 또는 절편)이라고 합니다.  
가중치는 각 특징이 예측에 미치는 영향을 결정합니다. 편향은 모든 특징이 0일 때 추정치의 값을 결정합니다.  
면적이 정확히 0인 새로 지어진 집은 볼 수 없지만, 원점을 통과하는 선에 국한하지 않고 특징의 모든 선형 함수를 표현할 수 있기 때문에 편향이 여전히 필요합니다.

입력이 𝑑 특징으로 구성된 경우 각 입력에 인덱스(1에서 𝑑 사이)를 할당하고 예측 ˆ 𝑦를 다음과 같이 표현할 수 있습니다(일반적으로 "모자" 기호는 추정치를 나타냅니다)  

$$\hat{y} = w_1  x_1 + \cdots + w_d  x_d + b.$$

여기서 X는 모든 예제에 대해 하나의 행과 모든 특징에 대해 하나의 열을 포함합니다. 특징 X의 집합에 대해 예측 ˆ y ∈R 𝑛은 행렬-벡터 곱을 통해 표현할 수 있습니다:

$${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b,$$

훈련 데이터셋 X와 해당 (알려진) 레이블 Y의 특징이 주어졌을 때, 선형 회귀의 목표는 가중치 벡터 w와 바이어스 항 𝑏를 찾는 것입니다.  
이를 통해 X와 동일한 분포에서 샘플링된 새로운 데이터 예제의 특징이 주어졌을 때, 새로운 예제의 레이블이 (예상대로) 가장 작은 오류로 예측될 것입니다.

#### Loss Function
손실은 일반적으로 작은 값이 더 좋고 완벽한 예측이 0의 손실을 초래하는 음수가 아닌 숫자가 됩니다. 회귀 문제의 경우 가장 일반적인 손실 함수는 제곱 오차입니다. 예제 𝑖에 대한 예측이 ˆ 𝑦(𝑖)이고 해당하는 참 레이블이 𝑦(𝑖)인 경우 제곱 오차는 다음과 같이 주어집니다:

$$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2.$$

전체 𝑛 예제 데이터셋에서 모델의 품질을 측정하기 위해 학습 세트의 손실을 평균(또는 동등하게 합산)하기만 하면 됩니다:

$$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2.$$

```math
\mathbf{w}^*, b^* = argmin_{\mathbf{w}, b}\  L(\mathbf{w}, b).
```

#### Analytic Solution
특히, 다음과 같은 간단한 공식을 적용하여 학습 데이터에서 평가된 최적의 매개변수를 분석적으로 찾을 수 있습니다. 먼저, 모든 1로 구성된 설계 행렬에 열을 추가하여 편향 𝑏를 매개변수 w에 포함시킬 수 있습니다.  
그런 다음 우리의 예측 문제는 ∥y-Xw ∥2를 최소화하는 것입니다. 

$$\begin{aligned}
    \partial_{\mathbf{w}} \|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2 =
    2 \mathbf{X}^\top (\mathbf{X} \mathbf{w} - \mathbf{y}) = 0
    \textrm{ and hence }
    \mathbf{X}^\top \mathbf{y} = \mathbf{X}^\top \mathbf{X} \mathbf{w}.
\end{aligned}$$

$$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}$$

#### Minibatch Stochastic Gradient Descent
가장 기본적인 형태로, 각 반복 𝑡에서 먼저 훈련 예제의 고정된 수 |B|로 구성된 미니배치 B 𝑡를 무작위로 샘플링합니다. 그런 다음 모델 매개변수에 대한 미니배치의 평균 손실의 도함수(gradient)를 계산합니다.  
마지막으로, 기울기에 미리 정해진 작은 양의 값 𝜂를 곱한 다음 현재 매개변수 값에서 결과 항을 뺍니다. 업데이트를 다음과 같이 표현할 수 있습니다:

$$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}_t} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b).$$

요약하자면, 미니배치 SGD는 다음과 같이 진행됩니다. (i) 일반적으로 무작위로 모델 매개변수 값을 초기화합니다.  
(ii) 데이터에서 무작위 미니배치를 반복적으로 샘플링하여 음의 기울기 방향으로 매개변수를 업데이트합니다. 이차 손실과 아핀 변환의 경우, 이는 닫힌 형태의 전개를 따릅니다.

<img width="642" alt="스크린샷 2025-04-24 오후 2 31 27" src="https://github.com/user-attachments/assets/f91683f8-d62f-4e2c-84bf-271f22f7831e" />

미니배치 B를 선택하기 때문에 크기 |B|로 정규화해야 합니다. 미니배치 크기와 학습률은 종종 사용자가 정의합니다. 학습 루프에서 업데이트되지 않는 이러한 조정 가능한 매개변수를 하이퍼파라미터라고 합니다.

#### Predictions
추론은 대체로 매개변수 값과 보지 못한 사례에 대한 가능성 있는 레이블을 포함한 증거에 근거하여 도달한 모든 결론을 의미합니다.

### 3.1.2 Vectorization for Speed
모델을 훈련할 때 일반적으로 전체 미니배치 예제를 동시에 처리하고자 합니다. 이를 효율적으로 수행하려면 비용이 많이 드는 Python에 for-loop을 작성하는 대신 계산을 벡터화하고 빠른 선형 대수 라이브러리를 활용해야 합니다. 


