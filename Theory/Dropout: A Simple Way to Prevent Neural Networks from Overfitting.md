# Dropout: A Simple Way to Prevent Neural Networks from Overfitting

### 1. 핵심 주장과 주요 기여

**Dropout: A Simple Way to Prevent Neural Networks from Overfitting** 논문의 핵심 주장은 **훈련 중 무작위로 신경망의 유닛(뉴런)을 제거하는 단순한 기법을 통해 깊은 신경망의 과적합을 효과적으로 방지할 수 있다**는 것입니다.[1]

주요 기여는 다음과 같습니다:

- **Exponential Model Averaging**: 드롭아웃은 지수적으로 많은 "thinned" 신경망들의 예측을 근사적으로 평균화하는 것과 동일합니다. $$2^n$$개의 가능한 부분 네트워크로부터 샘플링하면서 가중치를 공유합니다.[1]

- **Test Time Weight Scaling**: 훈련 시 확률 $$p$$로 유닛을 유지하면, 테스트 시 가중치에 $$p$$를 곱하기만 하면 됩니다. 이 간단한 스케일링으로 모든 thinned 네트워크의 효과를 근사할 수 있습니다.[1]

- **Co-adaptation 방지**: 드롭아웃은 은닉 유닛들이 특정 다른 유닛들에 과도하게 의존하는 복잡한 상호적응(co-adaptation)을 방지합니다.[1]

***

### 2. 문제 정의 및 해결 방법

#### 해결하고자 하는 문제

깊은 신경망은 매우 표현력이 뛰어나지만, **제한된 학습 데이터에서 과적합 문제가 심각**합니다. 전통적 정규화 방법들(L1/L2 정규화, 조기 종료)은 충분하지 않으며, 여러 대규모 신경망을 앙상블로 조합하는 것도 계산상 비효율적입니다.[1]

#### 제안하는 방법 (수식 포함)

**표준 신경망의 순전파(Feedforward):**

모든 은닉 유닛 $$i$$와 층 $$l = 0, \ldots, L-1$$에 대해:

$$
z^{l+1}_i = \mathbf{w}^{l+1}_i \cdot \mathbf{y}^l + b^{l+1}_i
$$

$$
y^{l+1}_i = f(z^{l+1}_i)
$$

여기서 $$f$$는 활성화 함수(예: sigmoid, ReLU)입니다.[1]

**드롭아웃이 적용된 신경망:**

각 층 $$l$$에서 베르누이 확률변수 $$\mathbf{r}^l \sim \text{Bernoulli}(p)$$를 샘플링하여:

$$
\mathbf{r}^l_j \sim \text{Bernoulli}(p)
$$

$$
\tilde{\mathbf{y}}^l = \mathbf{r}^l \odot \mathbf{y}^l
$$

$$
z^{l+1}_i = \mathbf{w}^{l+1}_i \cdot \tilde{\mathbf{y}}^l + b^{l+1}_i
$$

여기서 $$\odot$$는 원소별 곱(element-wise product)입니다. 즉, 각 유닛은 확률 $$p$$로 유지되고 확률 $$1-p$$로 제거됩니다.[1]

**테스트 시 가중치 스케일링:**

테스트 시점에서는 드롭아웃 없이 가중치를 스케일링합니다:

$$
\mathbf{W}^l_{\text{test}} = p \cdot \mathbf{W}^l
$$

이렇게 하면 훈련 시의 기댓값과 테스트 시의 실제 출력이 동일하게 됩니다.[1]

**Marginalization을 통한 정규화:**

선형 회귀에서 드롭아웃을 marginalize하면:

$$
\text{minimize}_{\mathbf{w}} \|\mathbf{y} - p\mathbf{X}\mathbf{w}\|^2 + p(1-p)\|\boldsymbol{\Sigma}\mathbf{w}\|^2
$$

이는 특수한 형태의 ridge 정규화와 동등합니다. 여기서 $$\boldsymbol{\Sigma}$$는 데이터 표준편차를 나타냅니다.[1]

***

### 3. 모델 구조 및 훈련

#### 기본 구조

**구조적 특징:**
- 순방향 신경망(feedforward networks)에 적용 가능
- 합성곱 신경망(CNN)에도 확대 적용 가능
- 제한 볼츠만 기계(RBM)에도 확대 가능[1]

**하이퍼파라미터:**
- **보유 확률 $$p$$**: 은닉층에서 보통 0.5, 입력층에서 보통 0.8
- **Max-norm 제약**: $$\|\mathbf{w}\|_2 \leq c$$ (보통 $$c=3\sim 4$$)[1]

#### 훈련 절차 (Section 5)

**확률적 경사하강법(SGD) 기반 훈련:**
1. 각 미니배치의 훈련 사례마다 새로운 thinned 네트워크 샘플링
2. 해당 부분 네트워크에서만 순전파와 역전파 수행
3. 손실 함수의 기울기를 미니배치 전체에서 평균화[1]

**최적화 기법:**
- **높은 학습률**: 드롭아웃 없는 신경망에 비해 10~100배 높은 학습률 사용
- **높은 모멘텀**: 보통 0.95~0.99의 모멘텀값 사용 (표준은 0.9)
- **Max-norm 정규화**: 가중치 벡터의 노름을 제한하여 안정성 확보[1]

**사전훈련(Pretraining):**
드롭아웃과 함께 사전훈련된 가중치를 사용할 때는 $$\frac{1}{p}$$로 스케일업하여 기댓값을 유지합니다.[1]

***

### 4. 성능 향상 결과

#### 이미지 분류

**MNIST:**
- 기준 신경망: 1.60% 오류율
- 드롭아웃 NN (ReLU): 1.25% → 1.06% (max-norm 추가)
- 대규모 네트워크 (8192 유닛): 0.95%
- DBM 사전훈련 + 드롭아웃: **0.79%** (당시 최고 성능)[1]

**Street View House Numbers (SVHN):**
- 기준 ConvNet: 3.95%
- 완전연결층만 드롭아웃: 3.02%
- 모든 층에 드롭아웃: **2.55%** (인간 성능 2.0%에 근접)[1]

**CIFAR-10/100:**
- CIFAR-10: 14.98% → 12.61% (드롭아웃 모든 층)
- CIFAR-100: 43.48% → 37.20% (대폭 개선)[1]

**ImageNet ILSVRC-2012:**
- 최고 기존 방법: Top-5 오류 26%
- 드롭아웃 ConvNet: **Top-5 오류 16%** (획기적 개선)
- 이 모델이 ILSVRC-2012 우승[1]

#### 음성 인식 (TIMIT)

- 6층 네트워크: 23.4% → 21.8%
- DBN 사전훈련 + 드롭아웃: 20.7% → **19.7%**[1]

#### 문서 분류 (Reuters-RCV1)

- 기준: 31.05% → **29.62%** (상대적으로 작은 개선, 큰 데이터셋에서는 과적합이 덜함)[1]

---

### 5. 일반화 성능 향상 메커니즘

#### 특성 품질 개선 (Feature Quality)

드롭아웃이 없는 네트워크에서 학습된 특성들은 **과도한 상호적응(co-adaptation)**을 보입니다. MNIST 실험에서:
- 드롭아웃 없음: 개별 유닛이 의미 있는 특성을 감지하지 못함
- 드롭아웃 적용 ($$p=0.5$$): 에지(edge), 획(stroke), 점(spot) 등 해석 가능한 특성 감지[1]

**왜 이것이 일반화에 도움이 되는가?**
각 은닉 유닛은 다른 임의의 유닛들과 함께 작동해야 하므로, 특정 유닛들에 의존하지 않고 **독립적으로 유용한 특성을 학습**합니다.[1]

#### 희소성(Sparsity) 증가

드롭아웃의 부수 효과로 은닉 유닛의 활성화가 자동으로 희소해집니다:
- 드롭아웃 없음: 평균 활성화 ~2.0
- 드롭아웃 적용: 평균 활성화 ~0.7
- 많은 유닛이 0에 가까운 값을 가짐[1]

이는 **명시적 희소성 정규화 없이도 자동으로 희소 표현을 학습**함을 의미합니다.

#### 데이터셋 크기 효과

드롭아웃의 이점은 데이터셋 크기에 따라 변합니다 (Figure 10):
- 극소 데이터셋 (100, 500): 드롭아웃이 도움이 안 됨 (과적합할 여지가 없음)
- 중간 크기 데이터셋: 최대 이점 (각 아키텍처/드롭아웃률마다 최적의 '단맛 지점' 존재)
- 큰 데이터셋: 이점 감소 (어차피 과적합이 덜함)[1]

#### Dropout 비율의 영향

드롭아웃 비율 $$p$$에 대한 민감도:
- $$p$$ 너무 낮음 (0.1): 과소적합, 높은 훈련 오류
- $$p = 0.4 \sim 0.8$$: 성능이 평탄 (로버스트함)
- $$p$$ 1에 가까움: 드롭아웃 없음과 유사하게 과적합 증가[1]

***

### 6. 주요 한계점

#### 훈련 시간 증가

**가장 큰 단점**: 드롭아웃 네트워크는 표준 신경망에 비해 **2~3배 더 오래 훈련**됩니다.[1]

원인:
- 각 훈련 사례가 다른 임의의 아키텍처를 훈련하므로 기울기가 매우 노이즈음
- 계산되는 기울기가 최종 테스트 아키텍처의 기울기가 아님[1]

**Trade-off**: 훈련 시간 증가 vs. 더 강한 정규화로 인한 과적합 방지

#### 하이퍼파라미터 튜닝의 어려움

- 네트워크 크기 $$n$$과 보유 확률 $$p$$의 상호작용: $$p \cdot n$$을 일정하게 유지 필요
- 작은 $$p$$는 큰 $$n$$ 필요 → 훈련 및 테스트 시간 증가[1]

#### Bayesian 신경망과의 비교

Bayesian 신경망이 이론적으로 더 우수합니다:
- 각 모델을 동일하게 가중: 드롭아웃
- 사후 확률로 가중: Bayesian 신경망 (더 정확하지만 계산 비용 높음)

Alternative Splicing 데이터셋에서:
- Bayesian NN: 623
- 드롭아웃 NN: **567** (우월하지만 거의 근접)[1]

#### 그 외 한계

- **Marginalization의 한계**: 선형 회귀에서는 닫힌 형태로 표현되지만, 깊은 네트워크에서는 근사만 가능하고 가정이 약함[1]
- **텍스트 도메인 성능**: 이미지/음성보다 개선폭이 적음 (데이터셋이 이미 충분히 큼)[1]

***

### 7. 미래 연구에 미치는 영향

#### 이미 미친 영향

- **AlexNet (2012)**: ImageNet 경쟁에서 혁신적 성능으로 딥러닝 부흥 견인 (드롭아웃 사용)
- **CNN 아키텍처 발전**: VGG, ResNet 등 현대 모델들도 드롭아웃 또는 변형 사용
- **표준화된 정규화 기법**: 학계와 산업에서 사실상의 표준 정규화 방법이 됨

#### 앞으로의 연구 방향

**1. 드롭아웃 변형 및 개선:**
- Gaussian Dropout: 베르누이 드롭아웃 대신 가우시안 노이즈 사용 (계산 이점 있음)[1]
- Marginalized Dropout: 결정론적 정규화로 변환하여 훈련 시간 단축 시도[1]
- Variational Dropout: 모든 시간 스텝에서 동일한 샘플링 (순환 신경망용)

**2. 다양한 아키텍처로의 확대:**
- Transformer 아키텍처에서의 드롭아웃 역할 재조명
- Vision Transformer, BERT 등 현대 모델에서 여전히 활용

**3. 이론적 이해 심화:**
- 왜 드롭아웃이 작동하는가에 대한 더 엄밀한 수학적 분석
- 최적 드롭아웃 비율을 자동으로 결정하는 방법 개발

#### 실무 적용 시 고려사항

**1. 드롭아웃 위치 선택:**
- 매개변수가 많은 층(완전연결층, 높은 특성맵 수)에 먼저 적용
- 컨볼루션층에도 적용 시 더 저수준 입력에 대한 노이즈 제공 효과[1]

**2. 하이퍼파라미터 선택 가이드 (Appendix A):**

| 파라미터 | 권장값 | 설명 |
|---------|-------|------|
| 은닉층 $$p$$ | 0.5~0.8 | 작업에 따라 검증 세트로 선택 |
| 입력층 $$p$$ | 0.8 (실수) | 입력 노이즈에 더 강함 |
| 네트워크 크기 | $$n \geq \frac{n_0}{p}$$ | 드롭아웃 없는 최적 크기 $$n_0$$의 역수배 |
| 학습률 | 기본의 10~100배 | 노이즈 많은 기울기 대응 |
| 모멘텀 | 0.95~0.99 | 기울기 동요 완화 |
| Max-norm | $$c=3\sim 4$$ | 가중치 폭발 방지 |[1]

**3. 데이터 크기별 전략:**
- 매우 작은 데이터셋: 드롭아웃보다 다른 정규화 기법 우선 고려
- 중간 크기: 드롭아웃이 매우 효과적
- 매우 큰 데이터셋: 적절한 드롭아웃률로도 자연스러운 정규화[1]

**4. 사전훈련과의 통합:**
- 사전훈련 가중치에서 $$\frac{1}{p}$$로 스케일업
- 미세조정(finetuning)시 더 작은 학습률 사용[1]

***

### 결론

Dropout은 **개념상 단순하면서도 실무상 극도로 효과적인 정규화 기법**으로, 깊은 신경망의 과적합 문제를 해결하는 게임 체인저가 되었습니다. 2012년 AlexNet 우승으로부터 시작하여 현재까지 거의 모든 깊은 학습 모델에서 활용되고 있습니다.[1]

앞으로의 연구는 더 이론적인 이해, 자동 하이퍼파라미터 결정, 다양한 아키텍처에 최적화된 변형 개발에 초점을 맞출 것으로 예상됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/823228c2-1558-4d6b-b887-1aea7fd1649d/srivastava14a.pdf)
