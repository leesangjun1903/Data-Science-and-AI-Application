# Statistical Modeling: The Two Cultures

## 1. 핵심 주장과 주요 기여

Leo Breiman의 "Statistical Modeling: The Two Cultures"는 통계학 분야의 두 가지 상반된 접근 방식을 제시하며, 통계학계의 패러다임 전환을 주장한 획기적인 논문입니다.[1]

**두 가지 문화(Two Cultures)**

논문은 통계 모델링에서 두 가지 근본적으로 다른 접근 방식을 제시합니다:[1]

**데이터 모델링 문화(Data Modeling Culture)**: 데이터가 특정 확률적 모델(stochastic data model)에 의해 생성된다고 가정합니다. 예를 들어, 선형 회귀, 로지스틱 회귀, Cox 모델 등을 사용하며, 모델의 매개변수를 추정하고 적합도 검정(goodness-of-fit tests)과 잔차 분석(residual analysis)을 통해 모델을 검증합니다. Breiman은 통계학자의 약 98%가 이 문화에 속한다고 추정했습니다.[1]

**알고리즘 모델링 문화(Algorithmic Modeling Culture)**: 블랙박스 내부를 복잡하고 알 수 없는 것으로 간주합니다. 의사결정 트리(decision trees), 신경망(neural nets) 등의 알고리즘을 사용하여 예측 정확도(predictive accuracy)로 모델을 검증합니다. 통계학자의 약 2%만이 이 문화에 속하지만, 다른 분야의 많은 연구자들이 이 접근법을 사용합니다.[1]

**주요 기여**

Breiman은 데이터 모델링에 대한 통계학계의 배타적 의존이 다음과 같은 문제를 야기했다고 주장합니다:[1]

- 부적절한 이론(irrelevant theory)과 의심스러운 과학적 결론을 초래했습니다
- 통계학자들이 더 적합한 알고리즘 모델을 사용하지 못하게 막았습니다
- 통계학자들이 흥미로운 새로운 문제들에 참여하지 못하게 했습니다

이 논문은 알고리즘 모델링의 중요성을 강조하며, Random Forests와 같은 혁신적인 방법론을 소개하여 현대 기계학습의 기초를 마련했습니다.[1]

## 2. 연구의 목적과 필요성

**연구의 목적**

Breiman의 연구는 통계학 분야가 데이터 모델에 대한 배타적 의존에서 벗어나 더 다양한 도구를 채택해야 한다는 필요성을 제기하는 것입니다. 그는 "만약 우리 분야의 목표가 데이터를 사용하여 문제를 해결하는 것이라면, 데이터 모델에 대한 배타적 의존에서 벗어나 더 다양한 도구 세트를 채택해야 한다"고 주장합니다.[1]

**연구의 필요성**

연구의 필요성은 여러 측면에서 제기됩니다:[1]

**현실적 문제 해결의 한계**: Breiman은 13년간의 컨설팅 경험을 통해 실제 문제들(오존 수준 예측, 질량 분광 분석, 음성 인식 등)에서 전통적인 데이터 모델의 한계를 경험했습니다. 예를 들어, 1970년대 로스앤젤레스의 오존 수준 예측 프로젝트는 450개 이상의 기상 변수를 다루어야 했으며, 선형 회귀와 변수 선택만으로는 충분한 예측 정확도를 달성하지 못했습니다.[1]

**데이터 모델의 오용**: 선형 회귀 모델을 예로 들면, 많은 연구들이 모델이 데이터에 적합한지 확인하지 않고 5% 유의수준에서 계수의 유의성만을 근거로 결론을 도출했습니다. 이는 성별 차별 연구 사례에서 보듯이 잘못된 결론을 초래할 수 있습니다.[1]

**새로운 데이터의 출현**: 테라바이트 규모의 데이터가 천문학, 유전학 등 다양한 분야에서 생성되고 있으며, 이러한 복잡한 데이터를 분석하기 위해서는 전통적인 데이터 모델이 적용하기 어렵습니다.[1]

**예측 정확도의 중요성**: Breiman은 교차 검증(cross-validation)이나 테스트 세트를 통한 예측 정확도 평가가 모델 적합성을 평가하는 더 객관적인 방법이라고 주장합니다. 그러나 당시 Journal of the American Statistical Association(JASA)의 논문들은 예측 정확도를 거의 보고하지 않았습니다.[1]

## 3. 연구 주제, 방법(수식), 결과

### 연구 주제

Breiman의 연구는 다음 세 가지 핵심 주제를 다룹니다:[1]

**Rashomon 효과**: 거의 동일한 예측 오차를 가진 여러 모델이 존재할 수 있다는 개념입니다. 예를 들어, 30개의 변수 중 5개를 선택하는 선형 회귀에서 약 140,000개의 조합이 가능하며, 이 중 많은 조합이 비슷한 잔차 제곱합(RSS)을 가질 수 있습니다.[1]

**Occam의 딜레마**: 단순성과 정확도 간의 충돌입니다. 의사결정 트리는 해석 가능성(interpretability)에서 A+를 받지만 예측에서는 B 수준입니다. 반면, 랜덤 포레스트는 예측에서 A+를 받지만 해석 가능성에서는 F를 받습니다.[1]

**Bellman과 차원의 저주**: 전통적으로 고차원은 위험하다고 여겨졌으나, Breiman은 차원을 증가시키는 것이 오히려 축복이 될 수 있다고 주장합니다. Support Vector Machines와 Shape Recognition Forest가 이를 보여주는 사례입니다.[1]

### 방법과 수식

**랜덤 포레스트(Random Forests)**

Breiman은 여러 데이터셋에서 단일 트리(CART)와 랜덤 포레스트를 비교했습니다. 랜덤 포레스트는 훈련 세트에 대한 부트스트랩 샘플을 사용하여 각 트리를 구성하며, 각 노드에서 입력 변수의 무작위 선택을 통해 최적의 분할을 찾습니다.[1]

**변수 중요도 측정**: $$m$$번째 변수의 중요도를 측정하기 위해, 현재 부트스트랩 샘플에서 제외된 모든 케이스에서 $$m$$번째 변수의 값을 무작위로 순열(permute)합니다. 이러한 케이스들을 현재 트리에 적용하고 분류를 기록합니다. 여러 트리를 성장시킨 후, 각 변수를 노이즈 처리함으로써 발생하는 오분류율의 증가율을 계산합니다.[1]

**Support Vector Machines**

2-클래스 데이터가 M차원 유클리드 공간에서 초평면으로 분리될 수 있다면, 최적 분리 초평면(optimal separating hyperplane)이 존재합니다. 분리가 발생하지 않으면 차원을 증가시킵니다. 예를 들어, 원래 예측 변수에 2차 단항식(quadratic monomials)을 추가하면 초평면이 더 복잡한 형태가 되어 분리 가능성이 높아집니다.[1]

**일반화 오차의 경계**: Vapnik의 이론에 따르면:[1]

$$ E_x[GE] \leq E_x[\text{number of support vectors}]/(N-1) $$

여기서 $$N$$은 샘플 크기이며, 기대값은 원래 훈련 세트와 동일한 기저 분포에서 추출된 크기 $$N$$의 모든 훈련 세트에 대한 것입니다.

**비교 분포(Comparison Distribution)**

두 분포 $$F$$와 $$G$$의 비교를 위해 비교 분포 $$D(u|F,G)$$와 비교 밀도 $$d(u|F,G) = D'(u|F,G)$$를 정의합니다[1]. $$F, G$$가 연속일 때:

$$ D(u|F,G) = G(F^{-1}(u)) $$
$$ d(u|F,G) = g(F^{-1}(u))/f(F^{-1}(u)) $$

### 연구 결과

**예측 정확도 비교**

Table 2는 10개의 데이터셋에서 단일 트리와 랜덤 포레스트의 테스트 세트 오분류 오차를 비교합니다:[1]

- **Breast Cancer**: 랜덤 포레스트 2.9% vs. 단일 트리 5.9% (50% 감소)
- **Ionosphere**: 랜덤 포레스트 5.5% vs. 단일 트리 11.2% (51% 감소)
- **Letters**: 랜덤 포레스트 3.4% vs. 단일 트리 12.4% (73% 감소)
- **Shuttle**: 랜덤 포레스트 0.007% vs. 단일 트리 0.062% (89% 감소)
- **Digit**: 랜덤 포레스트 6.2% vs. 단일 트리 17.1% (64% 감소)

**Statlog 프로젝트 비교**

Statlog 프로젝트는 18개의 분류기를 비교했으며, 랜덤 포레스트는 4개의 데이터셋에서 모두 1위를 차지하여 평균 순위 1.0을 기록했습니다. 다음 최고 분류기의 평균 순위는 7.3이었습니다.[1]

**의료 데이터 분석 사례**

**간염 데이터셋(Example I)**: 155명의 간염 환자 데이터에서 19개의 공변량을 사용한 분석 결과:[1]
- 로지스틱 회귀: 17.4% 오차율
- 랜덤 포레스트: 12.3% 오차율 (약 30% 감소)
- 랜덤 포레스트는 변수 12번과 17번이 가장 중요하다고 식별했으며, 각각 단독으로 사용 시 14.3%의 오차율을 보였습니다

**Bupa 간 데이터셋(Example II)**: 345명의 환자를 대상으로 한 2-클래스 생물의학 데이터셋에서 랜덤 포레스트는 28%의 오분류 오차율을 기록했으며, 클래스 2의 환자들이 두 개의 구별되는 그룹으로 나뉜다는 흥미로운 사실을 발견했습니다.[1]

**마이크로어레이 데이터(Example III)**: 81개 샘플, 4,682개 변수(유전자)를 가진 림프종 데이터셋에서 랜덤 포레스트는 변수 선택 없이도 낮은 오차율을 달성했으며, 각 유전자 발현의 중요도를 추정할 수 있었습니다.[1]

## 4. 결론 및 후속 연구 방향

### 저자가 제시한 시사점

Breiman은 통계학의 목표가 데이터를 사용하여 예측하고 기저 데이터 메커니즘에 대한 정보를 얻는 것이라고 강조합니다. 그는 다음과 같은 핵심 시사점을 제시합니다:[1]

**모델 선택의 유연성**: "어떤 종류의 모델을 사용해야 하는지는 돌판에 새겨져 있지 않습니다. 문제와 데이터에 초점을 맞춰야 합니다". 데이터 모델이 때로는 가장 적절한 해결책일 수 있지만, 문제와 데이터가 우선되어야 합니다.[1]

**예측 정확도와 정보의 관계**: "더 높은 예측 정확도는 기저 데이터 메커니즘에 대한 더 신뢰할 수 있는 정보와 연관되어 있습니다. 약한 예측 정확도는 의심스러운 결론을 초래할 수 있습니다".[1]

**알고리즘 모델의 정보 제공 능력**: 알고리즘 모델이 데이터 모델보다 더 나은 예측 정확도를 제공할 수 있으며, 기저 메커니즘에 대한 더 나은 정보를 제공할 수 있습니다.[1]

**통계학의 미래 방향**: Breiman은 통계학이 과학과 마찬가지로 데이터와 함께 작업하고 데이터에 대한 이론을 검증하는 것에 뿌리를 두고 있다고 강조합니다. 그는 "이번 세기에 우리 분야가 그 뿌리로 돌아가기를 희망한다"고 말합니다.[1]

**학제간 협력**: 지난 10년간 실제 문제에 대한 통계적 작업과 다른 분야와의 협력을 향한 통계학자들의 움직임이 있었으며, 이러한 추세가 계속되어야 한다고 주장합니다.[1]

### 후속 연구 계획

논문에서 Breiman은 직접적인 후속 연구 계획을 명시하지는 않았지만, 여러 연구 방향을 암시합니다:[1]

**트리 앙상블 이론**: Breiman은 자신의 마지막 논문 "Some infinity theory for tree ensembles"에서 함수 공간 분석(function space analysis)을 사용하여 트리 앙상블 방법의 작동 원리를 이해하려고 시도했습니다. 그는 "좋은 이론을 위해 내 왕국을 바치겠다"는 제목의 섹션을 포함시켰으며, 부스팅(boosting)이 왜 그렇게 잘 작동하는지에 대한 유한 표본 크기 이론이 없다고 언급했습니다.[1]

**고차원 데이터 분석**: 차원의 저주를 축복으로 바꾸는 방법론의 추가 개발이 필요합니다.[1]

**해석 가능성과 정확도의 균형**: Occam의 딜레마를 해결하기 위한 연구가 필요합니다.[1]

### 추가 후속 연구 방향 제안

**설명 가능한 AI(Explainable AI, XAI)**: 랜덤 포레스트와 같은 복잡한 앙상블 모델의 해석 가능성을 향상시키기 위한 연구가 필요합니다. SHAP(SHapley Additive exPlanations), LIME(Local Interpretable Model-agnostic Explanations) 같은 기법을 통해 블랙박스 모델의 의사결정 과정을 이해할 수 있는 방법론을 개발해야 합니다.

**하이브리드 모델링 접근법**: 데이터 모델과 알고리즘 모델의 장점을 결합한 하이브리드 접근법을 개발할 수 있습니다. 예를 들어, 물리적 법칙이나 도메인 지식을 신경망에 통합하는 Physics-Informed Neural Networks(PINNs)가 이러한 방향의 예시입니다.

**인과 추론과 예측 모델의 통합**: Breiman의 논문은 주로 예측에 초점을 맞추었지만, 인과 관계를 이해하는 것도 중요합니다. 알고리즘 모델을 사용하면서도 인과 효과를 추정할 수 있는 방법론(예: Causal Forests)에 대한 연구가 필요합니다.

**대규모 데이터와 실시간 처리**: 테라바이트 규모의 데이터에 대한 실시간 분석을 위한 확장 가능한 알고리즘 모델 개발이 필요합니다. 분산 컴퓨팅과 스트리밍 데이터 처리를 지원하는 랜덤 포레스트 및 기타 앙상블 방법의 개발이 필요합니다.

**모델 안정성과 강건성**: Rashomon 효과가 시사하는 모델의 불안정성 문제를 해결하기 위한 연구가 필요합니다. 배깅(bagging)과 같은 앙상블 방법이 이 문제를 부분적으로 해결할 수 있지만, 더 체계적인 접근이 필요합니다.

**도메인 특화 알고리즘**: 의료, 금융, 천문학 등 특정 분야의 특성을 고려한 맞춤형 알고리즘 모델을 개발해야 합니다. 예를 들어, 의료 영상 분석을 위한 딥러닝 모델이나 유전체 데이터 분석을 위한 특화된 랜덤 포레스트 변형이 필요합니다.

**불확실성 정량화**: 알고리즘 모델의 예측에 대한 불확실성을 정량화하는 방법론을 개발해야 합니다. Conformal Prediction이나 Bayesian Neural Networks와 같은 접근법이 이 방향의 예시입니다.

**공정성과 편향 완화**: 알고리즘 모델이 사회적으로 공정하고 편향되지 않은 결정을 내릴 수 있도록 하는 방법론을 개발해야 합니다. 특히 의료 진단, 신용 평가, 채용 결정 등 민감한 응용 분야에서 이는 매우 중요합니다.

**AutoML과 자동화된 모델 선택**: 데이터 과학자가 아닌 도메인 전문가도 알고리즘 모델을 효과적으로 사용할 수 있도록 자동화된 모델 선택 및 하이퍼파라미터 튜닝 기법을 개발해야 합니다.

**지속 학습(Continual Learning)**: 시간에 따라 변화하는 데이터 분포에 적응할 수 있는 알고리즘 모델을 개발해야 합니다. 특히 금융 시장이나 기후 예측과 같이 비정상적(non-stationary) 환경에서 중요합니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/50a8c52f-3eb0-41dc-8c82-0b32a0490244/1009213726.pdf)
