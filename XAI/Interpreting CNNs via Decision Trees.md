# Interpreting CNNs via Decision Trees

# 핵심 요약 및 주요 기여

**“Interpreting CNNs via Decision Trees”** 논문은 사전 학습된 합성곱 신경망(CNN)의 각 예측에 대해 **고수준의 의미론적 설명 및 정량적 기여 분석**을 제공하는 새로운 해석 프레임워크를 제안한다.  
- CNN의 고차원 특징 표현을 **객체 부위(part)** 개념으로 **분해**하고, 각 필터가 어떤 부위 정보를 어떻게 활용하는지를 정량적으로 측정  
- 이러한 부위 기반 설명을 **의사 결정 트리** 구조로 조직하여, 예측 합리성(rationale)을 **조밀한 일반 모드(coarse)가 가장 세부 모드(fine)** 순으로 계층적으로 제공  
- **비지도(annot-free)** 방식으로 필터를 부위별로 분리(disentangle)하고, FC 계층을 **선형 근사**하여 결정 모드(rationales)를 추출 및 통합  

# 문제 정의 및 제안 방법

## 해결하고자 하는 문제  
CNN은 탁월한 성능에도 불구하고 **내부 결정 과정을 투명하게 설명**하기 어려움.  
- 중간 계층 필터(feature)와 의미론적 개념(객체 부위) 간 연결 부족  
- 특정 입력 이미지 예측 시 어떤 필터·부위가 얼마나 기여했는지 정량적 분석 부재

## 전처리: 필터 분리(Disentangled Filters)  
논문은 **필터 손실(filter loss)**을 도입하여 최상위 합성곱 계층 필터 $$x_f$$가 단일 객체 부위에만 반응하도록 학습.[1]

$$
\mathcal{L}_f = - \mathrm{MI}\bigl(P(X_f),\,P(\Omega)\bigr)
$$  

- $$X_f$$: 다양한 입력에서 추출된 필터 $$f$$의 특징 맵  
- $$\Omega$$: 부위 위치 후보 집합  
- MI: 상호정보량(mutual information) 최소화를 통해 **단일 위치 활성화** 유도[1]

## 예측 합리화: 선형 근사  
최상위 conv–ReLU–FC 계층 연쇄를 **조각별 선형**으로 근사하여, 입력 이미지 $$I$$의 예측 점수 $$y$$를  

$$
y \approx g^\top x + b
$$  

로 표현.[1]
- $$x$$: 필터별 활성화 강도 벡터  
- $$g$$: 입력 특이 가중치 벡터로, 필터별 **정량적 기여도**  
- 각 차원 $$g_d x_d$$는 필터(부위) $$d$$의 기여  

## 의사 결정 트리 학습  
1. **각 이미지**에 대해 합리화 벡터 $$g_i$$를 잎 노드로 초기화  
2. **가장 유사 합리화** 쌍 노드 병합(merging) 반복  
3. 각 내부 노드는 **공통합리화(common rationale)** 모드로 학습:  

$$
   \min_{w,b}\sum_{i\in v} \|w^\top x_i + b - y_i\|^2 + \lambda\|w\|_1
   $$  

4. 트리 상층부는 **광범위한 일반 모드** 제공, 하층부는 **세부 모드** 제공[1]

# 모델 구조 및 수식

- **Disentangled CNN:** 기존 CNN에 필터 손실 모듈 추가  
- **근사 방정식:**  

$$
  y = g^\top x + b,\quad x_d = \frac{1}{s_d}\sum_{h,w}x_{h,w,d},\quad g_d = s_d\sum_{h,w}\frac{\partial y}{\partial x_{h,w,d}}
  $$  

- **의사결정트리 병합:** 두 노드 $$v, u$$ 병합 시 공통 가중치 $$g$$ 찾기  

$$
  \max_{g:\|g\|=1}\sum_{i\in v\cup u}\cos(g_i, g)
  $$

# 성능 평가 및 한계

- **기여도 오차**(object-part contribution error): 2nd 레이어 기준 평균 0.013 (CUB200-2011, 비율 오차)[1]
- **분포 적합도**(fitness): 2nd 레이어 평균 0.23–0.47 across architectures[d][1]
- **분류 정확도 손실**: CNN 대비 2nd 레이어 1.5–5.2% 감소, 세부 모드(leaf) 시 9–11% 감소[1]
- **제한점**  
  - 필터 손실만으로 완전한 부위 분리는 어려워 일부 오분류 존재  
  - 결정 모드의 희소화(sparsity)로 비주류 부위 기여 간과 가능  
  - ResNet 등 skip-connection 네트워크는 별도 구조 조정 필요  

# 일반화 성능 향상 가능성

의사결정트리는 **모델의 의사결정 패턴**을 명시적으로 추출하므로 다음과 같은 방식으로 **일반화 강화**에 기여할 수 있음:  
- **데이터 증강 및 샘플 선택:** 주요 모드에서 소수 모드까지 계층적 분류 모드를 활용해 **희귀 사례 강조 학습**  
- **오류 분석 및 편향 완화:** 일반 vs. 세부 모드 간 오차 비교로 부위 편향 감지 및 조정  
- **전이학습:** 다른 데이터셋·유사 태스크에서 공통 모드를 기반으로 **사전학습된 설명 구조** 활용  

# 향후 연구 영향 및 고려사항

- **설명 가능 AI(XAI) 발전:** 세부 모드까지 정량·의미 해석 가능한 설명 트리로 **모델 투명성** 강화  
- **다양한 네트워크 구조 적용:** 트랜스포머·Residual 네트워크 등으로 확장 연구 필요  
- **강화된 분리 손실:** 필터 분리에 부위 주석 일부 활용해 **해석 정확도** 개선  
- **실제 응용:** 의료·자동차·금융 분야에서 **결정 근거 투명성** 제공, 신뢰성·규제 준수 기여  

해당 연구는 CNN 예측의 **내부 의사결정 구조**를 해석 가능하게 함으로써, 모델 신뢰성 확보 및 일반화 성능 강화의 새로운 방향을 제시한다.[1]

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/d2a5269e-8ac0-4ef2-bc46-cb76d5d3177b/1802.00121v2.pdf)
