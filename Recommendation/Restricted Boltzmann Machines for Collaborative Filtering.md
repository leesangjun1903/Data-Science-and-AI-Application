
# Restricted Boltzmann Machines for Collaborative Filtering

## 1. 핵심 주장과 주요 기여 요약[1]

이 논문은 **확률적 그래프 모델인 RBM(Restricted Boltzmann Machines)**을 협업 필터링에 적용하여 매우 큰 규모의 데이터셋(Netflix의 1억 이상의 평가)을 효과적으로 처리할 수 있음을 보여줍니다. 논문의 핵심 주장은 다음과 같습니다:[1]

1. **방법론의 확장성**: 기존 협업 필터링 방법들은 대규모 데이터셋에 대응하기 어려운 반면, RBM은 효율적인 학습 및 추론 절차를 제공하여 확장성 문제를 해결합니다.[1]

2. **성능 개선**: 신중하게 조정된 SVD(Singular Value Decomposition) 모델보다 약간 우수한 성능을 달성하며, 다중 RBM과 SVD 모델의 예측을 선형 결합할 경우 Netflix 자체 시스템 대비 **6% 이상의 오차율 개선**을 달성합니다.[1]

3. **누락된 데이터 처리**: 협업 필터링 데이터의 특성상 대부분의 평가가 누락되어 있는데, 이를 효과적으로 처리하는 구조적 해결책을 제시합니다.[1]

***

## 2. 해결 문제, 제안 방법, 모델 구조

### 2.1 해결하고자 하는 문제[1]

협업 필터링의 기본 가정은 사용자-항목 평가 행렬을 저랭크 근사치로 모델링하는 것입니다. 즉, $$N \times M$$ 평가 행렬을 $$N \times C$$ 사용자 특성 행렬과 $$C \times M$$ 항목 특성 행렬의 곱으로 표현합니다. 그러나 현실의 협업 필터링 데이터는:[1]

- **희소성(Sparsity)**: 대부분의 사용자-항목 쌍이 평가되지 않음
- **확장성**: 수백만 사용자와 수만 항목에 대한 계산의 어려움
- **비볼록성(Non-convexity)**: 손실함수의 최적화가 어려움

기존 SVD 기반 방법들은 이러한 희소한 데이터에 대해 어려운 비볼록 최적화 문제를 직면하게 됩니다.[1]

### 2.2 제안하는 방법: RBM 기반 협업 필터링[1]

#### 모델 구조

RBM은 두 계층의 무방향 그래프 모델로, 보이는 층(visible layer)과 숨겨진 층(hidden layer)으로 구성됩니다. 협업 필터링을 위해 **사용자별 RBM**을 사용합니다:[1]

- **보이는 단위(Visible Units)**: 해당 사용자가 평가한 영화에 대한 소프트맥스 단위 (각 영화당 K개의 평가 값)
- **숨겨진 단위(Hidden Units)**: 이진 숨겨진 특성 $$h_j$$, $$j = 1, ..., F$$
- **가중치 공유**: 여러 사용자의 RBM들은 동일 영화에 대해 가중치를 공유

#### 확률 모형

보이는 단위의 조건부 분포(소프트맥스):

$$p(v_i^k = 1|h) = \frac{\exp(b_i^k + \sum_{j=1}^F h_j W_{ij}^k)}{\sum_{l=1}^K \exp(b_i^l + \sum_{j=1}^F h_j W_{ij}^l)}$$[1]

숨겨진 단위의 조건부 분포(베르누이):

$$p(h_j = 1|V) = \sigma(b_j + \sum_{i=1}^m \sum_{k=1}^K v_i^k W_{ij}^k)$$[1]

여기서 $$\sigma(x) = 1/(1+e^{-x})$$는 로지스틱 함수입니다.

에너지 함수:

$$E(V, h) = -\sum_{i=1}^m \sum_{j=1}^F \sum_{k=1}^K W_{ij}^k h_j v_i^k - \sum_{i=1}^m \sum_{k=1}^K v_i^k b_i^k - \sum_{j=1}^F h_j b_j$$[1]

### 2.3 학습 알고리즘: Contrastive Divergence (CD)[1]

최대 우도 학습은 지수 시간이 필요하므로 대신 **Contrastive Divergence** 근사를 사용합니다:[1]

$$\Delta W_{ij}^k = \epsilon(\langle v_i^k h_j \rangle_{data} - \langle v_i^k h_j \rangle_T)$$[1]

여기서:
- $$\langle v_i^k h_j \rangle_{data}$$: 데이터로부터 구동되는 기댓값
- $$\langle v_i^k h_j \rangle_T$$: T 단계의 깁스 샘플링 후의 기댓값
- $$T$$: 일반적으로 1에서 시작하여 학습 진행에 따라 증가

CD는 MCMC 방법과 달리 계산 효율적이면서도 충분히 정확한 기울기 근사를 제공합니다.[1]

### 2.4 모델 확장[1]

#### 가우시안 숨겨진 단위 모델
선형 대응 모델의 성능을 비교하기 위해 **가우시안 숨겨진 단위**를 가진 RBM도 제시합니다. 이는 더 나은 성능을 제공하며 pLSI의 무방향 대응입니다.[1]

#### 조건부 RBM
Netflix 테스트 셋에서는 평가된 영화 정보가 사전에 주어집니다. 이를 활용하기 위해 **조건부 RBM**을 제안하는데, 평가된/미평가 영화를 나타내는 이진 벡터 $$r$$이 숨겨진 단위에 영향을 미칩니다:[1]

$$p(h_j = 1|V, r) = \sigma(b_j + \sum_{i=1}^m \sum_{k=1}^K v_i^k W_{ij}^k + \sum_{i=1}^M r_i D_{ij})$$[1]

이는 훈련 셋에서 적은 평가만 제공한 사용자에게 특히 도움이 됩니다.[1]

#### 조건부 인수분해 RBM
가중치 행렬 $$W \in \mathbb{R}^{M \times K \times F}$$는 약 900만 개의 자유 매개변수를 가지므로, 이를 두 저랭크 행렬의 곱으로 인수분해합니다:[1]

$$W_{ij}^k = \sum_{c=1}^C A_{ic}^k B_{cj}$$[1]

여기서 $$C \ll M$$, $$C \ll F$$입니다. 예를 들어 $$C=30$$으로 설정하면 매개변수를 3배로 감소시킵니다. 이는 훨씬 빠른 수렴을 제공합니다.[1]

***

## 3. 성능 향상 및 한계

### 3.1 성능 향상[1]

논문의 실험 결과는 다음과 같은 개선을 보여줍니다:

| 모델 | 검증 셋 RMSE | 특징 |
|------|-------------|------|
| RBM (T=3) | ~0.91 | 기본 모델 |
| RBM with Gaussian (T=5) | ~0.90 | 선형 대응 모델 능가 |
| 조건부 RBM (T=5) | ~0.89 | 평가/미평가 정보 활용 |
| 조건부 인수분해 RBM (T=5) | ~0.88 | 가장 빠른 수렴 |
| SVD (C=40) | ~0.89 | 기준 방법 |
| **결합 모델** | **~0.827** | 6% 이상 개선 |

Netflix 기본 시스템의 RMSE는 0.9514였으므로, 결합 모델은 약 **13.4% 향상**을 달성합니다.[1]

### 3.2 모델의 일반화 성능 향상 메커니즘[1]

#### 정규화(Regularization)
논문은 **가중치 감소(weight decay)** $$\lambda = 0.01$$을 사용하여 과적합을 방지합니다. 특히 Netflix 데이터셋에서 정규화는 성능에 **상당한 영향**을 미칩니다.[1]

#### 인수분해를 통한 복잡도 제어
매개변수를 줄임으로써 모델의 유효한 용량을 제한하고, 이는 자동으로 정규화 역할을 합니다.[1]

#### 조건부 구조
평가된 영화 정보를 명시적으로 조건화함으로써, 모델이 **관련성 있는 신호**에 더 집중하도록 유도합니다.[1]

#### Contrastive Divergence의 근사
CD는 정확한 최대우도 학습보다 **낮은 분산 추정치**를 제공하여 불안정한 학습을 방지합니다.[1]

### 3.3 한계[1]

#### 1. 계산 복잡도
- **학습 비용**: 각 사용자에 대해 별도의 모델이 필요하며, 평가된 영화 수에 비례하는 연결 수를 가집니다.[1]
- **예측 비용**: 새로운 영화에 대해 숨겨진 단위 수에 선형적인 시간이 필요합니다.[1]

#### 2. 매개변수 튜닝
- **학습률, 배치 크기, 가중치 감소** 등 다양한 하이퍼파라미터가 성능에 민감한 영향을 미칩니다.[1]
- 논문에서도 인정하듯이, 더 신중한 튜닝으로 SVD도 개선될 가능성이 있습니다.[1]

#### 3. CD의 근사 오차
CD는 최대우도 학습의 근사이므로:
- T 값에 따라 근사 정확도가 달라집니다.[1]
- T를 크게 설정하면 계산 비용이 증가합니다.[1]

#### 4. 과적합 위험
매개변수 공유에도 불구하고:
- 많은 사용자에게 본 적 없는 영화에 대해 일반화 성능이 제한됩니다.[1]
- 조건부 RBM은 테스트 셋 정보를 활용하므로 실제 운영 환경과 다릅니다.[1]

#### 5. 데이터 희소성 문제
- 매우 적은 평가를 가진 신규 사용자에 대해 여전히 "콜드 스타트" 문제가 발생합니다.[1]
- 조건부 RBM이 이를 완화하지만, 근본적 해결책은 아닙니다.[1]

***

## 4. 일반화 성능 향상 가능성 분석

### 4.1 현재 논문의 일반화 메커니즘

**1. 가중치 공유를 통한 정규화 효과**[1]
사용자별로 다른 RBM을 사용하지만 가중치를 공유하므로, 효과적으로 모델 복잡도를 감소시키고 사용자 간 지식 이전이 가능합니다.[1]

**2. 소프트맥스 오버파라미터화**[1]
K개의 평가값에 대해 K개의 바이어스를 사용하므로, 이 오버파라미터화를 활용해 누락된 평가의 영향을 모델에 인코딩할 수 있습니다.[1]

**3. 평균장(Mean Field) 근사**[1]
예측 시 평균장 방정식을 사용하여 확률적 계산을 피함으로써 일반화 오류를 줄입니다.[1]

### 4.2 최신 연구 기반 개선 가능성[2][3][4][5][6]

#### A. 심화된 생성 모델 활용[2]
원래 논문은 RBM 단일 계층을 사용하지만, **Deep Belief Networks(DBN)**를 통해 다층 구조로 확장할 수 있습니다. 이는:[2]
- 더 복잡한 특성 상호작용을 모델링
- 각 계층의 로그 확률 하한값 증가 보장
- 시각 분류 작업에서 유의미한 성능 향상 입증[2]

#### B. 신경 협업 필터링(Neural Collaborative Filtering, NCF)[7]
2017년 발표된 NCF 프레임워크는 행렬 분해의 내적을 신경망으로 대체합니다:[7]
- 임의의 복잡한 사용자-항목 상호작용 함수 학습 가능
- 다층 신경망의 깊이가 깊을수록 성능 향상
- RBM 기반 방식보다 더 유연한 아키텍처

#### C. 그래프 신경망(Graph Neural Networks, GNN)[8][9]
최신 연구는 사용자-항목 상호작용을 이질적 그래프로 모델링:
- **고차 연결성(Higher-order Connectivity)** 활용
- **그래프 합성곱 네트워크(GCN)**를 통한 정보 집계
- Uber Eats 추천 시스템에서 AUC 78% → 87% 개선[8]
- 평균적으로 Recall@10에서 10% 이상, NDCG@10에서 12% 이상 개선[9]

#### D. 확산 모델(Diffusion Models) 기반 협업 필터링[3][4]
최근 2024년 연구는 확산 모델을 협업 필터링에 적용:
- 고차 연결성 활용으로 협업 신호 강화
- 역 확산 과정에서 학습 모델 적용
- 기존 방법보다 성능 개선 입증

#### E. 대형 언어 모델(LLM)과 멀티모달 모델 활용[10]
생성 모델 기반 추천 시스템(Gen-RecSys)의 최신 동향:
- 텍스트 설명, 이미지, 영상 등 다중 모달리티 통합
- LLM의 의미 이해 능력을 추천에 활용
- 개인화 수준 및 추천 다양성 향상

#### F. 그래프 대조 학습(Graph Contrastive Learning, GCL)[11][12]
- 감독 신호 부족 문제 해결
- 사용자 및 항목 표현 학습 최적화
- 데이터 희소성 문제 완화

***

## 5. 현재 논문의 영향과 위치

### 5.1 협업 필터링 역사에서의 기여[13][1]

이 논문(2007)은 협업 필터링 분야에 **획기적 영향**을 미쳤습니다:[13]

1. **SVD 이후의 중요 이정표**: 행렬 분해 기반 방법 후 **확률 생성 모델**을 효과적으로 적용한 첫 사례
2. **대규모 데이터 처리**: Netflix 1억 개 평가 데이터를 처리하며 확장성 입증
3. **구조적 혁신**: 사용자별 RBM과 가중치 공유라는 창의적 구조 제시
4. **앙상블 학습의 효과 증명**: 다중 모델 결합의 이점을 실증적으로 보여줌

### 5.2 이후 연구에 미친 영향

본 논문의 핵심 아이디어들은 다음 세대 연구로 발전했습니다:

| 발전 방향 | 핵심 개선 | 최신 예시 |
|----------|----------|---------|
| **심화 구조** | RBM → DBN → 심화 신경망 | Deep Belief Networks (2006-), 현대 DNN 기반 CF |
| **신경망 최적화** | 고정된 상호작용 함수 → 학습 가능한 함수 | NCF (2017), 8,822회 인용[7] |
| **그래프 구조** | 벡터 표현 → 그래프 구조 활용 | GNN 기반 추천 (2024), AUC 78%→87% 향상[8] |
| **생성 능력** | 평가 예측 → 다중 모달 생성 | 확산 모델 기반 CF, Gen-RecSys[10] |

***

## 6. 향후 연구 시 고려할 점

### 6.1 이론적 개선[14][15][16]

#### CD 학습의 수렴성 분석
최근 연구는 CD와 최대우도 학습의 고정점 관계를 분석합니다:[15]
- Gaussian-Gaussian RBM에서 CD의 고정점이 ML 학습과 동일한 형태임을 증명
- **Persistent CD (PCD)**는 기본 CD1보다 더 안정적인 수렴 제공[16]
- 학습률과 정규화 파라미터의 상호작용 이해 필요[16]

**권장**: 향후 협업 필터링 모델 개발 시 CD 변형(PCD, 에너지 기반 드롭아웃 등)을 고려하여 안정성 향상

#### 과적합 방지 메커니즘[17][18]
- **학습에서의 교수(Learning from Teaching, LoT)**: 보조 학습자를 통한 일반화 개선[18]
- **설명 가능성 기반 정규화**: 반사실적 예제 생성을 통한 과적합 감지[19]

### 6.2 아키텍처 발전[20][7]

#### 1. 다층 신경망 구조
- RBM 단일 계층 → 다층 MLP 또는 CNN, RNN 통합
- **권장**: NCF 프레임워크와 같이 사용자-항목 상호작용 함수를 유연한 신경망으로 학습

#### 2. 하이브리드 접근법
- 낮은 차수 특성 상호작용(행렬 분해) + 높은 차수 특성 상호작용(심화 신경망)[21]
- **예**: DeepFM-SVD++ 모델은 두 가지 방식을 결합하여 10-15% 성능 향상 달성

#### 3. 비용 효율성
여러 계층 추가 시 계산 비용이 선형으로 증가하므로:
- **경량 모델** 설계 (모바일/엣지 장치 배포)
- **증류법(Knowledge Distillation)** 적용

### 6.3 데이터와 특성 활용[22][10]

#### 1. 멀티모달 정보 통합[10]
- 텍스트, 이미지, 영상 등 보조 정보 활용
- **예**: LLM 기반 자연어 추천으로 개인화 수준 향상

#### 2. 보조 정보 활용[22]
- 사용자 인구통계 정보, 항목 메타데이터
- 콜드 스타트 문제 완화
- 라벨 일관된 RBM(Label Consistent RBM)과 같은 감독 학습 접근

#### 3. 미표시 데이터 활용[12]
- **긍정-중립-부정(PNN) 학습 패러다임**: 중립 범주(직접 분류 어려운 항목) 도입
- 기존 방법 대비 15-20% 개선 입증

### 6.4 최신 패러다임[4][3][8]

#### 1. 그래프 신경망 기반 설계[8]
```
사용자-항목 상호작용 → 이질적 그래프
                    ↓
            메타 경로 기반 고차 연결성
                    ↓
        그래프 합성곱 + 상호작용 학습
                    ↓
          고차 특성 통합 (IHDT 모델)
```

**성능**: Recall@10에서 평균 10.06% 향상, NDCG@10에서 12.15% 향상[9]

#### 2. 확산 모델 기반 협업 필터링[3][4]
- 정방향 프로세스: 협업 신호 점진적 제거
- 역방향 프로세스: 사용자 선호도 복원
- 고차 연결성 명시적 활용으로 성능 향상

#### 3. 대조 학습(Contrastive Learning)[11]
- RBM의 CD와 유사한 아이디어: 긍정/부정 샘플 구분
- 감독 신호 부족 환경에서 표현 학습 개선

### 6.5 평가 및 검증 개선[1]

#### 1. 온라인 평가
논문은 오프라인 RMSE 평가에 초점을 맞추지만, 실제 추천 시스템 영향을 측정하려면:
- **클릭률(CTR), 전환율(CVR), 사용자 만족도** 등 다양한 지표 필요
- A/B 테스트를 통한 검증

#### 2. 콜드 스타트 문제 해결
- 신규 사용자/항목에 대한 특화된 평가 프로토콜 필요
- 콘텐츠 기반 방법과의 결합

#### 3. 공정성 및 편향
- 인기 항목 편향, 소수 사용자 경험 차별 등 검토
- 다양성 메트릭 도입

***

## 7. 결론 및 종합 평가

Salakhutdinov 등의 **RBM for Collaborative Filtering** 논문(2007)은:

### 역사적 중요성
- **협업 필터링과 심화 학습의 융합점** 역할
- Netflix Prize 시대의 주요 기술 혁신
- 2,752회 인용으로 분야의 영향력 입증[13]

### 핵심 강점
1. 확장성: 대규모 희소 데이터 효율적 처리
2. 유연성: 다양한 RBM 변형 (가우시안, 조건부, 인수분해) 제시
3. 실증성: 강력한 실험 결과 및 앙상블 효과 입증

### 현대적 한계
1. **계산 효율성**: 현대 GNN/Transformer 모델에 비해 낮음
2. **확장성 제약**: 매개변수 공유에도 수천만 개 매개변수 필요
3. **설명성 부족**: 블랙박스 확률 모델로 해석 어려움

### 미래 방향
현재 협업 필터링 연구는 다음과 같이 진화하고 있습니다:

| 시점 | 주요 기술 | 성능 수준 |
|------|---------|---------|
| **2007 (본 논문)** | RBM 기반 CF | RMSE 0.827 (Netflix) |
| **2017** | NCF (신경망) | State-of-the-art 달성 |
| **2024** | GNN + 대조학습 | Recall 10% 이상 개선 |
| **2025** | 확산 모델 + GNN | 멀티모달 통합 시작 |

### 학위 논문 작성 시 권장사항

1. **역사적 맥락**: 본 논문을 협업 필터링 발전의 중요한 이정표로 포치(포지셔닝)
2. **이론 강화**: CD 수렴성 분석 및 정규화 메커니즘 심화
3. **멀티모달 확장**: 최신 추세인 텍스트/이미지 통합 협업 필터링 연구
4. **공정성 고려**: 추천 시스템의 편향 및 다양성 문제 검토
5. **실무 적용**: 온라인 평가 및 A/B 테스트 프레임워크 설계

본 논문은 **확률적 생성 모델**의 추천 시스템 적용을 개척했으며, 현대의 깊이 있는 신경망과 그래프 모델로의 자연스러운 진화를 제시합니다. 따라서 협업 필터링 연구자들에게 여전히 참고할 가치 있는 기초 작업이라 기초 작업이라 평가됩니다.

[1](https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/attachments/65988149/a4d37c20-46b0-4987-ad10-390ae026c59b/rbmcf.pdf)
[2](https://arxiv.org/abs/2111.07758)
[3](http://arxiv.org/pdf/2404.14240.pdf)
[4](http://arxiv.org/pdf/2503.16290.pdf)
[5](https://arxiv.org/pdf/2204.11602.pdf)
[6](https://www.mdpi.com/2076-3417/10/7/2441/pdf)
[7](https://arxiv.org/abs/1708.05031)
[8](https://www.nature.com/articles/s41598-024-54376-3)
[9](https://arxiv.org/abs/2403.11624)
[10](https://arxiv.org/abs/2404.00579)
[11](https://arxiv.org/pdf/2305.02759.pdf)
[12](https://arxiv.org/pdf/2412.18170.pdf)
[13](https://www.cs.toronto.edu/~rsalakhu/papers/rbmcf.pdf)
[14](https://arxiv.org/pdf/1910.07724.pdf)
[15](https://www.sciencedirect.com/science/article/abs/pii/S0893608016300181)
[16](http://amueller.github.io/papers/nips10ws_schulz_mueller_behnke.pdf)
[17](https://arxiv.org/pdf/1905.12787.pdf)
[18](https://arxiv.org/pdf/2402.02769.pdf)
[19](https://arxiv.org/pdf/2502.09193.pdf)
[20](https://arxiv.org/abs/2412.01378)
[21](https://www.nature.com/articles/s41598-025-97407-3)
[22](https://journals.sagepub.com/doi/10.1177/14727978251318997)
[23](https://arxiv.org/pdf/1409.2944.pdf)
[24](https://pubmed.ncbi.nlm.nih.gov/38550618/)
[25](https://www.themoonlight.io/ko/review/recommendation-with-generative-models)
[26](https://jameskle.com/s/JamesLe-Independent-Study-Report-Recommendation-Systems.pdf)
[27](https://ijci.journals.ekb.eg/article_207864_d767aea4b612492d52156288691fa256.pdf)
[28](https://www.sciencedirect.com/science/article/pii/S1877050925016035)
[29](https://arxiv.org/pdf/0802.1430.pdf)
[30](http://arxiv.org/pdf/2401.13193.pdf)
[31](http://arxiv.org/pdf/1911.03010.pdf)
[32](http://arxiv.org/pdf/2303.07001.pdf)
[33](https://www.ijimai.org/journal/sites/default/files/2022-05/ijimai_7_4_2.pdf)
[34](https://www.geeksforgeeks.org/deep-learning/contrastive-divergence-in-restricted-boltzmann-machines/)
[35](https://assemblyai.com/blog/ai-trends-graph-neural-networks)
[36](https://aegelfand.github.io/pub/ComparisonOfAlgsForCollabFilteringOnRBMs.pdf)
[37](https://arxiv.org/pdf/2101.06741.pdf)
